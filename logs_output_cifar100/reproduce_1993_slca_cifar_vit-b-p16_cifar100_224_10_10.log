2023-12-08 15:32:15,985 [trainer.py] => config: /home/rohitk/SLCA/exps/slca_cifar.json
2023-12-08 15:32:15,986 [trainer.py] => test_only: False
2023-12-08 15:32:15,986 [trainer.py] => prefix: reproduce
2023-12-08 15:32:15,986 [trainer.py] => dataset: cifar100_224
2023-12-08 15:32:15,986 [trainer.py] => memory_size: 0
2023-12-08 15:32:15,986 [trainer.py] => memory_per_class: 0
2023-12-08 15:32:15,986 [trainer.py] => fixed_memory: False
2023-12-08 15:32:15,986 [trainer.py] => shuffle: True
2023-12-08 15:32:15,986 [trainer.py] => init_cls: 10
2023-12-08 15:32:15,986 [trainer.py] => increment: 10
2023-12-08 15:32:15,986 [trainer.py] => model_name: slca_cifar
2023-12-08 15:32:15,986 [trainer.py] => model_postfix: 20e
2023-12-08 15:32:15,986 [trainer.py] => convnet_type: vit-b-p16
2023-12-08 15:32:15,986 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-12-08 15:32:15,986 [trainer.py] => seed: 1993
2023-12-08 15:32:15,986 [trainer.py] => epochs: 20
2023-12-08 15:32:15,986 [trainer.py] => ca_epochs: 5
2023-12-08 15:32:15,986 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-08 15:32:15,986 [trainer.py] => milestones: [18]
2023-12-08 15:32:15,986 [trainer.py] => run_id: 0
2023-12-08 15:33:13,211 [data_manager.py] => [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2023-12-08 15:33:14,901 [hub.py] => Downloading: "https://storage.googleapis.com/vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz" to /home/rohitk/.cache/torch/hub/checkpoints/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz

2023-12-08 15:33:56,901 [trainer.py] => All params: 85798656
2023-12-08 15:33:56,902 [trainer.py] => Trainable params: 85798656
2023-12-08 15:33:56,927 [slca.py] => Learning on 0-10
2023-12-08 15:34:26,782 [slca.py] => Task 0, Epoch 1/20 => Loss 0.788
2023-12-08 15:34:49,864 [slca.py] => Task 0, Epoch 2/20 => Loss 0.242
2023-12-08 15:35:13,190 [slca.py] => Task 0, Epoch 3/20 => Loss 0.188
2023-12-08 15:35:36,630 [slca.py] => Task 0, Epoch 4/20 => Loss 0.181
2023-12-08 15:36:13,747 [slca.py] => Task 0, Epoch 5/20 => Loss 0.164, Train_accy 94.760, Test_accy 99.000
2023-12-08 15:36:37,361 [slca.py] => Task 0, Epoch 6/20 => Loss 0.168
2023-12-08 15:37:00,976 [slca.py] => Task 0, Epoch 7/20 => Loss 0.154
2023-12-08 15:37:24,598 [slca.py] => Task 0, Epoch 8/20 => Loss 0.135
2023-12-08 15:37:48,179 [slca.py] => Task 0, Epoch 9/20 => Loss 0.117
2023-12-08 15:38:25,578 [slca.py] => Task 0, Epoch 10/20 => Loss 0.151, Train_accy 95.220, Test_accy 99.100
2023-12-08 15:38:49,163 [slca.py] => Task 0, Epoch 11/20 => Loss 0.147
2023-12-08 15:39:12,759 [slca.py] => Task 0, Epoch 12/20 => Loss 0.138
2023-12-08 15:39:36,531 [slca.py] => Task 0, Epoch 13/20 => Loss 0.130
2023-12-08 15:40:00,180 [slca.py] => Task 0, Epoch 14/20 => Loss 0.163
2023-12-08 15:40:37,882 [slca.py] => Task 0, Epoch 15/20 => Loss 0.138, Train_accy 95.960, Test_accy 99.300
2023-12-08 15:41:01,531 [slca.py] => Task 0, Epoch 16/20 => Loss 0.121
2023-12-08 15:41:25,116 [slca.py] => Task 0, Epoch 17/20 => Loss 0.133
2023-12-08 15:41:48,833 [slca.py] => Task 0, Epoch 18/20 => Loss 0.126
2023-12-08 15:42:12,535 [slca.py] => Task 0, Epoch 19/20 => Loss 0.115
2023-12-08 15:42:50,005 [slca.py] => Task 0, Epoch 20/20 => Loss 0.106, Train_accy 96.240, Test_accy 99.100
2023-12-08 15:43:11,707 [slca.py] => Exemplar size: 0
2023-12-08 15:43:12,228 [trainer.py] => No NME accuracy.
2023-12-08 15:43:12,228 [trainer.py] => CNN: {'total': 99.1, '00-09': 99.1, 'old': 0, 'new': 99.1}
2023-12-08 15:43:12,228 [trainer.py] => CNN top1 curve: [99.1]
2023-12-08 15:43:12,228 [trainer.py] => CNN top1 avg: 99.1
2023-12-08 15:43:12,228 [trainer.py] => CNN top5 curve: [100.0]

2023-12-08 15:43:12,229 [trainer.py] => All params: 85806346
2023-12-08 15:43:12,230 [trainer.py] => Trainable params: 85806346
2023-12-08 15:43:12,230 [slca.py] => Learning on 10-20
2023-12-08 15:43:35,854 [slca.py] => Task 1, Epoch 1/20 => Loss 0.569
2023-12-08 15:43:59,574 [slca.py] => Task 1, Epoch 2/20 => Loss 0.219
2023-12-08 15:44:23,424 [slca.py] => Task 1, Epoch 3/20 => Loss 0.208
2023-12-08 15:44:47,113 [slca.py] => Task 1, Epoch 4/20 => Loss 0.210
2023-12-08 15:45:26,933 [slca.py] => Task 1, Epoch 5/20 => Loss 0.192, Train_accy 88.140, Test_accy 96.850
2023-12-08 15:45:50,631 [slca.py] => Task 1, Epoch 6/20 => Loss 0.188
2023-12-08 15:46:14,371 [slca.py] => Task 1, Epoch 7/20 => Loss 0.184
2023-12-08 15:46:38,076 [slca.py] => Task 1, Epoch 8/20 => Loss 0.175
2023-12-08 15:47:01,824 [slca.py] => Task 1, Epoch 9/20 => Loss 0.195
2023-12-08 15:47:41,435 [slca.py] => Task 1, Epoch 10/20 => Loss 0.160, Train_accy 89.880, Test_accy 97.250
2023-12-08 15:48:05,177 [slca.py] => Task 1, Epoch 11/20 => Loss 0.167
2023-12-08 15:48:28,870 [slca.py] => Task 1, Epoch 12/20 => Loss 0.166
2023-12-08 15:48:52,571 [slca.py] => Task 1, Epoch 13/20 => Loss 0.180
2023-12-08 15:49:16,377 [slca.py] => Task 1, Epoch 14/20 => Loss 0.159
2023-12-08 15:49:55,977 [slca.py] => Task 1, Epoch 15/20 => Loss 0.160, Train_accy 91.000, Test_accy 97.250
2023-12-08 15:50:19,642 [slca.py] => Task 1, Epoch 16/20 => Loss 0.145
2023-12-08 15:50:43,403 [slca.py] => Task 1, Epoch 17/20 => Loss 0.141
2023-12-08 15:51:07,132 [slca.py] => Task 1, Epoch 18/20 => Loss 0.132
2023-12-08 15:51:30,936 [slca.py] => Task 1, Epoch 19/20 => Loss 0.147
2023-12-08 15:52:10,681 [slca.py] => Task 1, Epoch 20/20 => Loss 0.142, Train_accy 92.160, Test_accy 97.500
2023-12-08 15:52:36,532 [slca.py] => CA Task 1 => Loss 0.006, Test_accy 97.650
2023-12-08 15:52:43,352 [slca.py] => CA Task 1 => Loss 0.004, Test_accy 97.750
2023-12-08 15:52:50,117 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 97.750
2023-12-08 15:52:56,949 [slca.py] => CA Task 1 => Loss 0.004, Test_accy 97.800
2023-12-08 15:53:03,716 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 97.800
2023-12-08 15:53:09,804 [slca.py] => Exemplar size: 0
2023-12-08 15:53:10,282 [trainer.py] => No NME accuracy.
2023-12-08 15:53:10,282 [trainer.py] => CNN: {'total': 97.8, '00-09': 97.6, '10-19': 98.0, 'old': 97.6, 'new': 98.0}
2023-12-08 15:53:10,282 [trainer.py] => CNN top1 curve: [99.1, 97.8]
2023-12-08 15:53:10,282 [trainer.py] => CNN top1 avg: 98.44999999999999
2023-12-08 15:53:10,282 [trainer.py] => CNN top5 curve: [100.0, 99.85]

2023-12-08 15:53:10,283 [trainer.py] => All params: 85814036
2023-12-08 15:53:10,283 [trainer.py] => Trainable params: 85814036
2023-12-08 15:53:10,284 [slca.py] => Learning on 20-30
2023-12-08 15:53:33,954 [slca.py] => Task 2, Epoch 1/20 => Loss 0.402
2023-12-08 15:53:57,844 [slca.py] => Task 2, Epoch 2/20 => Loss 0.109
2023-12-08 15:54:21,635 [slca.py] => Task 2, Epoch 3/20 => Loss 0.100
2023-12-08 15:54:45,313 [slca.py] => Task 2, Epoch 4/20 => Loss 0.101
2023-12-08 15:55:26,909 [slca.py] => Task 2, Epoch 5/20 => Loss 0.121, Train_accy 91.640, Test_accy 96.330
2023-12-08 15:55:50,626 [slca.py] => Task 2, Epoch 6/20 => Loss 0.096
2023-12-08 15:56:14,377 [slca.py] => Task 2, Epoch 7/20 => Loss 0.087
2023-12-08 15:56:38,177 [slca.py] => Task 2, Epoch 8/20 => Loss 0.080
2023-12-08 15:57:01,967 [slca.py] => Task 2, Epoch 9/20 => Loss 0.080
2023-12-08 15:57:43,677 [slca.py] => Task 2, Epoch 10/20 => Loss 0.085, Train_accy 92.380, Test_accy 96.530
2023-12-08 15:58:07,402 [slca.py] => Task 2, Epoch 11/20 => Loss 0.071
2023-12-08 15:58:31,129 [slca.py] => Task 2, Epoch 12/20 => Loss 0.087
2023-12-08 15:58:54,944 [slca.py] => Task 2, Epoch 13/20 => Loss 0.076
2023-12-08 15:59:18,748 [slca.py] => Task 2, Epoch 14/20 => Loss 0.085
2023-12-08 16:00:00,385 [slca.py] => Task 2, Epoch 15/20 => Loss 0.075, Train_accy 93.460, Test_accy 96.470
2023-12-08 16:00:24,069 [slca.py] => Task 2, Epoch 16/20 => Loss 0.087
2023-12-08 16:00:47,806 [slca.py] => Task 2, Epoch 17/20 => Loss 0.088
2023-12-08 16:01:11,642 [slca.py] => Task 2, Epoch 18/20 => Loss 0.077
2023-12-08 16:01:35,399 [slca.py] => Task 2, Epoch 19/20 => Loss 0.088
2023-12-08 16:02:17,204 [slca.py] => Task 2, Epoch 20/20 => Loss 0.074, Train_accy 93.540, Test_accy 96.400
2023-12-08 16:02:45,841 [slca.py] => CA Task 2 => Loss 0.018, Test_accy 96.870
2023-12-08 16:02:55,606 [slca.py] => CA Task 2 => Loss 0.013, Test_accy 97.030
2023-12-08 16:03:05,389 [slca.py] => CA Task 2 => Loss 0.009, Test_accy 97.270
2023-12-08 16:03:15,133 [slca.py] => CA Task 2 => Loss 0.010, Test_accy 97.270
2023-12-08 16:03:24,928 [slca.py] => CA Task 2 => Loss 0.008, Test_accy 97.270
2023-12-08 16:03:33,686 [slca.py] => Exemplar size: 0
2023-12-08 16:03:34,178 [trainer.py] => No NME accuracy.
2023-12-08 16:03:34,179 [trainer.py] => CNN: {'total': 97.27, '00-09': 96.7, '10-19': 96.4, '20-29': 98.7, 'old': 96.55, 'new': 98.7}
2023-12-08 16:03:34,179 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27]
2023-12-08 16:03:34,179 [trainer.py] => CNN top1 avg: 98.05666666666666
2023-12-08 16:03:34,179 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73]

2023-12-08 16:03:34,179 [trainer.py] => All params: 85821726
2023-12-08 16:03:34,180 [trainer.py] => Trainable params: 85821726
2023-12-08 16:03:34,181 [slca.py] => Learning on 30-40
2023-12-08 16:03:57,826 [slca.py] => Task 3, Epoch 1/20 => Loss 0.484
2023-12-08 16:04:21,559 [slca.py] => Task 3, Epoch 2/20 => Loss 0.195
2023-12-08 16:04:45,330 [slca.py] => Task 3, Epoch 3/20 => Loss 0.162
2023-12-08 16:05:09,058 [slca.py] => Task 3, Epoch 4/20 => Loss 0.165
2023-12-08 16:05:52,945 [slca.py] => Task 3, Epoch 5/20 => Loss 0.161, Train_accy 84.860, Test_accy 95.350
2023-12-08 16:06:16,687 [slca.py] => Task 3, Epoch 6/20 => Loss 0.147
2023-12-08 16:06:40,417 [slca.py] => Task 3, Epoch 7/20 => Loss 0.133
2023-12-08 16:07:04,264 [slca.py] => Task 3, Epoch 8/20 => Loss 0.134
2023-12-08 16:07:28,077 [slca.py] => Task 3, Epoch 9/20 => Loss 0.124
2023-12-08 16:08:11,886 [slca.py] => Task 3, Epoch 10/20 => Loss 0.118, Train_accy 87.400, Test_accy 95.680
2023-12-08 16:08:35,687 [slca.py] => Task 3, Epoch 11/20 => Loss 0.127
2023-12-08 16:08:59,476 [slca.py] => Task 3, Epoch 12/20 => Loss 0.123
2023-12-08 16:09:23,283 [slca.py] => Task 3, Epoch 13/20 => Loss 0.116
2023-12-08 16:09:47,067 [slca.py] => Task 3, Epoch 14/20 => Loss 0.125
2023-12-08 16:10:30,963 [slca.py] => Task 3, Epoch 15/20 => Loss 0.124, Train_accy 88.040, Test_accy 95.950
2023-12-08 16:10:54,759 [slca.py] => Task 3, Epoch 16/20 => Loss 0.104
2023-12-08 16:11:18,609 [slca.py] => Task 3, Epoch 17/20 => Loss 0.102
2023-12-08 16:11:42,359 [slca.py] => Task 3, Epoch 18/20 => Loss 0.118
2023-12-08 16:12:06,125 [slca.py] => Task 3, Epoch 19/20 => Loss 0.099
2023-12-08 16:12:50,083 [slca.py] => Task 3, Epoch 20/20 => Loss 0.103, Train_accy 89.700, Test_accy 95.620
2023-12-08 16:13:21,960 [slca.py] => CA Task 3 => Loss 0.025, Test_accy 96.200
2023-12-08 16:13:34,656 [slca.py] => CA Task 3 => Loss 0.015, Test_accy 96.350
2023-12-08 16:13:47,493 [slca.py] => CA Task 3 => Loss 0.013, Test_accy 96.380
2023-12-08 16:14:00,351 [slca.py] => CA Task 3 => Loss 0.013, Test_accy 96.320
2023-12-08 16:14:13,391 [slca.py] => CA Task 3 => Loss 0.013, Test_accy 96.320
2023-12-08 16:14:24,732 [slca.py] => Exemplar size: 0
2023-12-08 16:14:25,208 [trainer.py] => No NME accuracy.
2023-12-08 16:14:25,208 [trainer.py] => CNN: {'total': 96.32, '00-09': 95.2, '10-19': 95.2, '20-29': 98.0, '30-39': 96.9, 'old': 96.13, 'new': 96.9}
2023-12-08 16:14:25,208 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27, 96.32]
2023-12-08 16:14:25,208 [trainer.py] => CNN top1 avg: 97.62249999999999
2023-12-08 16:14:25,208 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.68]

2023-12-08 16:14:25,209 [trainer.py] => All params: 85829416
2023-12-08 16:14:25,209 [trainer.py] => Trainable params: 85829416
2023-12-08 16:14:25,210 [slca.py] => Learning on 40-50
2023-12-08 16:14:48,971 [slca.py] => Task 4, Epoch 1/20 => Loss 0.424
2023-12-08 16:15:12,732 [slca.py] => Task 4, Epoch 2/20 => Loss 0.118
2023-12-08 16:15:36,472 [slca.py] => Task 4, Epoch 3/20 => Loss 0.118
2023-12-08 16:16:00,291 [slca.py] => Task 4, Epoch 4/20 => Loss 0.132
2023-12-08 16:16:46,263 [slca.py] => Task 4, Epoch 5/20 => Loss 0.126, Train_accy 85.680, Test_accy 93.480
2023-12-08 16:17:10,070 [slca.py] => Task 4, Epoch 6/20 => Loss 0.104
2023-12-08 16:17:33,739 [slca.py] => Task 4, Epoch 7/20 => Loss 0.095
2023-12-08 16:17:57,550 [slca.py] => Task 4, Epoch 8/20 => Loss 0.092
2023-12-08 16:18:21,359 [slca.py] => Task 4, Epoch 9/20 => Loss 0.100
2023-12-08 16:19:07,159 [slca.py] => Task 4, Epoch 10/20 => Loss 0.103, Train_accy 87.540, Test_accy 93.840
2023-12-08 16:19:30,899 [slca.py] => Task 4, Epoch 11/20 => Loss 0.095
2023-12-08 16:19:54,663 [slca.py] => Task 4, Epoch 12/20 => Loss 0.083
2023-12-08 16:20:18,524 [slca.py] => Task 4, Epoch 13/20 => Loss 0.088
2023-12-08 16:20:42,465 [slca.py] => Task 4, Epoch 14/20 => Loss 0.071
2023-12-08 16:21:28,389 [slca.py] => Task 4, Epoch 15/20 => Loss 0.083, Train_accy 89.060, Test_accy 93.760
2023-12-08 16:21:52,144 [slca.py] => Task 4, Epoch 16/20 => Loss 0.087
2023-12-08 16:22:16,026 [slca.py] => Task 4, Epoch 17/20 => Loss 0.074
2023-12-08 16:22:39,688 [slca.py] => Task 4, Epoch 18/20 => Loss 0.080
2023-12-08 16:23:03,559 [slca.py] => Task 4, Epoch 19/20 => Loss 0.072
2023-12-08 16:23:49,347 [slca.py] => Task 4, Epoch 20/20 => Loss 0.091, Train_accy 89.700, Test_accy 93.840
2023-12-08 16:24:24,057 [slca.py] => CA Task 4 => Loss 0.034, Test_accy 94.860
2023-12-08 16:24:39,933 [slca.py] => CA Task 4 => Loss 0.025, Test_accy 94.940
2023-12-08 16:24:55,772 [slca.py] => CA Task 4 => Loss 0.020, Test_accy 95.000
2023-12-08 16:25:11,739 [slca.py] => CA Task 4 => Loss 0.021, Test_accy 94.980
2023-12-08 16:25:27,648 [slca.py] => CA Task 4 => Loss 0.018, Test_accy 95.020
2023-12-08 16:25:41,695 [slca.py] => Exemplar size: 0
2023-12-08 16:25:42,184 [trainer.py] => No NME accuracy.
2023-12-08 16:25:42,185 [trainer.py] => CNN: {'total': 95.02, '00-09': 93.8, '10-19': 93.4, '20-29': 97.4, '30-39': 94.4, '40-49': 96.1, 'old': 94.75, 'new': 96.1}
2023-12-08 16:25:42,185 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27, 96.32, 95.02]
2023-12-08 16:25:42,185 [trainer.py] => CNN top1 avg: 97.10199999999999
2023-12-08 16:25:42,185 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.68, 99.58]

2023-12-08 16:25:42,185 [trainer.py] => All params: 85837106
2023-12-08 16:25:42,186 [trainer.py] => Trainable params: 85837106
2023-12-08 16:25:42,187 [slca.py] => Learning on 50-60
2023-12-08 16:26:05,898 [slca.py] => Task 5, Epoch 1/20 => Loss 0.479
2023-12-08 16:26:29,716 [slca.py] => Task 5, Epoch 2/20 => Loss 0.164
2023-12-08 16:26:53,465 [slca.py] => Task 5, Epoch 3/20 => Loss 0.147
2023-12-08 16:27:17,212 [slca.py] => Task 5, Epoch 4/20 => Loss 0.152
2023-12-08 16:28:05,247 [slca.py] => Task 5, Epoch 5/20 => Loss 0.141, Train_accy 85.520, Test_accy 92.770
2023-12-08 16:28:29,086 [slca.py] => Task 5, Epoch 6/20 => Loss 0.129
2023-12-08 16:28:52,997 [slca.py] => Task 5, Epoch 7/20 => Loss 0.115
2023-12-08 16:29:16,927 [slca.py] => Task 5, Epoch 8/20 => Loss 0.108
2023-12-08 16:29:40,679 [slca.py] => Task 5, Epoch 9/20 => Loss 0.113
2023-12-08 16:30:28,389 [slca.py] => Task 5, Epoch 10/20 => Loss 0.116, Train_accy 87.100, Test_accy 93.100
2023-12-08 16:30:52,198 [slca.py] => Task 5, Epoch 11/20 => Loss 0.120
2023-12-08 16:31:15,936 [slca.py] => Task 5, Epoch 12/20 => Loss 0.105
2023-12-08 16:31:39,675 [slca.py] => Task 5, Epoch 13/20 => Loss 0.124
2023-12-08 16:32:03,516 [slca.py] => Task 5, Epoch 14/20 => Loss 0.106
2023-12-08 16:32:51,332 [slca.py] => Task 5, Epoch 15/20 => Loss 0.087, Train_accy 88.360, Test_accy 93.130
2023-12-08 16:33:15,095 [slca.py] => Task 5, Epoch 16/20 => Loss 0.102
2023-12-08 16:33:38,930 [slca.py] => Task 5, Epoch 17/20 => Loss 0.119
2023-12-08 16:34:02,775 [slca.py] => Task 5, Epoch 18/20 => Loss 0.116
2023-12-08 16:34:26,620 [slca.py] => Task 5, Epoch 19/20 => Loss 0.114
2023-12-08 16:35:14,437 [slca.py] => Task 5, Epoch 20/20 => Loss 0.097, Train_accy 89.440, Test_accy 93.100
2023-12-08 16:35:52,045 [slca.py] => CA Task 5 => Loss 0.044, Test_accy 94.120
2023-12-08 16:36:10,976 [slca.py] => CA Task 5 => Loss 0.032, Test_accy 94.270
2023-12-08 16:36:29,787 [slca.py] => CA Task 5 => Loss 0.024, Test_accy 94.470
2023-12-08 16:36:48,641 [slca.py] => CA Task 5 => Loss 0.025, Test_accy 94.320
2023-12-08 16:37:07,500 [slca.py] => CA Task 5 => Loss 0.022, Test_accy 94.200
2023-12-08 16:37:24,131 [slca.py] => Exemplar size: 0
2023-12-08 16:37:24,654 [trainer.py] => No NME accuracy.
2023-12-08 16:37:24,654 [trainer.py] => CNN: {'total': 94.2, '00-09': 92.7, '10-19': 93.3, '20-29': 96.4, '30-39': 93.7, '40-49': 93.8, '50-59': 95.3, 'old': 93.98, 'new': 95.3}
2023-12-08 16:37:24,654 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27, 96.32, 95.02, 94.2]
2023-12-08 16:37:24,654 [trainer.py] => CNN top1 avg: 96.61833333333333
2023-12-08 16:37:24,654 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.68, 99.58, 99.57]

2023-12-08 16:37:24,655 [trainer.py] => All params: 85844796
2023-12-08 16:37:24,655 [trainer.py] => Trainable params: 85844796
2023-12-08 16:37:24,656 [slca.py] => Learning on 60-70
2023-12-08 16:37:48,334 [slca.py] => Task 6, Epoch 1/20 => Loss 0.387
2023-12-08 16:38:12,083 [slca.py] => Task 6, Epoch 2/20 => Loss 0.114
2023-12-08 16:38:35,740 [slca.py] => Task 6, Epoch 3/20 => Loss 0.101
2023-12-08 16:38:59,542 [slca.py] => Task 6, Epoch 4/20 => Loss 0.100
2023-12-08 16:39:49,388 [slca.py] => Task 6, Epoch 5/20 => Loss 0.085, Train_accy 86.060, Test_accy 92.810
2023-12-08 16:40:13,162 [slca.py] => Task 6, Epoch 6/20 => Loss 0.085
2023-12-08 16:40:36,895 [slca.py] => Task 6, Epoch 7/20 => Loss 0.086
2023-12-08 16:41:00,699 [slca.py] => Task 6, Epoch 8/20 => Loss 0.091
2023-12-08 16:41:24,480 [slca.py] => Task 6, Epoch 9/20 => Loss 0.088
2023-12-08 16:42:14,290 [slca.py] => Task 6, Epoch 10/20 => Loss 0.073, Train_accy 87.780, Test_accy 92.930
2023-12-08 16:42:37,999 [slca.py] => Task 6, Epoch 11/20 => Loss 0.078
2023-12-08 16:43:01,804 [slca.py] => Task 6, Epoch 12/20 => Loss 0.087
2023-12-08 16:43:25,618 [slca.py] => Task 6, Epoch 13/20 => Loss 0.091
2023-12-08 16:43:49,550 [slca.py] => Task 6, Epoch 14/20 => Loss 0.095
2023-12-08 16:44:39,381 [slca.py] => Task 6, Epoch 15/20 => Loss 0.082, Train_accy 88.120, Test_accy 92.840
2023-12-08 16:45:03,127 [slca.py] => Task 6, Epoch 16/20 => Loss 0.088
2023-12-08 16:45:26,843 [slca.py] => Task 6, Epoch 17/20 => Loss 0.085
2023-12-08 16:45:50,565 [slca.py] => Task 6, Epoch 18/20 => Loss 0.075
2023-12-08 16:46:14,374 [slca.py] => Task 6, Epoch 19/20 => Loss 0.085
2023-12-08 16:47:04,213 [slca.py] => Task 6, Epoch 20/20 => Loss 0.084, Train_accy 89.560, Test_accy 92.940
2023-12-08 16:47:45,281 [slca.py] => CA Task 6 => Loss 0.049, Test_accy 93.800
2023-12-08 16:48:07,114 [slca.py] => CA Task 6 => Loss 0.033, Test_accy 93.770
2023-12-08 16:48:28,955 [slca.py] => CA Task 6 => Loss 0.027, Test_accy 93.870
2023-12-08 16:48:50,817 [slca.py] => CA Task 6 => Loss 0.027, Test_accy 93.930
2023-12-08 16:49:12,680 [slca.py] => CA Task 6 => Loss 0.024, Test_accy 93.900
2023-12-08 16:49:32,131 [slca.py] => Exemplar size: 0
2023-12-08 16:49:32,617 [trainer.py] => No NME accuracy.
2023-12-08 16:49:32,617 [trainer.py] => CNN: {'total': 93.9, '00-09': 90.0, '10-19': 93.1, '20-29': 95.9, '30-39': 93.8, '40-49': 94.5, '50-59': 92.7, '60-69': 97.3, 'old': 93.33, 'new': 97.3}
2023-12-08 16:49:32,618 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27, 96.32, 95.02, 94.2, 93.9]
2023-12-08 16:49:32,618 [trainer.py] => CNN top1 avg: 96.22999999999999
2023-12-08 16:49:32,618 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.68, 99.58, 99.57, 99.54]

2023-12-08 16:49:32,618 [trainer.py] => All params: 85852486
2023-12-08 16:49:32,619 [trainer.py] => Trainable params: 85852486
2023-12-08 16:49:32,620 [slca.py] => Learning on 70-80
2023-12-08 16:49:56,234 [slca.py] => Task 7, Epoch 1/20 => Loss 0.455
2023-12-08 16:50:19,990 [slca.py] => Task 7, Epoch 2/20 => Loss 0.151
2023-12-08 16:50:43,836 [slca.py] => Task 7, Epoch 3/20 => Loss 0.163
2023-12-08 16:51:07,618 [slca.py] => Task 7, Epoch 4/20 => Loss 0.137
2023-12-08 16:51:59,738 [slca.py] => Task 7, Epoch 5/20 => Loss 0.139, Train_accy 79.520, Test_accy 90.950
2023-12-08 16:52:23,538 [slca.py] => Task 7, Epoch 6/20 => Loss 0.121
2023-12-08 16:52:47,385 [slca.py] => Task 7, Epoch 7/20 => Loss 0.134
2023-12-08 16:53:11,239 [slca.py] => Task 7, Epoch 8/20 => Loss 0.131
2023-12-08 16:53:35,135 [slca.py] => Task 7, Epoch 9/20 => Loss 0.120
2023-12-08 16:54:27,394 [slca.py] => Task 7, Epoch 10/20 => Loss 0.118, Train_accy 81.260, Test_accy 91.100
2023-12-08 16:54:51,202 [slca.py] => Task 7, Epoch 11/20 => Loss 0.124
2023-12-08 16:55:15,040 [slca.py] => Task 7, Epoch 12/20 => Loss 0.124
2023-12-08 16:55:38,999 [slca.py] => Task 7, Epoch 13/20 => Loss 0.117
2023-12-08 16:56:02,774 [slca.py] => Task 7, Epoch 14/20 => Loss 0.109
2023-12-08 16:56:54,887 [slca.py] => Task 7, Epoch 15/20 => Loss 0.108, Train_accy 83.420, Test_accy 90.940
2023-12-08 16:57:18,755 [slca.py] => Task 7, Epoch 16/20 => Loss 0.112
2023-12-08 16:57:42,499 [slca.py] => Task 7, Epoch 17/20 => Loss 0.103
2023-12-08 16:58:06,360 [slca.py] => Task 7, Epoch 18/20 => Loss 0.106
2023-12-08 16:58:30,252 [slca.py] => Task 7, Epoch 19/20 => Loss 0.109
2023-12-08 16:59:22,091 [slca.py] => Task 7, Epoch 20/20 => Loss 0.091, Train_accy 84.660, Test_accy 91.080
2023-12-08 17:00:06,264 [slca.py] => CA Task 7 => Loss 0.067, Test_accy 92.310
2023-12-08 17:00:31,137 [slca.py] => CA Task 7 => Loss 0.048, Test_accy 92.150
2023-12-08 17:00:56,119 [slca.py] => CA Task 7 => Loss 0.038, Test_accy 92.080
2023-12-08 17:01:21,049 [slca.py] => CA Task 7 => Loss 0.036, Test_accy 92.180
2023-12-08 17:01:45,831 [slca.py] => CA Task 7 => Loss 0.034, Test_accy 92.150
2023-12-08 17:02:08,104 [slca.py] => Exemplar size: 0
2023-12-08 17:02:08,594 [trainer.py] => No NME accuracy.
2023-12-08 17:02:08,594 [trainer.py] => CNN: {'total': 92.15, '00-09': 89.5, '10-19': 92.6, '20-29': 95.5, '30-39': 92.6, '40-49': 88.6, '50-59': 90.0, '60-69': 94.5, '70-79': 93.9, 'old': 91.9, 'new': 93.9}
2023-12-08 17:02:08,594 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27, 96.32, 95.02, 94.2, 93.9, 92.15]
2023-12-08 17:02:08,594 [trainer.py] => CNN top1 avg: 95.72
2023-12-08 17:02:08,595 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.68, 99.58, 99.57, 99.54, 99.49]

2023-12-08 17:02:08,595 [trainer.py] => All params: 85860176
2023-12-08 17:02:08,596 [trainer.py] => Trainable params: 85860176
2023-12-08 17:02:08,596 [slca.py] => Learning on 80-90
2023-12-08 17:02:32,308 [slca.py] => Task 8, Epoch 1/20 => Loss 0.458
2023-12-08 17:02:56,025 [slca.py] => Task 8, Epoch 2/20 => Loss 0.146
2023-12-08 17:03:19,753 [slca.py] => Task 8, Epoch 3/20 => Loss 0.150
2023-12-08 17:03:43,519 [slca.py] => Task 8, Epoch 4/20 => Loss 0.135
2023-12-08 17:04:37,290 [slca.py] => Task 8, Epoch 5/20 => Loss 0.115, Train_accy 79.600, Test_accy 90.440
2023-12-08 17:05:01,070 [slca.py] => Task 8, Epoch 6/20 => Loss 0.113
2023-12-08 17:05:24,829 [slca.py] => Task 8, Epoch 7/20 => Loss 0.117
2023-12-08 17:05:48,581 [slca.py] => Task 8, Epoch 8/20 => Loss 0.112
2023-12-08 17:06:12,349 [slca.py] => Task 8, Epoch 9/20 => Loss 0.110
2023-12-08 17:07:06,193 [slca.py] => Task 8, Epoch 10/20 => Loss 0.105, Train_accy 82.880, Test_accy 90.400
2023-12-08 17:07:29,994 [slca.py] => Task 8, Epoch 11/20 => Loss 0.122
2023-12-08 17:07:53,746 [slca.py] => Task 8, Epoch 12/20 => Loss 0.129
2023-12-08 17:08:17,470 [slca.py] => Task 8, Epoch 13/20 => Loss 0.131
2023-12-08 17:08:41,241 [slca.py] => Task 8, Epoch 14/20 => Loss 0.127
2023-12-08 17:09:34,805 [slca.py] => Task 8, Epoch 15/20 => Loss 0.104, Train_accy 83.840, Test_accy 90.170
2023-12-08 17:09:58,513 [slca.py] => Task 8, Epoch 16/20 => Loss 0.117
2023-12-08 17:10:22,365 [slca.py] => Task 8, Epoch 17/20 => Loss 0.101
2023-12-08 17:10:46,112 [slca.py] => Task 8, Epoch 18/20 => Loss 0.105
2023-12-08 17:11:09,925 [slca.py] => Task 8, Epoch 19/20 => Loss 0.101
2023-12-08 17:12:03,986 [slca.py] => Task 8, Epoch 20/20 => Loss 0.094, Train_accy 85.780, Test_accy 90.460
2023-12-08 17:12:50,745 [slca.py] => CA Task 8 => Loss 0.068, Test_accy 91.640
2023-12-08 17:13:18,752 [slca.py] => CA Task 8 => Loss 0.049, Test_accy 91.280
2023-12-08 17:13:46,613 [slca.py] => CA Task 8 => Loss 0.039, Test_accy 91.300
2023-12-08 17:14:14,518 [slca.py] => CA Task 8 => Loss 0.039, Test_accy 91.410
2023-12-08 17:14:42,525 [slca.py] => CA Task 8 => Loss 0.036, Test_accy 91.380
2023-12-08 17:15:07,295 [slca.py] => Exemplar size: 0
2023-12-08 17:15:07,780 [trainer.py] => No NME accuracy.
2023-12-08 17:15:07,780 [trainer.py] => CNN: {'total': 91.38, '00-09': 90.1, '10-19': 91.4, '20-29': 93.9, '30-39': 93.0, '40-49': 87.2, '50-59': 87.1, '60-69': 93.8, '70-79': 93.2, '80-89': 92.7, 'old': 91.21, 'new': 92.7}
2023-12-08 17:15:07,780 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27, 96.32, 95.02, 94.2, 93.9, 92.15, 91.38]
2023-12-08 17:15:07,780 [trainer.py] => CNN top1 avg: 95.23777777777778
2023-12-08 17:15:07,780 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.68, 99.58, 99.57, 99.54, 99.49, 99.43]

2023-12-08 17:15:07,781 [trainer.py] => All params: 85867866
2023-12-08 17:15:07,781 [trainer.py] => Trainable params: 85867866
2023-12-08 17:15:07,782 [slca.py] => Learning on 90-100
2023-12-08 17:15:31,485 [slca.py] => Task 9, Epoch 1/20 => Loss 0.464
2023-12-08 17:15:55,283 [slca.py] => Task 9, Epoch 2/20 => Loss 0.189
2023-12-08 17:16:19,035 [slca.py] => Task 9, Epoch 3/20 => Loss 0.167
2023-12-08 17:16:42,919 [slca.py] => Task 9, Epoch 4/20 => Loss 0.138
2023-12-08 17:17:39,198 [slca.py] => Task 9, Epoch 5/20 => Loss 0.136, Train_accy 81.600, Test_accy 90.250
2023-12-08 17:18:02,983 [slca.py] => Task 9, Epoch 6/20 => Loss 0.141
2023-12-08 17:18:26,892 [slca.py] => Task 9, Epoch 7/20 => Loss 0.128
2023-12-08 17:18:50,723 [slca.py] => Task 9, Epoch 8/20 => Loss 0.129
2023-12-08 17:19:14,574 [slca.py] => Task 9, Epoch 9/20 => Loss 0.115
2023-12-08 17:20:10,708 [slca.py] => Task 9, Epoch 10/20 => Loss 0.119, Train_accy 83.240, Test_accy 90.390
2023-12-08 17:20:34,394 [slca.py] => Task 9, Epoch 11/20 => Loss 0.121
2023-12-08 17:20:58,382 [slca.py] => Task 9, Epoch 12/20 => Loss 0.128
2023-12-08 17:21:22,266 [slca.py] => Task 9, Epoch 13/20 => Loss 0.122
2023-12-08 17:21:46,084 [slca.py] => Task 9, Epoch 14/20 => Loss 0.106
2023-12-08 17:22:42,389 [slca.py] => Task 9, Epoch 15/20 => Loss 0.098, Train_accy 84.640, Test_accy 90.210
2023-12-08 17:23:06,283 [slca.py] => Task 9, Epoch 16/20 => Loss 0.114
2023-12-08 17:23:30,070 [slca.py] => Task 9, Epoch 17/20 => Loss 0.115
2023-12-08 17:23:54,003 [slca.py] => Task 9, Epoch 18/20 => Loss 0.121
2023-12-08 17:24:17,912 [slca.py] => Task 9, Epoch 19/20 => Loss 0.105
2023-12-08 17:25:13,951 [slca.py] => Task 9, Epoch 20/20 => Loss 0.108, Train_accy 85.840, Test_accy 90.270
2023-12-08 17:26:03,877 [slca.py] => CA Task 9 => Loss 0.068, Test_accy 91.200
2023-12-08 17:26:34,987 [slca.py] => CA Task 9 => Loss 0.047, Test_accy 91.150
2023-12-08 17:27:05,916 [slca.py] => CA Task 9 => Loss 0.039, Test_accy 91.290
2023-12-08 17:27:36,862 [slca.py] => CA Task 9 => Loss 0.036, Test_accy 91.350
2023-12-08 17:28:07,804 [slca.py] => CA Task 9 => Loss 0.033, Test_accy 91.350
2023-12-08 17:28:35,231 [slca.py] => Exemplar size: 0
2023-12-08 17:28:35,718 [trainer.py] => No NME accuracy.
2023-12-08 17:28:35,719 [trainer.py] => CNN: {'total': 91.35, '00-09': 88.5, '10-19': 91.3, '20-29': 94.9, '30-39': 92.7, '40-49': 86.7, '50-59': 87.4, '60-69': 92.8, '70-79': 91.6, '80-89': 92.6, '90-99': 95.0, 'old': 90.94, 'new': 95.0}
2023-12-08 17:28:35,719 [trainer.py] => CNN top1 curve: [99.1, 97.8, 97.27, 96.32, 95.02, 94.2, 93.9, 92.15, 91.38, 91.35]
2023-12-08 17:28:35,719 [trainer.py] => CNN top1 avg: 94.849
2023-12-08 17:28:35,719 [trainer.py] => CNN top5 curve: [100.0, 99.85, 99.73, 99.68, 99.58, 99.57, 99.54, 99.49, 99.43, 99.2]

2023-12-08 17:28:35,742 [trainer.py] => config: /home/rohitk/SLCA/exps/slca_cifar.json
2023-12-08 17:28:35,742 [trainer.py] => test_only: False
2023-12-08 17:28:35,742 [trainer.py] => prefix: reproduce
2023-12-08 17:28:35,742 [trainer.py] => dataset: cifar100_224
2023-12-08 17:28:35,742 [trainer.py] => memory_size: 0
2023-12-08 17:28:35,743 [trainer.py] => memory_per_class: 0
2023-12-08 17:28:35,743 [trainer.py] => fixed_memory: False
2023-12-08 17:28:35,743 [trainer.py] => shuffle: True
2023-12-08 17:28:35,743 [trainer.py] => init_cls: 10
2023-12-08 17:28:35,743 [trainer.py] => increment: 10
2023-12-08 17:28:35,743 [trainer.py] => model_name: slca_cifar
2023-12-08 17:28:35,743 [trainer.py] => model_postfix: 20e
2023-12-08 17:28:35,743 [trainer.py] => convnet_type: vit-b-p16
2023-12-08 17:28:35,743 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-12-08 17:28:35,743 [trainer.py] => seed: 1996
2023-12-08 17:28:35,743 [trainer.py] => epochs: 20
2023-12-08 17:28:35,743 [trainer.py] => ca_epochs: 5
2023-12-08 17:28:35,743 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-08 17:28:35,743 [trainer.py] => milestones: [18]
2023-12-08 17:28:35,743 [trainer.py] => run_id: 1
2023-12-08 17:28:37,284 [data_manager.py] => [35, 18, 40, 85, 63, 62, 77, 73, 12, 44, 31, 26, 78, 33, 9, 86, 92, 87, 93, 71, 45, 7, 17, 6, 24, 21, 27, 99, 81, 2, 94, 57, 13, 30, 8, 55, 76, 46, 49, 47, 60, 36, 16, 43, 54, 83, 88, 51, 15, 20, 53, 66, 50, 3, 98, 90, 80, 5, 32, 75, 89, 10, 82, 64, 29, 70, 91, 52, 95, 23, 69, 56, 96, 68, 39, 42, 19, 0, 22, 72, 65, 61, 74, 59, 25, 37, 48, 84, 34, 28, 4, 11, 14, 79, 97, 41, 38, 67, 1, 58]
2023-12-08 17:28:39,310 [trainer.py] => All params: 85798656
2023-12-08 17:28:39,311 [trainer.py] => Trainable params: 85798656
2023-12-08 17:28:39,311 [slca.py] => Learning on 0-10
2023-12-08 17:29:03,114 [slca.py] => Task 0, Epoch 1/20 => Loss 0.785
2023-12-08 17:29:27,067 [slca.py] => Task 0, Epoch 2/20 => Loss 0.216
2023-12-08 17:29:50,780 [slca.py] => Task 0, Epoch 3/20 => Loss 0.165
2023-12-08 17:30:14,711 [slca.py] => Task 0, Epoch 4/20 => Loss 0.162
2023-12-08 17:30:52,413 [slca.py] => Task 0, Epoch 5/20 => Loss 0.181, Train_accy 94.980, Test_accy 98.900
2023-12-08 17:31:16,142 [slca.py] => Task 0, Epoch 6/20 => Loss 0.171
2023-12-08 17:31:39,915 [slca.py] => Task 0, Epoch 7/20 => Loss 0.161
2023-12-08 17:32:03,630 [slca.py] => Task 0, Epoch 8/20 => Loss 0.151
2023-12-08 17:32:27,388 [slca.py] => Task 0, Epoch 9/20 => Loss 0.144
2023-12-08 17:33:05,008 [slca.py] => Task 0, Epoch 10/20 => Loss 0.145, Train_accy 94.980, Test_accy 99.000
2023-12-08 17:33:28,679 [slca.py] => Task 0, Epoch 11/20 => Loss 0.133
2023-12-08 17:33:52,489 [slca.py] => Task 0, Epoch 12/20 => Loss 0.143
2023-12-08 17:34:16,219 [slca.py] => Task 0, Epoch 13/20 => Loss 0.128
2023-12-08 17:34:40,106 [slca.py] => Task 0, Epoch 14/20 => Loss 0.140
2023-12-08 17:35:17,786 [slca.py] => Task 0, Epoch 15/20 => Loss 0.126, Train_accy 95.940, Test_accy 99.100
2023-12-08 17:35:41,520 [slca.py] => Task 0, Epoch 16/20 => Loss 0.117
2023-12-08 17:36:05,429 [slca.py] => Task 0, Epoch 17/20 => Loss 0.114
2023-12-08 17:36:29,138 [slca.py] => Task 0, Epoch 18/20 => Loss 0.127
2023-12-08 17:36:52,907 [slca.py] => Task 0, Epoch 19/20 => Loss 0.131
2023-12-08 17:37:30,661 [slca.py] => Task 0, Epoch 20/20 => Loss 0.097, Train_accy 96.320, Test_accy 99.200
2023-12-08 17:37:52,797 [slca.py] => Exemplar size: 0
2023-12-08 17:37:53,305 [trainer.py] => No NME accuracy.
2023-12-08 17:37:53,306 [trainer.py] => CNN: {'total': 99.2, '00-09': 99.2, 'old': 0, 'new': 99.2}
2023-12-08 17:37:53,306 [trainer.py] => CNN top1 curve: [99.2]
2023-12-08 17:37:53,306 [trainer.py] => CNN top1 avg: 99.2
2023-12-08 17:37:53,306 [trainer.py] => CNN top5 curve: [100.0]

2023-12-08 17:37:53,307 [trainer.py] => All params: 85806346
2023-12-08 17:37:53,307 [trainer.py] => Trainable params: 85806346
2023-12-08 17:37:53,308 [slca.py] => Learning on 10-20
2023-12-08 17:38:16,972 [slca.py] => Task 1, Epoch 1/20 => Loss 0.473
2023-12-08 17:38:40,653 [slca.py] => Task 1, Epoch 2/20 => Loss 0.178
2023-12-08 17:39:04,456 [slca.py] => Task 1, Epoch 3/20 => Loss 0.140
2023-12-08 17:39:28,236 [slca.py] => Task 1, Epoch 4/20 => Loss 0.152
2023-12-08 17:40:08,115 [slca.py] => Task 1, Epoch 5/20 => Loss 0.147, Train_accy 91.180, Test_accy 97.450
2023-12-08 17:40:31,834 [slca.py] => Task 1, Epoch 6/20 => Loss 0.131
2023-12-08 17:40:55,635 [slca.py] => Task 1, Epoch 7/20 => Loss 0.122
2023-12-08 17:41:19,443 [slca.py] => Task 1, Epoch 8/20 => Loss 0.139
2023-12-08 17:41:43,227 [slca.py] => Task 1, Epoch 9/20 => Loss 0.124
2023-12-08 17:42:22,955 [slca.py] => Task 1, Epoch 10/20 => Loss 0.117, Train_accy 92.080, Test_accy 97.750
2023-12-08 17:42:46,662 [slca.py] => Task 1, Epoch 11/20 => Loss 0.121
2023-12-08 17:43:10,426 [slca.py] => Task 1, Epoch 12/20 => Loss 0.127
2023-12-08 17:43:34,353 [slca.py] => Task 1, Epoch 13/20 => Loss 0.122
2023-12-08 17:43:58,165 [slca.py] => Task 1, Epoch 14/20 => Loss 0.117
2023-12-08 17:44:38,050 [slca.py] => Task 1, Epoch 15/20 => Loss 0.105, Train_accy 93.440, Test_accy 98.250
2023-12-08 17:45:01,790 [slca.py] => Task 1, Epoch 16/20 => Loss 0.107
2023-12-08 17:45:25,512 [slca.py] => Task 1, Epoch 17/20 => Loss 0.100
2023-12-08 17:45:49,290 [slca.py] => Task 1, Epoch 18/20 => Loss 0.083
2023-12-08 17:46:12,935 [slca.py] => Task 1, Epoch 19/20 => Loss 0.099
2023-12-08 17:46:52,658 [slca.py] => Task 1, Epoch 20/20 => Loss 0.100, Train_accy 94.160, Test_accy 97.800
2023-12-08 17:47:18,219 [slca.py] => CA Task 1 => Loss 0.013, Test_accy 97.950
2023-12-08 17:47:25,052 [slca.py] => CA Task 1 => Loss 0.011, Test_accy 98.050
2023-12-08 17:47:31,861 [slca.py] => CA Task 1 => Loss 0.009, Test_accy 98.000
2023-12-08 17:47:38,617 [slca.py] => CA Task 1 => Loss 0.007, Test_accy 98.000
2023-12-08 17:47:45,473 [slca.py] => CA Task 1 => Loss 0.009, Test_accy 97.950
2023-12-08 17:47:51,657 [slca.py] => Exemplar size: 0
2023-12-08 17:47:52,144 [trainer.py] => No NME accuracy.
2023-12-08 17:47:52,145 [trainer.py] => CNN: {'total': 97.95, '00-09': 97.5, '10-19': 98.4, 'old': 97.5, 'new': 98.4}
2023-12-08 17:47:52,145 [trainer.py] => CNN top1 curve: [99.2, 97.95]
2023-12-08 17:47:52,145 [trainer.py] => CNN top1 avg: 98.575
2023-12-08 17:47:52,145 [trainer.py] => CNN top5 curve: [100.0, 99.95]

2023-12-08 17:47:52,146 [trainer.py] => All params: 85814036
2023-12-08 17:47:52,146 [trainer.py] => Trainable params: 85814036
2023-12-08 17:47:52,147 [slca.py] => Learning on 20-30
2023-12-08 17:48:15,851 [slca.py] => Task 2, Epoch 1/20 => Loss 0.494
2023-12-08 17:48:39,667 [slca.py] => Task 2, Epoch 2/20 => Loss 0.171
2023-12-08 17:49:03,429 [slca.py] => Task 2, Epoch 3/20 => Loss 0.151
2023-12-08 17:49:27,124 [slca.py] => Task 2, Epoch 4/20 => Loss 0.163
2023-12-08 17:50:08,951 [slca.py] => Task 2, Epoch 5/20 => Loss 0.146, Train_accy 87.780, Test_accy 95.200
2023-12-08 17:50:32,744 [slca.py] => Task 2, Epoch 6/20 => Loss 0.143
2023-12-08 17:50:56,560 [slca.py] => Task 2, Epoch 7/20 => Loss 0.135
2023-12-08 17:51:20,427 [slca.py] => Task 2, Epoch 8/20 => Loss 0.128
2023-12-08 17:51:44,215 [slca.py] => Task 2, Epoch 9/20 => Loss 0.130
2023-12-08 17:52:25,983 [slca.py] => Task 2, Epoch 10/20 => Loss 0.124, Train_accy 88.900, Test_accy 95.270
2023-12-08 17:52:49,885 [slca.py] => Task 2, Epoch 11/20 => Loss 0.120
2023-12-08 17:53:13,655 [slca.py] => Task 2, Epoch 12/20 => Loss 0.129
2023-12-08 17:53:37,405 [slca.py] => Task 2, Epoch 13/20 => Loss 0.116
2023-12-08 17:54:01,209 [slca.py] => Task 2, Epoch 14/20 => Loss 0.120
2023-12-08 17:54:43,160 [slca.py] => Task 2, Epoch 15/20 => Loss 0.118, Train_accy 90.180, Test_accy 95.400
2023-12-08 17:55:06,961 [slca.py] => Task 2, Epoch 16/20 => Loss 0.125
2023-12-08 17:55:30,800 [slca.py] => Task 2, Epoch 17/20 => Loss 0.112
2023-12-08 17:55:54,668 [slca.py] => Task 2, Epoch 18/20 => Loss 0.106
2023-12-08 17:56:18,394 [slca.py] => Task 2, Epoch 19/20 => Loss 0.112
2023-12-08 17:57:00,177 [slca.py] => Task 2, Epoch 20/20 => Loss 0.098, Train_accy 91.000, Test_accy 95.630
2023-12-08 17:57:28,928 [slca.py] => CA Task 2 => Loss 0.030, Test_accy 95.700
2023-12-08 17:57:38,802 [slca.py] => CA Task 2 => Loss 0.022, Test_accy 95.870
2023-12-08 17:57:48,675 [slca.py] => CA Task 2 => Loss 0.019, Test_accy 95.900
2023-12-08 17:57:58,442 [slca.py] => CA Task 2 => Loss 0.017, Test_accy 95.900
2023-12-08 17:58:08,312 [slca.py] => CA Task 2 => Loss 0.016, Test_accy 95.900
2023-12-08 17:58:17,100 [slca.py] => Exemplar size: 0
2023-12-08 17:58:17,581 [trainer.py] => No NME accuracy.
2023-12-08 17:58:17,582 [trainer.py] => CNN: {'total': 95.9, '00-09': 94.8, '10-19': 95.6, '20-29': 97.3, 'old': 95.2, 'new': 97.3}
2023-12-08 17:58:17,582 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9]
2023-12-08 17:58:17,582 [trainer.py] => CNN top1 avg: 97.68333333333334
2023-12-08 17:58:17,582 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77]

2023-12-08 17:58:17,582 [trainer.py] => All params: 85821726
2023-12-08 17:58:17,583 [trainer.py] => Trainable params: 85821726
2023-12-08 17:58:17,584 [slca.py] => Learning on 30-40
2023-12-08 17:58:41,247 [slca.py] => Task 3, Epoch 1/20 => Loss 0.456
2023-12-08 17:59:05,020 [slca.py] => Task 3, Epoch 2/20 => Loss 0.129
2023-12-08 17:59:28,780 [slca.py] => Task 3, Epoch 3/20 => Loss 0.100
2023-12-08 17:59:52,571 [slca.py] => Task 3, Epoch 4/20 => Loss 0.087
2023-12-08 18:00:36,300 [slca.py] => Task 3, Epoch 5/20 => Loss 0.103, Train_accy 86.860, Test_accy 94.950
2023-12-08 18:01:00,009 [slca.py] => Task 3, Epoch 6/20 => Loss 0.097
2023-12-08 18:01:23,802 [slca.py] => Task 3, Epoch 7/20 => Loss 0.083
2023-12-08 18:01:47,703 [slca.py] => Task 3, Epoch 8/20 => Loss 0.100
2023-12-08 18:02:11,451 [slca.py] => Task 3, Epoch 9/20 => Loss 0.080
2023-12-08 18:02:55,250 [slca.py] => Task 3, Epoch 10/20 => Loss 0.073, Train_accy 88.340, Test_accy 95.080
2023-12-08 18:03:19,062 [slca.py] => Task 3, Epoch 11/20 => Loss 0.078
2023-12-08 18:03:42,812 [slca.py] => Task 3, Epoch 12/20 => Loss 0.087
2023-12-08 18:04:06,565 [slca.py] => Task 3, Epoch 13/20 => Loss 0.085
2023-12-08 18:04:30,455 [slca.py] => Task 3, Epoch 14/20 => Loss 0.080
2023-12-08 18:05:14,385 [slca.py] => Task 3, Epoch 15/20 => Loss 0.078, Train_accy 88.920, Test_accy 94.800
2023-12-08 18:05:38,169 [slca.py] => Task 3, Epoch 16/20 => Loss 0.076
2023-12-08 18:06:01,957 [slca.py] => Task 3, Epoch 17/20 => Loss 0.072
2023-12-08 18:06:25,823 [slca.py] => Task 3, Epoch 18/20 => Loss 0.087
2023-12-08 18:06:49,707 [slca.py] => Task 3, Epoch 19/20 => Loss 0.069
2023-12-08 18:07:33,487 [slca.py] => Task 3, Epoch 20/20 => Loss 0.088, Train_accy 89.760, Test_accy 94.720
2023-12-08 18:08:05,497 [slca.py] => CA Task 3 => Loss 0.039, Test_accy 95.020
2023-12-08 18:08:18,366 [slca.py] => CA Task 3 => Loss 0.028, Test_accy 95.320
2023-12-08 18:08:31,168 [slca.py] => CA Task 3 => Loss 0.021, Test_accy 95.450
2023-12-08 18:08:44,108 [slca.py] => CA Task 3 => Loss 0.019, Test_accy 95.450
2023-12-08 18:08:56,901 [slca.py] => CA Task 3 => Loss 0.022, Test_accy 95.480
2023-12-08 18:09:08,452 [slca.py] => Exemplar size: 0
2023-12-08 18:09:08,935 [trainer.py] => No NME accuracy.
2023-12-08 18:09:08,936 [trainer.py] => CNN: {'total': 95.48, '00-09': 92.8, '10-19': 94.9, '20-29': 96.4, '30-39': 97.8, 'old': 94.7, 'new': 97.8}
2023-12-08 18:09:08,936 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9, 95.48]
2023-12-08 18:09:08,936 [trainer.py] => CNN top1 avg: 97.13250000000001
2023-12-08 18:09:08,936 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77, 99.72]

2023-12-08 18:09:08,937 [trainer.py] => All params: 85829416
2023-12-08 18:09:08,937 [trainer.py] => Trainable params: 85829416
2023-12-08 18:09:08,938 [slca.py] => Learning on 40-50
2023-12-08 18:09:32,718 [slca.py] => Task 4, Epoch 1/20 => Loss 0.406
2023-12-08 18:09:56,432 [slca.py] => Task 4, Epoch 2/20 => Loss 0.110
2023-12-08 18:10:20,245 [slca.py] => Task 4, Epoch 3/20 => Loss 0.102
2023-12-08 18:10:44,192 [slca.py] => Task 4, Epoch 4/20 => Loss 0.089
2023-12-08 18:11:30,116 [slca.py] => Task 4, Epoch 5/20 => Loss 0.085, Train_accy 88.740, Test_accy 95.020
2023-12-08 18:11:53,962 [slca.py] => Task 4, Epoch 6/20 => Loss 0.086
2023-12-08 18:12:17,869 [slca.py] => Task 4, Epoch 7/20 => Loss 0.084
2023-12-08 18:12:41,756 [slca.py] => Task 4, Epoch 8/20 => Loss 0.084
2023-12-08 18:13:05,644 [slca.py] => Task 4, Epoch 9/20 => Loss 0.082
2023-12-08 18:13:51,596 [slca.py] => Task 4, Epoch 10/20 => Loss 0.079, Train_accy 90.420, Test_accy 94.660
2023-12-08 18:14:15,538 [slca.py] => Task 4, Epoch 11/20 => Loss 0.080
2023-12-08 18:14:39,325 [slca.py] => Task 4, Epoch 12/20 => Loss 0.069
2023-12-08 18:15:03,144 [slca.py] => Task 4, Epoch 13/20 => Loss 0.085
2023-12-08 18:15:26,903 [slca.py] => Task 4, Epoch 14/20 => Loss 0.084
2023-12-08 18:16:12,771 [slca.py] => Task 4, Epoch 15/20 => Loss 0.090, Train_accy 90.860, Test_accy 94.660
2023-12-08 18:16:36,542 [slca.py] => Task 4, Epoch 16/20 => Loss 0.073
2023-12-08 18:17:00,418 [slca.py] => Task 4, Epoch 17/20 => Loss 0.080
2023-12-08 18:17:24,189 [slca.py] => Task 4, Epoch 18/20 => Loss 0.074
2023-12-08 18:17:48,047 [slca.py] => Task 4, Epoch 19/20 => Loss 0.073
2023-12-08 18:18:33,874 [slca.py] => Task 4, Epoch 20/20 => Loss 0.076, Train_accy 91.480, Test_accy 94.720
2023-12-08 18:19:08,929 [slca.py] => CA Task 4 => Loss 0.035, Test_accy 95.340
2023-12-08 18:19:24,834 [slca.py] => CA Task 4 => Loss 0.028, Test_accy 95.500
2023-12-08 18:19:40,680 [slca.py] => CA Task 4 => Loss 0.023, Test_accy 95.380
2023-12-08 18:19:56,568 [slca.py] => CA Task 4 => Loss 0.018, Test_accy 95.540
2023-12-08 18:20:12,477 [slca.py] => CA Task 4 => Loss 0.019, Test_accy 95.560
2023-12-08 18:20:26,569 [slca.py] => Exemplar size: 0
2023-12-08 18:20:27,098 [trainer.py] => No NME accuracy.
2023-12-08 18:20:27,098 [trainer.py] => CNN: {'total': 95.56, '00-09': 93.4, '10-19': 93.6, '20-29': 96.3, '30-39': 97.2, '40-49': 97.3, 'old': 95.12, 'new': 97.3}
2023-12-08 18:20:27,098 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9, 95.48, 95.56]
2023-12-08 18:20:27,098 [trainer.py] => CNN top1 avg: 96.81800000000001
2023-12-08 18:20:27,099 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77, 99.72, 99.72]

2023-12-08 18:20:27,099 [trainer.py] => All params: 85837106
2023-12-08 18:20:27,100 [trainer.py] => Trainable params: 85837106
2023-12-08 18:20:27,100 [slca.py] => Learning on 50-60
2023-12-08 18:20:50,767 [slca.py] => Task 5, Epoch 1/20 => Loss 0.465
2023-12-08 18:21:14,556 [slca.py] => Task 5, Epoch 2/20 => Loss 0.182
2023-12-08 18:21:38,462 [slca.py] => Task 5, Epoch 3/20 => Loss 0.156
2023-12-08 18:22:02,256 [slca.py] => Task 5, Epoch 4/20 => Loss 0.155
2023-12-08 18:22:50,457 [slca.py] => Task 5, Epoch 5/20 => Loss 0.138, Train_accy 79.120, Test_accy 93.700
2023-12-08 18:23:14,270 [slca.py] => Task 5, Epoch 6/20 => Loss 0.140
2023-12-08 18:23:38,122 [slca.py] => Task 5, Epoch 7/20 => Loss 0.149
2023-12-08 18:24:01,986 [slca.py] => Task 5, Epoch 8/20 => Loss 0.152
2023-12-08 18:24:25,863 [slca.py] => Task 5, Epoch 9/20 => Loss 0.138
2023-12-08 18:25:13,716 [slca.py] => Task 5, Epoch 10/20 => Loss 0.134, Train_accy 81.760, Test_accy 93.920
2023-12-08 18:25:37,594 [slca.py] => Task 5, Epoch 11/20 => Loss 0.148
2023-12-08 18:26:01,418 [slca.py] => Task 5, Epoch 12/20 => Loss 0.178
2023-12-08 18:26:25,304 [slca.py] => Task 5, Epoch 13/20 => Loss 0.140
2023-12-08 18:26:49,125 [slca.py] => Task 5, Epoch 14/20 => Loss 0.128
2023-12-08 18:27:37,206 [slca.py] => Task 5, Epoch 15/20 => Loss 0.130, Train_accy 84.460, Test_accy 93.880
2023-12-08 18:28:00,915 [slca.py] => Task 5, Epoch 16/20 => Loss 0.137
2023-12-08 18:28:24,780 [slca.py] => Task 5, Epoch 17/20 => Loss 0.132
2023-12-08 18:28:48,526 [slca.py] => Task 5, Epoch 18/20 => Loss 0.126
2023-12-08 18:29:12,365 [slca.py] => Task 5, Epoch 19/20 => Loss 0.120
2023-12-08 18:30:00,202 [slca.py] => Task 5, Epoch 20/20 => Loss 0.115, Train_accy 84.220, Test_accy 93.900
2023-12-08 18:30:37,911 [slca.py] => CA Task 5 => Loss 0.042, Test_accy 94.450
2023-12-08 18:30:56,858 [slca.py] => CA Task 5 => Loss 0.028, Test_accy 94.580
2023-12-08 18:31:15,816 [slca.py] => CA Task 5 => Loss 0.023, Test_accy 94.600
2023-12-08 18:31:34,653 [slca.py] => CA Task 5 => Loss 0.022, Test_accy 94.630
2023-12-08 18:31:53,483 [slca.py] => CA Task 5 => Loss 0.021, Test_accy 94.680
2023-12-08 18:32:10,247 [slca.py] => Exemplar size: 0
2023-12-08 18:32:10,730 [trainer.py] => No NME accuracy.
2023-12-08 18:32:10,730 [trainer.py] => CNN: {'total': 94.68, '00-09': 90.3, '10-19': 92.7, '20-29': 94.4, '30-39': 96.2, '40-49': 97.0, '50-59': 97.5, 'old': 94.12, 'new': 97.5}
2023-12-08 18:32:10,731 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9, 95.48, 95.56, 94.68]
2023-12-08 18:32:10,731 [trainer.py] => CNN top1 avg: 96.46166666666666
2023-12-08 18:32:10,731 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77, 99.72, 99.72, 99.68]

2023-12-08 18:32:10,731 [trainer.py] => All params: 85844796
2023-12-08 18:32:10,732 [trainer.py] => Trainable params: 85844796
2023-12-08 18:32:10,733 [slca.py] => Learning on 60-70
2023-12-08 18:32:34,457 [slca.py] => Task 6, Epoch 1/20 => Loss 0.388
2023-12-08 18:32:58,255 [slca.py] => Task 6, Epoch 2/20 => Loss 0.122
2023-12-08 18:33:22,091 [slca.py] => Task 6, Epoch 3/20 => Loss 0.092
2023-12-08 18:33:46,013 [slca.py] => Task 6, Epoch 4/20 => Loss 0.091
2023-12-08 18:34:36,136 [slca.py] => Task 6, Epoch 5/20 => Loss 0.092, Train_accy 79.600, Test_accy 91.710
2023-12-08 18:34:59,894 [slca.py] => Task 6, Epoch 6/20 => Loss 0.090
2023-12-08 18:35:23,685 [slca.py] => Task 6, Epoch 7/20 => Loss 0.089
2023-12-08 18:35:47,473 [slca.py] => Task 6, Epoch 8/20 => Loss 0.082
2023-12-08 18:36:11,220 [slca.py] => Task 6, Epoch 9/20 => Loss 0.079
2023-12-08 18:37:01,236 [slca.py] => Task 6, Epoch 10/20 => Loss 0.081, Train_accy 83.400, Test_accy 92.310
2023-12-08 18:37:25,108 [slca.py] => Task 6, Epoch 11/20 => Loss 0.073
2023-12-08 18:37:48,943 [slca.py] => Task 6, Epoch 12/20 => Loss 0.082
2023-12-08 18:38:12,787 [slca.py] => Task 6, Epoch 13/20 => Loss 0.081
2023-12-08 18:38:36,610 [slca.py] => Task 6, Epoch 14/20 => Loss 0.073
2023-12-08 18:39:26,578 [slca.py] => Task 6, Epoch 15/20 => Loss 0.086, Train_accy 83.540, Test_accy 92.310
2023-12-08 18:39:50,329 [slca.py] => Task 6, Epoch 16/20 => Loss 0.091
2023-12-08 18:40:14,090 [slca.py] => Task 6, Epoch 17/20 => Loss 0.077
2023-12-08 18:40:37,915 [slca.py] => Task 6, Epoch 18/20 => Loss 0.087
2023-12-08 18:41:01,741 [slca.py] => Task 6, Epoch 19/20 => Loss 0.080
2023-12-08 18:41:51,548 [slca.py] => Task 6, Epoch 20/20 => Loss 0.077, Train_accy 80.360, Test_accy 91.670
2023-12-08 18:42:32,414 [slca.py] => CA Task 6 => Loss 0.059, Test_accy 93.060
2023-12-08 18:42:54,274 [slca.py] => CA Task 6 => Loss 0.037, Test_accy 93.130
2023-12-08 18:43:16,246 [slca.py] => CA Task 6 => Loss 0.031, Test_accy 93.060
2023-12-08 18:43:38,062 [slca.py] => CA Task 6 => Loss 0.028, Test_accy 93.110
2023-12-08 18:43:59,896 [slca.py] => CA Task 6 => Loss 0.027, Test_accy 93.140
2023-12-08 18:44:19,480 [slca.py] => Exemplar size: 0
2023-12-08 18:44:19,978 [trainer.py] => No NME accuracy.
2023-12-08 18:44:19,978 [trainer.py] => CNN: {'total': 93.14, '00-09': 88.2, '10-19': 91.7, '20-29': 94.0, '30-39': 91.8, '40-49': 95.8, '50-59': 96.0, '60-69': 94.5, 'old': 92.92, 'new': 94.5}
2023-12-08 18:44:19,978 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9, 95.48, 95.56, 94.68, 93.14]
2023-12-08 18:44:19,978 [trainer.py] => CNN top1 avg: 95.98714285714286
2023-12-08 18:44:19,978 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77, 99.72, 99.72, 99.68, 99.59]

2023-12-08 18:44:19,979 [trainer.py] => All params: 85852486
2023-12-08 18:44:19,979 [trainer.py] => Trainable params: 85852486
2023-12-08 18:44:19,980 [slca.py] => Learning on 70-80
2023-12-08 18:44:43,808 [slca.py] => Task 7, Epoch 1/20 => Loss 0.385
2023-12-08 18:45:07,613 [slca.py] => Task 7, Epoch 2/20 => Loss 0.111
2023-12-08 18:45:31,465 [slca.py] => Task 7, Epoch 3/20 => Loss 0.115
2023-12-08 18:45:55,287 [slca.py] => Task 7, Epoch 4/20 => Loss 0.096
2023-12-08 18:46:47,221 [slca.py] => Task 7, Epoch 5/20 => Loss 0.098, Train_accy 83.400, Test_accy 91.290
2023-12-08 18:47:11,076 [slca.py] => Task 7, Epoch 6/20 => Loss 0.094
2023-12-08 18:47:34,849 [slca.py] => Task 7, Epoch 7/20 => Loss 0.102
2023-12-08 18:47:58,767 [slca.py] => Task 7, Epoch 8/20 => Loss 0.100
2023-12-08 18:48:22,532 [slca.py] => Task 7, Epoch 9/20 => Loss 0.089
2023-12-08 18:49:14,420 [slca.py] => Task 7, Epoch 10/20 => Loss 0.090, Train_accy 84.220, Test_accy 91.160
2023-12-08 18:49:38,187 [slca.py] => Task 7, Epoch 11/20 => Loss 0.087
2023-12-08 18:50:01,844 [slca.py] => Task 7, Epoch 12/20 => Loss 0.086
2023-12-08 18:50:25,897 [slca.py] => Task 7, Epoch 13/20 => Loss 0.091
2023-12-08 18:50:49,685 [slca.py] => Task 7, Epoch 14/20 => Loss 0.079
2023-12-08 18:51:41,755 [slca.py] => Task 7, Epoch 15/20 => Loss 0.079, Train_accy 85.460, Test_accy 91.140
2023-12-08 18:52:05,655 [slca.py] => Task 7, Epoch 16/20 => Loss 0.095
2023-12-08 18:52:29,437 [slca.py] => Task 7, Epoch 17/20 => Loss 0.076
2023-12-08 18:52:53,330 [slca.py] => Task 7, Epoch 18/20 => Loss 0.082
2023-12-08 18:53:17,101 [slca.py] => Task 7, Epoch 19/20 => Loss 0.106
2023-12-08 18:54:09,154 [slca.py] => Task 7, Epoch 20/20 => Loss 0.081, Train_accy 85.500, Test_accy 91.220
2023-12-08 18:54:52,794 [slca.py] => CA Task 7 => Loss 0.061, Test_accy 92.450
2023-12-08 18:55:17,780 [slca.py] => CA Task 7 => Loss 0.041, Test_accy 92.520
2023-12-08 18:55:42,576 [slca.py] => CA Task 7 => Loss 0.034, Test_accy 92.620
2023-12-08 18:56:07,336 [slca.py] => CA Task 7 => Loss 0.032, Test_accy 92.700
2023-12-08 18:56:32,221 [slca.py] => CA Task 7 => Loss 0.028, Test_accy 92.620
2023-12-08 18:56:54,247 [slca.py] => Exemplar size: 0
2023-12-08 18:56:54,731 [trainer.py] => No NME accuracy.
2023-12-08 18:56:54,731 [trainer.py] => CNN: {'total': 92.62, '00-09': 88.5, '10-19': 89.1, '20-29': 94.4, '30-39': 89.0, '40-49': 95.5, '50-59': 96.2, '60-69': 92.6, '70-79': 95.7, 'old': 92.19, 'new': 95.7}
2023-12-08 18:56:54,731 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9, 95.48, 95.56, 94.68, 93.14, 92.62]
2023-12-08 18:56:54,731 [trainer.py] => CNN top1 avg: 95.56625
2023-12-08 18:56:54,731 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77, 99.72, 99.72, 99.68, 99.59, 99.55]

2023-12-08 18:56:54,732 [trainer.py] => All params: 85860176
2023-12-08 18:56:54,732 [trainer.py] => Trainable params: 85860176
2023-12-08 18:56:54,733 [slca.py] => Learning on 80-90
2023-12-08 18:57:18,544 [slca.py] => Task 8, Epoch 1/20 => Loss 0.469
2023-12-08 18:57:42,390 [slca.py] => Task 8, Epoch 2/20 => Loss 0.157
2023-12-08 18:58:06,130 [slca.py] => Task 8, Epoch 3/20 => Loss 0.172
2023-12-08 18:58:29,999 [slca.py] => Task 8, Epoch 4/20 => Loss 0.166
2023-12-08 18:59:24,126 [slca.py] => Task 8, Epoch 5/20 => Loss 0.162, Train_accy 74.140, Test_accy 89.960
2023-12-08 18:59:47,945 [slca.py] => Task 8, Epoch 6/20 => Loss 0.148
2023-12-08 19:00:11,677 [slca.py] => Task 8, Epoch 7/20 => Loss 0.141
2023-12-08 19:00:35,444 [slca.py] => Task 8, Epoch 8/20 => Loss 0.140
2023-12-08 19:00:59,337 [slca.py] => Task 8, Epoch 9/20 => Loss 0.141
2023-12-08 19:01:53,468 [slca.py] => Task 8, Epoch 10/20 => Loss 0.143, Train_accy 76.860, Test_accy 90.040
2023-12-08 19:02:17,310 [slca.py] => Task 8, Epoch 11/20 => Loss 0.147
2023-12-08 19:02:41,119 [slca.py] => Task 8, Epoch 12/20 => Loss 0.127
2023-12-08 19:03:05,011 [slca.py] => Task 8, Epoch 13/20 => Loss 0.119
2023-12-08 19:03:28,887 [slca.py] => Task 8, Epoch 14/20 => Loss 0.134
2023-12-08 19:04:23,004 [slca.py] => Task 8, Epoch 15/20 => Loss 0.110, Train_accy 78.580, Test_accy 90.300
2023-12-08 19:04:46,926 [slca.py] => Task 8, Epoch 16/20 => Loss 0.102
2023-12-08 19:05:10,756 [slca.py] => Task 8, Epoch 17/20 => Loss 0.117
2023-12-08 19:05:34,522 [slca.py] => Task 8, Epoch 18/20 => Loss 0.123
2023-12-08 19:05:58,389 [slca.py] => Task 8, Epoch 19/20 => Loss 0.122
2023-12-08 19:06:52,505 [slca.py] => Task 8, Epoch 20/20 => Loss 0.114, Train_accy 80.200, Test_accy 90.210
2023-12-08 19:07:39,743 [slca.py] => CA Task 8 => Loss 0.069, Test_accy 91.590
2023-12-08 19:08:07,740 [slca.py] => CA Task 8 => Loss 0.047, Test_accy 91.700
2023-12-08 19:08:35,637 [slca.py] => CA Task 8 => Loss 0.038, Test_accy 91.860
2023-12-08 19:09:03,737 [slca.py] => CA Task 8 => Loss 0.036, Test_accy 91.800
2023-12-08 19:09:31,687 [slca.py] => CA Task 8 => Loss 0.035, Test_accy 91.820
2023-12-08 19:09:56,387 [slca.py] => Exemplar size: 0
2023-12-08 19:09:56,874 [trainer.py] => No NME accuracy.
2023-12-08 19:09:56,874 [trainer.py] => CNN: {'total': 91.82, '00-09': 87.4, '10-19': 89.2, '20-29': 94.2, '30-39': 88.8, '40-49': 94.3, '50-59': 93.5, '60-69': 90.6, '70-79': 94.9, '80-89': 93.5, 'old': 91.61, 'new': 93.5}
2023-12-08 19:09:56,874 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9, 95.48, 95.56, 94.68, 93.14, 92.62, 91.82]
2023-12-08 19:09:56,874 [trainer.py] => CNN top1 avg: 95.14999999999999
2023-12-08 19:09:56,874 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77, 99.72, 99.72, 99.68, 99.59, 99.55, 99.43]

2023-12-08 19:09:56,875 [trainer.py] => All params: 85867866
2023-12-08 19:09:56,876 [trainer.py] => Trainable params: 85867866
2023-12-08 19:09:56,876 [slca.py] => Learning on 90-100
2023-12-08 19:10:20,509 [slca.py] => Task 9, Epoch 1/20 => Loss 0.436
2023-12-08 19:10:44,283 [slca.py] => Task 9, Epoch 2/20 => Loss 0.146
2023-12-08 19:11:08,163 [slca.py] => Task 9, Epoch 3/20 => Loss 0.142
2023-12-08 19:11:31,968 [slca.py] => Task 9, Epoch 4/20 => Loss 0.129
2023-12-08 19:12:28,051 [slca.py] => Task 9, Epoch 5/20 => Loss 0.149, Train_accy 81.220, Test_accy 89.410
2023-12-08 19:12:51,902 [slca.py] => Task 9, Epoch 6/20 => Loss 0.120
2023-12-08 19:13:15,702 [slca.py] => Task 9, Epoch 7/20 => Loss 0.108
2023-12-08 19:13:39,565 [slca.py] => Task 9, Epoch 8/20 => Loss 0.129
2023-12-08 19:14:03,384 [slca.py] => Task 9, Epoch 9/20 => Loss 0.113
2023-12-08 19:14:59,212 [slca.py] => Task 9, Epoch 10/20 => Loss 0.114, Train_accy 82.440, Test_accy 89.710
2023-12-08 19:15:23,005 [slca.py] => Task 9, Epoch 11/20 => Loss 0.120
2023-12-08 19:15:46,852 [slca.py] => Task 9, Epoch 12/20 => Loss 0.139
2023-12-08 19:16:10,566 [slca.py] => Task 9, Epoch 13/20 => Loss 0.120
2023-12-08 19:16:34,581 [slca.py] => Task 9, Epoch 14/20 => Loss 0.098
2023-12-08 19:17:30,639 [slca.py] => Task 9, Epoch 15/20 => Loss 0.114, Train_accy 84.180, Test_accy 89.720
2023-12-08 19:17:54,430 [slca.py] => Task 9, Epoch 16/20 => Loss 0.107
2023-12-08 19:18:18,228 [slca.py] => Task 9, Epoch 17/20 => Loss 0.093
2023-12-08 19:18:42,116 [slca.py] => Task 9, Epoch 18/20 => Loss 0.109
2023-12-08 19:19:05,918 [slca.py] => Task 9, Epoch 19/20 => Loss 0.093
2023-12-08 19:20:02,003 [slca.py] => Task 9, Epoch 20/20 => Loss 0.096, Train_accy 85.340, Test_accy 89.670
2023-12-08 19:20:52,328 [slca.py] => CA Task 9 => Loss 0.073, Test_accy 91.060
2023-12-08 19:21:23,374 [slca.py] => CA Task 9 => Loss 0.051, Test_accy 90.930
2023-12-08 19:21:54,492 [slca.py] => CA Task 9 => Loss 0.038, Test_accy 91.210
2023-12-08 19:22:25,335 [slca.py] => CA Task 9 => Loss 0.036, Test_accy 90.980
2023-12-08 19:22:56,140 [slca.py] => CA Task 9 => Loss 0.034, Test_accy 90.980
2023-12-08 19:23:23,762 [slca.py] => Exemplar size: 0
2023-12-08 19:23:24,247 [trainer.py] => No NME accuracy.
2023-12-08 19:23:24,248 [trainer.py] => CNN: {'total': 90.98, '00-09': 83.8, '10-19': 89.9, '20-29': 90.3, '30-39': 85.7, '40-49': 93.5, '50-59': 93.0, '60-69': 90.3, '70-79': 94.8, '80-89': 92.2, '90-99': 96.3, 'old': 90.39, 'new': 96.3}
2023-12-08 19:23:24,248 [trainer.py] => CNN top1 curve: [99.2, 97.95, 95.9, 95.48, 95.56, 94.68, 93.14, 92.62, 91.82, 90.98]
2023-12-08 19:23:24,248 [trainer.py] => CNN top1 avg: 94.73299999999999
2023-12-08 19:23:24,248 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.77, 99.72, 99.72, 99.68, 99.59, 99.55, 99.43, 99.29]

2023-12-08 19:23:24,272 [trainer.py] => config: /home/rohitk/SLCA/exps/slca_cifar.json
2023-12-08 19:23:24,273 [trainer.py] => test_only: False
2023-12-08 19:23:24,273 [trainer.py] => prefix: reproduce
2023-12-08 19:23:24,273 [trainer.py] => dataset: cifar100_224
2023-12-08 19:23:24,273 [trainer.py] => memory_size: 0
2023-12-08 19:23:24,273 [trainer.py] => memory_per_class: 0
2023-12-08 19:23:24,273 [trainer.py] => fixed_memory: False
2023-12-08 19:23:24,273 [trainer.py] => shuffle: True
2023-12-08 19:23:24,273 [trainer.py] => init_cls: 10
2023-12-08 19:23:24,273 [trainer.py] => increment: 10
2023-12-08 19:23:24,273 [trainer.py] => model_name: slca_cifar
2023-12-08 19:23:24,273 [trainer.py] => model_postfix: 20e
2023-12-08 19:23:24,273 [trainer.py] => convnet_type: vit-b-p16
2023-12-08 19:23:24,273 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1)]
2023-12-08 19:23:24,273 [trainer.py] => seed: 1997
2023-12-08 19:23:24,273 [trainer.py] => epochs: 20
2023-12-08 19:23:24,273 [trainer.py] => ca_epochs: 5
2023-12-08 19:23:24,273 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-08 19:23:24,273 [trainer.py] => milestones: [18]
2023-12-08 19:23:24,273 [trainer.py] => run_id: 2
2023-12-08 19:23:25,823 [data_manager.py] => [88, 77, 36, 54, 92, 4, 11, 24, 71, 31, 23, 99, 98, 91, 3, 7, 50, 26, 61, 90, 44, 35, 46, 32, 62, 95, 63, 58, 12, 74, 79, 47, 6, 67, 42, 78, 51, 65, 30, 2, 76, 53, 56, 72, 60, 37, 96, 97, 40, 1, 82, 83, 70, 64, 17, 15, 52, 94, 10, 43, 34, 20, 59, 25, 68, 89, 33, 87, 69, 27, 9, 39, 93, 73, 22, 84, 5, 75, 55, 19, 45, 21, 86, 66, 85, 0, 49, 13, 28, 81, 80, 8, 41, 38, 18, 14, 16, 48, 57, 29]
2023-12-08 19:23:27,790 [trainer.py] => All params: 85798656
2023-12-08 19:23:27,791 [trainer.py] => Trainable params: 85798656
2023-12-08 19:23:27,791 [slca.py] => Learning on 0-10
2023-12-08 19:23:51,464 [slca.py] => Task 0, Epoch 1/20 => Loss 0.718
2023-12-08 19:24:15,313 [slca.py] => Task 0, Epoch 2/20 => Loss 0.193
2023-12-08 19:24:39,220 [slca.py] => Task 0, Epoch 3/20 => Loss 0.153
2023-12-08 19:25:02,978 [slca.py] => Task 0, Epoch 4/20 => Loss 0.138
2023-12-08 19:25:40,979 [slca.py] => Task 0, Epoch 5/20 => Loss 0.140, Train_accy 95.500, Test_accy 98.400
2023-12-08 19:26:04,794 [slca.py] => Task 0, Epoch 6/20 => Loss 0.135
2023-12-08 19:26:28,822 [slca.py] => Task 0, Epoch 7/20 => Loss 0.136
2023-12-08 19:26:52,609 [slca.py] => Task 0, Epoch 8/20 => Loss 0.122
2023-12-08 19:27:16,532 [slca.py] => Task 0, Epoch 9/20 => Loss 0.118
2023-12-08 19:27:54,560 [slca.py] => Task 0, Epoch 10/20 => Loss 0.119, Train_accy 96.300, Test_accy 98.600
2023-12-08 19:28:18,399 [slca.py] => Task 0, Epoch 11/20 => Loss 0.117
2023-12-08 19:28:42,277 [slca.py] => Task 0, Epoch 12/20 => Loss 0.109
2023-12-08 19:29:06,184 [slca.py] => Task 0, Epoch 13/20 => Loss 0.121
2023-12-08 19:29:30,031 [slca.py] => Task 0, Epoch 14/20 => Loss 0.113
2023-12-08 19:30:08,173 [slca.py] => Task 0, Epoch 15/20 => Loss 0.104, Train_accy 96.920, Test_accy 98.700
2023-12-08 19:30:32,129 [slca.py] => Task 0, Epoch 16/20 => Loss 0.098
2023-12-08 19:30:56,022 [slca.py] => Task 0, Epoch 17/20 => Loss 0.098
2023-12-08 19:31:19,890 [slca.py] => Task 0, Epoch 18/20 => Loss 0.086
2023-12-08 19:31:43,712 [slca.py] => Task 0, Epoch 19/20 => Loss 0.095
2023-12-08 19:32:21,784 [slca.py] => Task 0, Epoch 20/20 => Loss 0.085, Train_accy 96.620, Test_accy 99.000
2023-12-08 19:32:44,787 [slca.py] => Exemplar size: 0
2023-12-08 19:32:45,280 [trainer.py] => No NME accuracy.
2023-12-08 19:32:45,280 [trainer.py] => CNN: {'total': 99.0, '00-09': 99.0, 'old': 0, 'new': 99.0}
2023-12-08 19:32:45,280 [trainer.py] => CNN top1 curve: [99.0]
2023-12-08 19:32:45,280 [trainer.py] => CNN top1 avg: 99.0
2023-12-08 19:32:45,280 [trainer.py] => CNN top5 curve: [100.0]

2023-12-08 19:32:45,281 [trainer.py] => All params: 85806346
2023-12-08 19:32:45,281 [trainer.py] => Trainable params: 85806346
2023-12-08 19:32:45,282 [slca.py] => Learning on 10-20
2023-12-08 19:33:09,094 [slca.py] => Task 1, Epoch 1/20 => Loss 0.527
2023-12-08 19:33:32,971 [slca.py] => Task 1, Epoch 2/20 => Loss 0.172
2023-12-08 19:33:56,848 [slca.py] => Task 1, Epoch 3/20 => Loss 0.158
2023-12-08 19:34:20,718 [slca.py] => Task 1, Epoch 4/20 => Loss 0.165
2023-12-08 19:35:00,738 [slca.py] => Task 1, Epoch 5/20 => Loss 0.159, Train_accy 84.300, Test_accy 94.500
2023-12-08 19:35:24,596 [slca.py] => Task 1, Epoch 6/20 => Loss 0.154
2023-12-08 19:35:48,424 [slca.py] => Task 1, Epoch 7/20 => Loss 0.132
2023-12-08 19:36:12,274 [slca.py] => Task 1, Epoch 8/20 => Loss 0.123
2023-12-08 19:36:36,191 [slca.py] => Task 1, Epoch 9/20 => Loss 0.143
2023-12-08 19:37:16,334 [slca.py] => Task 1, Epoch 10/20 => Loss 0.134, Train_accy 86.360, Test_accy 94.750
2023-12-08 19:37:40,231 [slca.py] => Task 1, Epoch 11/20 => Loss 0.133
2023-12-08 19:38:04,160 [slca.py] => Task 1, Epoch 12/20 => Loss 0.139
2023-12-08 19:38:27,960 [slca.py] => Task 1, Epoch 13/20 => Loss 0.136
2023-12-08 19:38:51,852 [slca.py] => Task 1, Epoch 14/20 => Loss 0.120
2023-12-08 19:39:31,987 [slca.py] => Task 1, Epoch 15/20 => Loss 0.130, Train_accy 85.820, Test_accy 94.300
2023-12-08 19:39:56,095 [slca.py] => Task 1, Epoch 16/20 => Loss 0.111
2023-12-08 19:40:20,021 [slca.py] => Task 1, Epoch 17/20 => Loss 0.112
2023-12-08 19:40:43,942 [slca.py] => Task 1, Epoch 18/20 => Loss 0.109
2023-12-08 19:41:07,753 [slca.py] => Task 1, Epoch 19/20 => Loss 0.115
2023-12-08 19:41:47,989 [slca.py] => Task 1, Epoch 20/20 => Loss 0.117, Train_accy 87.660, Test_accy 95.050
2023-12-08 19:42:14,285 [slca.py] => CA Task 1 => Loss 0.054, Test_accy 96.200
2023-12-08 19:42:21,203 [slca.py] => CA Task 1 => Loss 0.035, Test_accy 96.450
2023-12-08 19:42:28,017 [slca.py] => CA Task 1 => Loss 0.031, Test_accy 96.550
2023-12-08 19:42:34,946 [slca.py] => CA Task 1 => Loss 0.028, Test_accy 96.400
2023-12-08 19:42:41,809 [slca.py] => CA Task 1 => Loss 0.028, Test_accy 96.400
2023-12-08 19:42:47,955 [slca.py] => Exemplar size: 0
2023-12-08 19:42:48,440 [trainer.py] => No NME accuracy.
2023-12-08 19:42:48,440 [trainer.py] => CNN: {'total': 96.4, '00-09': 95.2, '10-19': 97.6, 'old': 95.2, 'new': 97.6}
2023-12-08 19:42:48,441 [trainer.py] => CNN top1 curve: [99.0, 96.4]
2023-12-08 19:42:48,441 [trainer.py] => CNN top1 avg: 97.7
2023-12-08 19:42:48,441 [trainer.py] => CNN top5 curve: [100.0, 99.95]

2023-12-08 19:42:48,441 [trainer.py] => All params: 85814036
2023-12-08 19:42:48,442 [trainer.py] => Trainable params: 85814036
2023-12-08 19:42:48,443 [slca.py] => Learning on 20-30
2023-12-08 19:43:12,295 [slca.py] => Task 2, Epoch 1/20 => Loss 0.512
2023-12-08 19:43:36,119 [slca.py] => Task 2, Epoch 2/20 => Loss 0.188
2023-12-08 19:43:59,928 [slca.py] => Task 2, Epoch 3/20 => Loss 0.184
2023-12-08 19:44:23,854 [slca.py] => Task 2, Epoch 4/20 => Loss 0.168
2023-12-08 19:45:05,960 [slca.py] => Task 2, Epoch 5/20 => Loss 0.163, Train_accy 81.900, Test_accy 91.030
2023-12-08 19:45:29,765 [slca.py] => Task 2, Epoch 6/20 => Loss 0.160
2023-12-08 19:45:53,617 [slca.py] => Task 2, Epoch 7/20 => Loss 0.152
2023-12-08 19:46:17,469 [slca.py] => Task 2, Epoch 8/20 => Loss 0.157
2023-12-08 19:46:41,324 [slca.py] => Task 2, Epoch 9/20 => Loss 0.139
2023-12-08 19:47:23,366 [slca.py] => Task 2, Epoch 10/20 => Loss 0.148, Train_accy 84.760, Test_accy 92.370
2023-12-08 19:47:47,254 [slca.py] => Task 2, Epoch 11/20 => Loss 0.144
2023-12-08 19:48:11,100 [slca.py] => Task 2, Epoch 12/20 => Loss 0.135
2023-12-08 19:48:34,887 [slca.py] => Task 2, Epoch 13/20 => Loss 0.143
2023-12-08 19:48:58,682 [slca.py] => Task 2, Epoch 14/20 => Loss 0.136
2023-12-08 19:49:40,841 [slca.py] => Task 2, Epoch 15/20 => Loss 0.135, Train_accy 86.780, Test_accy 92.170
2023-12-08 19:50:04,592 [slca.py] => Task 2, Epoch 16/20 => Loss 0.138
2023-12-08 19:50:28,515 [slca.py] => Task 2, Epoch 17/20 => Loss 0.119
2023-12-08 19:50:52,385 [slca.py] => Task 2, Epoch 18/20 => Loss 0.111
2023-12-08 19:51:16,211 [slca.py] => Task 2, Epoch 19/20 => Loss 0.139
2023-12-08 19:51:58,267 [slca.py] => Task 2, Epoch 20/20 => Loss 0.122, Train_accy 87.860, Test_accy 92.430
2023-12-08 19:52:27,490 [slca.py] => CA Task 2 => Loss 0.078, Test_accy 93.630
2023-12-08 19:52:37,428 [slca.py] => CA Task 2 => Loss 0.058, Test_accy 93.630
2023-12-08 19:52:47,132 [slca.py] => CA Task 2 => Loss 0.044, Test_accy 93.670
2023-12-08 19:52:56,990 [slca.py] => CA Task 2 => Loss 0.040, Test_accy 93.500
2023-12-08 19:53:07,013 [slca.py] => CA Task 2 => Loss 0.039, Test_accy 93.500
2023-12-08 19:53:15,729 [slca.py] => Exemplar size: 0
2023-12-08 19:53:16,244 [trainer.py] => No NME accuracy.
2023-12-08 19:53:16,244 [trainer.py] => CNN: {'total': 93.5, '00-09': 90.8, '10-19': 94.1, '20-29': 95.6, 'old': 92.45, 'new': 95.6}
2023-12-08 19:53:16,244 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5]
2023-12-08 19:53:16,244 [trainer.py] => CNN top1 avg: 96.3
2023-12-08 19:53:16,244 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73]

2023-12-08 19:53:16,245 [trainer.py] => All params: 85821726
2023-12-08 19:53:16,245 [trainer.py] => Trainable params: 85821726
2023-12-08 19:53:16,246 [slca.py] => Learning on 30-40
2023-12-08 19:53:39,764 [slca.py] => Task 3, Epoch 1/20 => Loss 0.437
2023-12-08 19:54:03,290 [slca.py] => Task 3, Epoch 2/20 => Loss 0.148
2023-12-08 19:54:26,827 [slca.py] => Task 3, Epoch 3/20 => Loss 0.130
2023-12-08 19:54:50,573 [slca.py] => Task 3, Epoch 4/20 => Loss 0.129
2023-12-08 19:55:34,032 [slca.py] => Task 3, Epoch 5/20 => Loss 0.124, Train_accy 85.560, Test_accy 91.680
2023-12-08 19:55:57,612 [slca.py] => Task 3, Epoch 6/20 => Loss 0.118
2023-12-08 19:56:21,127 [slca.py] => Task 3, Epoch 7/20 => Loss 0.132
2023-12-08 19:56:44,854 [slca.py] => Task 3, Epoch 8/20 => Loss 0.123
2023-12-08 19:57:08,597 [slca.py] => Task 3, Epoch 9/20 => Loss 0.103
2023-12-08 19:57:52,076 [slca.py] => Task 3, Epoch 10/20 => Loss 0.107, Train_accy 86.880, Test_accy 91.900
2023-12-08 19:58:15,755 [slca.py] => Task 3, Epoch 11/20 => Loss 0.111
2023-12-08 19:58:39,262 [slca.py] => Task 3, Epoch 12/20 => Loss 0.103
2023-12-08 19:59:02,872 [slca.py] => Task 3, Epoch 13/20 => Loss 0.100
2023-12-08 19:59:26,560 [slca.py] => Task 3, Epoch 14/20 => Loss 0.107
2023-12-08 20:00:09,988 [slca.py] => Task 3, Epoch 15/20 => Loss 0.096, Train_accy 88.100, Test_accy 91.850
2023-12-08 20:00:33,705 [slca.py] => Task 3, Epoch 16/20 => Loss 0.103
2023-12-08 20:00:57,390 [slca.py] => Task 3, Epoch 17/20 => Loss 0.095
2023-12-08 20:01:20,905 [slca.py] => Task 3, Epoch 18/20 => Loss 0.096
2023-12-08 20:01:44,684 [slca.py] => Task 3, Epoch 19/20 => Loss 0.096
2023-12-08 20:02:28,400 [slca.py] => Task 3, Epoch 20/20 => Loss 0.097, Train_accy 88.760, Test_accy 91.850
2023-12-08 20:03:00,618 [slca.py] => CA Task 3 => Loss 0.083, Test_accy 92.800
2023-12-08 20:03:13,685 [slca.py] => CA Task 3 => Loss 0.058, Test_accy 92.920
2023-12-08 20:03:26,708 [slca.py] => CA Task 3 => Loss 0.048, Test_accy 93.080
2023-12-08 20:03:39,802 [slca.py] => CA Task 3 => Loss 0.042, Test_accy 93.250
2023-12-08 20:03:52,718 [slca.py] => CA Task 3 => Loss 0.043, Test_accy 93.320
2023-12-08 20:04:04,219 [slca.py] => Exemplar size: 0
2023-12-08 20:04:04,715 [trainer.py] => No NME accuracy.
2023-12-08 20:04:04,715 [trainer.py] => CNN: {'total': 93.32, '00-09': 88.5, '10-19': 94.1, '20-29': 93.8, '30-39': 96.9, 'old': 92.13, 'new': 96.9}
2023-12-08 20:04:04,715 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5, 93.32]
2023-12-08 20:04:04,715 [trainer.py] => CNN top1 avg: 95.55499999999999
2023-12-08 20:04:04,715 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73, 99.75]

2023-12-08 20:04:04,716 [trainer.py] => All params: 85829416
2023-12-08 20:04:04,716 [trainer.py] => Trainable params: 85829416
2023-12-08 20:04:04,717 [slca.py] => Learning on 40-50
2023-12-08 20:04:28,599 [slca.py] => Task 4, Epoch 1/20 => Loss 0.464
2023-12-08 20:04:52,547 [slca.py] => Task 4, Epoch 2/20 => Loss 0.156
2023-12-08 20:05:16,541 [slca.py] => Task 4, Epoch 3/20 => Loss 0.144
2023-12-08 20:05:40,462 [slca.py] => Task 4, Epoch 4/20 => Loss 0.144
2023-12-08 20:06:26,614 [slca.py] => Task 4, Epoch 5/20 => Loss 0.125, Train_accy 84.100, Test_accy 91.620
2023-12-08 20:06:50,521 [slca.py] => Task 4, Epoch 6/20 => Loss 0.118
2023-12-08 20:07:14,372 [slca.py] => Task 4, Epoch 7/20 => Loss 0.102
2023-12-08 20:07:38,331 [slca.py] => Task 4, Epoch 8/20 => Loss 0.113
2023-12-08 20:08:02,306 [slca.py] => Task 4, Epoch 9/20 => Loss 0.103
2023-12-08 20:08:48,594 [slca.py] => Task 4, Epoch 10/20 => Loss 0.114, Train_accy 86.260, Test_accy 91.860
2023-12-08 20:09:12,658 [slca.py] => Task 4, Epoch 11/20 => Loss 0.129
2023-12-08 20:09:36,629 [slca.py] => Task 4, Epoch 12/20 => Loss 0.112
2023-12-08 20:10:00,424 [slca.py] => Task 4, Epoch 13/20 => Loss 0.105
2023-12-08 20:10:24,092 [slca.py] => Task 4, Epoch 14/20 => Loss 0.117
2023-12-08 20:11:09,589 [slca.py] => Task 4, Epoch 15/20 => Loss 0.097, Train_accy 87.700, Test_accy 92.020
2023-12-08 20:11:33,171 [slca.py] => Task 4, Epoch 16/20 => Loss 0.104
2023-12-08 20:11:56,832 [slca.py] => Task 4, Epoch 17/20 => Loss 0.101
2023-12-08 20:12:20,360 [slca.py] => Task 4, Epoch 18/20 => Loss 0.098
2023-12-08 20:12:43,944 [slca.py] => Task 4, Epoch 19/20 => Loss 0.097
2023-12-08 20:13:29,300 [slca.py] => Task 4, Epoch 20/20 => Loss 0.095, Train_accy 86.980, Test_accy 92.040
2023-12-08 20:14:04,424 [slca.py] => CA Task 4 => Loss 0.082, Test_accy 92.540
2023-12-08 20:14:20,237 [slca.py] => CA Task 4 => Loss 0.055, Test_accy 93.180
2023-12-08 20:14:35,979 [slca.py] => CA Task 4 => Loss 0.048, Test_accy 92.920
2023-12-08 20:14:51,877 [slca.py] => CA Task 4 => Loss 0.043, Test_accy 92.920
2023-12-08 20:15:07,804 [slca.py] => CA Task 4 => Loss 0.036, Test_accy 93.040
2023-12-08 20:15:21,908 [slca.py] => Exemplar size: 0
2023-12-08 20:15:22,444 [trainer.py] => No NME accuracy.
2023-12-08 20:15:22,444 [trainer.py] => CNN: {'total': 93.04, '00-09': 87.4, '10-19': 93.1, '20-29': 93.0, '30-39': 94.6, '40-49': 97.1, 'old': 92.02, 'new': 97.1}
2023-12-08 20:15:22,444 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5, 93.32, 93.04]
2023-12-08 20:15:22,445 [trainer.py] => CNN top1 avg: 95.05199999999999
2023-12-08 20:15:22,445 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73, 99.75, 99.76]

2023-12-08 20:15:22,445 [trainer.py] => All params: 85837106
2023-12-08 20:15:22,446 [trainer.py] => Trainable params: 85837106
2023-12-08 20:15:22,447 [slca.py] => Learning on 50-60
2023-12-08 20:15:46,035 [slca.py] => Task 5, Epoch 1/20 => Loss 0.379
2023-12-08 20:16:09,701 [slca.py] => Task 5, Epoch 2/20 => Loss 0.089
2023-12-08 20:16:33,217 [slca.py] => Task 5, Epoch 3/20 => Loss 0.089
2023-12-08 20:16:56,784 [slca.py] => Task 5, Epoch 4/20 => Loss 0.102
2023-12-08 20:17:44,469 [slca.py] => Task 5, Epoch 5/20 => Loss 0.089, Train_accy 88.340, Test_accy 90.880
2023-12-08 20:18:08,043 [slca.py] => Task 5, Epoch 6/20 => Loss 0.076
2023-12-08 20:18:31,726 [slca.py] => Task 5, Epoch 7/20 => Loss 0.088
2023-12-08 20:18:55,244 [slca.py] => Task 5, Epoch 8/20 => Loss 0.076
2023-12-08 20:19:18,915 [slca.py] => Task 5, Epoch 9/20 => Loss 0.084
2023-12-08 20:20:06,641 [slca.py] => Task 5, Epoch 10/20 => Loss 0.081, Train_accy 88.880, Test_accy 90.870
2023-12-08 20:20:30,490 [slca.py] => Task 5, Epoch 11/20 => Loss 0.080
2023-12-08 20:20:54,403 [slca.py] => Task 5, Epoch 12/20 => Loss 0.083
2023-12-08 20:21:18,263 [slca.py] => Task 5, Epoch 13/20 => Loss 0.077
2023-12-08 20:21:42,086 [slca.py] => Task 5, Epoch 14/20 => Loss 0.069
2023-12-08 20:22:30,378 [slca.py] => Task 5, Epoch 15/20 => Loss 0.079, Train_accy 88.940, Test_accy 91.080
2023-12-08 20:22:54,190 [slca.py] => Task 5, Epoch 16/20 => Loss 0.070
2023-12-08 20:23:18,078 [slca.py] => Task 5, Epoch 17/20 => Loss 0.073
2023-12-08 20:23:41,922 [slca.py] => Task 5, Epoch 18/20 => Loss 0.059
2023-12-08 20:24:05,889 [slca.py] => Task 5, Epoch 19/20 => Loss 0.069
2023-12-08 20:24:54,080 [slca.py] => Task 5, Epoch 20/20 => Loss 0.062, Train_accy 90.180, Test_accy 91.080
2023-12-08 20:25:32,676 [slca.py] => CA Task 5 => Loss 0.085, Test_accy 91.900
2023-12-08 20:25:51,633 [slca.py] => CA Task 5 => Loss 0.058, Test_accy 91.720
2023-12-08 20:26:10,792 [slca.py] => CA Task 5 => Loss 0.051, Test_accy 91.850
2023-12-08 20:26:29,658 [slca.py] => CA Task 5 => Loss 0.046, Test_accy 91.750
2023-12-08 20:26:48,774 [slca.py] => CA Task 5 => Loss 0.044, Test_accy 91.800
2023-12-08 20:27:05,621 [slca.py] => Exemplar size: 0
2023-12-08 20:27:06,115 [trainer.py] => No NME accuracy.
2023-12-08 20:27:06,116 [trainer.py] => CNN: {'total': 91.8, '00-09': 86.7, '10-19': 90.0, '20-29': 91.8, '30-39': 91.3, '40-49': 95.4, '50-59': 95.6, 'old': 91.04, 'new': 95.6}
2023-12-08 20:27:06,116 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5, 93.32, 93.04, 91.8]
2023-12-08 20:27:06,116 [trainer.py] => CNN top1 avg: 94.50999999999999
2023-12-08 20:27:06,116 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73, 99.75, 99.76, 99.62]

2023-12-08 20:27:06,117 [trainer.py] => All params: 85844796
2023-12-08 20:27:06,117 [trainer.py] => Trainable params: 85844796
2023-12-08 20:27:06,118 [slca.py] => Learning on 60-70
2023-12-08 20:27:29,966 [slca.py] => Task 6, Epoch 1/20 => Loss 0.510
2023-12-08 20:27:53,785 [slca.py] => Task 6, Epoch 2/20 => Loss 0.239
2023-12-08 20:28:17,516 [slca.py] => Task 6, Epoch 3/20 => Loss 0.178
2023-12-08 20:28:41,328 [slca.py] => Task 6, Epoch 4/20 => Loss 0.183
2023-12-08 20:29:31,346 [slca.py] => Task 6, Epoch 5/20 => Loss 0.167, Train_accy 82.860, Test_accy 90.990
2023-12-08 20:29:55,103 [slca.py] => Task 6, Epoch 6/20 => Loss 0.167
2023-12-08 20:30:18,836 [slca.py] => Task 6, Epoch 7/20 => Loss 0.153
2023-12-08 20:30:42,406 [slca.py] => Task 6, Epoch 8/20 => Loss 0.139
2023-12-08 20:31:05,888 [slca.py] => Task 6, Epoch 9/20 => Loss 0.141
2023-12-08 20:31:55,602 [slca.py] => Task 6, Epoch 10/20 => Loss 0.135, Train_accy 84.660, Test_accy 91.230
2023-12-08 20:32:19,133 [slca.py] => Task 6, Epoch 11/20 => Loss 0.134
2023-12-08 20:32:42,767 [slca.py] => Task 6, Epoch 12/20 => Loss 0.137
2023-12-08 20:33:06,449 [slca.py] => Task 6, Epoch 13/20 => Loss 0.140
2023-12-08 20:33:30,136 [slca.py] => Task 6, Epoch 14/20 => Loss 0.128
2023-12-08 20:34:19,905 [slca.py] => Task 6, Epoch 15/20 => Loss 0.123, Train_accy 87.580, Test_accy 91.290
2023-12-08 20:34:43,439 [slca.py] => Task 6, Epoch 16/20 => Loss 0.141
2023-12-08 20:35:07,162 [slca.py] => Task 6, Epoch 17/20 => Loss 0.138
2023-12-08 20:35:30,930 [slca.py] => Task 6, Epoch 18/20 => Loss 0.126
2023-12-08 20:35:54,730 [slca.py] => Task 6, Epoch 19/20 => Loss 0.123
2023-12-08 20:36:44,374 [slca.py] => Task 6, Epoch 20/20 => Loss 0.121, Train_accy 87.340, Test_accy 91.240
2023-12-08 20:37:25,483 [slca.py] => CA Task 6 => Loss 0.083, Test_accy 91.690
2023-12-08 20:37:47,214 [slca.py] => CA Task 6 => Loss 0.056, Test_accy 91.810
2023-12-08 20:38:08,869 [slca.py] => CA Task 6 => Loss 0.048, Test_accy 91.670
2023-12-08 20:38:30,415 [slca.py] => CA Task 6 => Loss 0.044, Test_accy 91.760
2023-12-08 20:38:51,977 [slca.py] => CA Task 6 => Loss 0.038, Test_accy 91.830
2023-12-08 20:39:11,120 [slca.py] => Exemplar size: 0
2023-12-08 20:39:11,658 [trainer.py] => No NME accuracy.
2023-12-08 20:39:11,658 [trainer.py] => CNN: {'total': 91.83, '00-09': 85.8, '10-19': 89.3, '20-29': 92.3, '30-39': 91.3, '40-49': 94.2, '50-59': 94.6, '60-69': 95.3, 'old': 91.25, 'new': 95.3}
2023-12-08 20:39:11,658 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5, 93.32, 93.04, 91.8, 91.83]
2023-12-08 20:39:11,659 [trainer.py] => CNN top1 avg: 94.12714285714286
2023-12-08 20:39:11,659 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73, 99.75, 99.76, 99.62, 99.51]

2023-12-08 20:39:11,659 [trainer.py] => All params: 85852486
2023-12-08 20:39:11,660 [trainer.py] => Trainable params: 85852486
2023-12-08 20:39:11,660 [slca.py] => Learning on 70-80
2023-12-08 20:39:35,230 [slca.py] => Task 7, Epoch 1/20 => Loss 0.469
2023-12-08 20:39:58,816 [slca.py] => Task 7, Epoch 2/20 => Loss 0.158
2023-12-08 20:40:22,478 [slca.py] => Task 7, Epoch 3/20 => Loss 0.154
2023-12-08 20:40:46,050 [slca.py] => Task 7, Epoch 4/20 => Loss 0.151
2023-12-08 20:41:37,591 [slca.py] => Task 7, Epoch 5/20 => Loss 0.151, Train_accy 82.940, Test_accy 91.090
2023-12-08 20:42:01,355 [slca.py] => Task 7, Epoch 6/20 => Loss 0.126
2023-12-08 20:42:24,825 [slca.py] => Task 7, Epoch 7/20 => Loss 0.117
2023-12-08 20:42:48,516 [slca.py] => Task 7, Epoch 8/20 => Loss 0.121
2023-12-08 20:43:12,175 [slca.py] => Task 7, Epoch 9/20 => Loss 0.135
2023-12-08 20:44:03,731 [slca.py] => Task 7, Epoch 10/20 => Loss 0.123, Train_accy 84.980, Test_accy 91.260
2023-12-08 20:44:27,228 [slca.py] => Task 7, Epoch 11/20 => Loss 0.128
2023-12-08 20:44:50,854 [slca.py] => Task 7, Epoch 12/20 => Loss 0.120
2023-12-08 20:45:14,581 [slca.py] => Task 7, Epoch 13/20 => Loss 0.141
2023-12-08 20:45:38,247 [slca.py] => Task 7, Epoch 14/20 => Loss 0.134
2023-12-08 20:46:29,555 [slca.py] => Task 7, Epoch 15/20 => Loss 0.121, Train_accy 85.280, Test_accy 91.060
2023-12-08 20:46:53,294 [slca.py] => Task 7, Epoch 16/20 => Loss 0.116
2023-12-08 20:47:17,096 [slca.py] => Task 7, Epoch 17/20 => Loss 0.120
2023-12-08 20:47:40,854 [slca.py] => Task 7, Epoch 18/20 => Loss 0.117
2023-12-08 20:48:04,493 [slca.py] => Task 7, Epoch 19/20 => Loss 0.104
2023-12-08 20:48:55,724 [slca.py] => Task 7, Epoch 20/20 => Loss 0.093, Train_accy 86.400, Test_accy 91.310
2023-12-08 20:49:39,956 [slca.py] => CA Task 7 => Loss 0.076, Test_accy 91.940
2023-12-08 20:50:04,601 [slca.py] => CA Task 7 => Loss 0.053, Test_accy 91.720
2023-12-08 20:50:29,257 [slca.py] => CA Task 7 => Loss 0.045, Test_accy 91.760
2023-12-08 20:50:53,963 [slca.py] => CA Task 7 => Loss 0.040, Test_accy 91.850
2023-12-08 20:51:18,341 [slca.py] => CA Task 7 => Loss 0.038, Test_accy 91.940
2023-12-08 20:51:40,257 [slca.py] => Exemplar size: 0
2023-12-08 20:51:40,770 [trainer.py] => No NME accuracy.
2023-12-08 20:51:40,770 [trainer.py] => CNN: {'total': 91.94, '00-09': 84.2, '10-19': 90.9, '20-29': 92.0, '30-39': 90.4, '40-49': 92.8, '50-59': 94.4, '60-69': 94.8, '70-79': 96.0, 'old': 91.36, 'new': 96.0}
2023-12-08 20:51:40,770 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5, 93.32, 93.04, 91.8, 91.83, 91.94]
2023-12-08 20:51:40,770 [trainer.py] => CNN top1 avg: 93.85375
2023-12-08 20:51:40,770 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73, 99.75, 99.76, 99.62, 99.51, 99.41]

2023-12-08 20:51:40,771 [trainer.py] => All params: 85860176
2023-12-08 20:51:40,772 [trainer.py] => Trainable params: 85860176
2023-12-08 20:51:40,772 [slca.py] => Learning on 80-90
2023-12-08 20:52:04,314 [slca.py] => Task 8, Epoch 1/20 => Loss 0.429
2023-12-08 20:52:27,939 [slca.py] => Task 8, Epoch 2/20 => Loss 0.149
2023-12-08 20:52:51,536 [slca.py] => Task 8, Epoch 3/20 => Loss 0.144
2023-12-08 20:53:15,153 [slca.py] => Task 8, Epoch 4/20 => Loss 0.130
2023-12-08 20:54:08,629 [slca.py] => Task 8, Epoch 5/20 => Loss 0.117, Train_accy 80.580, Test_accy 90.930
2023-12-08 20:54:32,399 [slca.py] => Task 8, Epoch 6/20 => Loss 0.116
2023-12-08 20:54:56,037 [slca.py] => Task 8, Epoch 7/20 => Loss 0.117
2023-12-08 20:55:19,659 [slca.py] => Task 8, Epoch 8/20 => Loss 0.116
2023-12-08 20:55:43,230 [slca.py] => Task 8, Epoch 9/20 => Loss 0.114
2023-12-08 20:56:36,405 [slca.py] => Task 8, Epoch 10/20 => Loss 0.121, Train_accy 83.780, Test_accy 91.180
2023-12-08 20:56:59,934 [slca.py] => Task 8, Epoch 11/20 => Loss 0.131
2023-12-08 20:57:23,548 [slca.py] => Task 8, Epoch 12/20 => Loss 0.107
2023-12-08 20:57:47,376 [slca.py] => Task 8, Epoch 13/20 => Loss 0.109
2023-12-08 20:58:11,386 [slca.py] => Task 8, Epoch 14/20 => Loss 0.112
2023-12-08 20:59:05,841 [slca.py] => Task 8, Epoch 15/20 => Loss 0.116, Train_accy 84.700, Test_accy 91.080
2023-12-08 20:59:29,614 [slca.py] => Task 8, Epoch 16/20 => Loss 0.105
2023-12-08 20:59:53,629 [slca.py] => Task 8, Epoch 17/20 => Loss 0.121
2023-12-08 21:00:17,500 [slca.py] => Task 8, Epoch 18/20 => Loss 0.109
2023-12-08 21:00:41,392 [slca.py] => Task 8, Epoch 19/20 => Loss 0.113
2023-12-08 21:01:35,842 [slca.py] => Task 8, Epoch 20/20 => Loss 0.092, Train_accy 86.440, Test_accy 90.970
2023-12-08 21:02:23,176 [slca.py] => CA Task 8 => Loss 0.075, Test_accy 91.280
2023-12-08 21:02:51,356 [slca.py] => CA Task 8 => Loss 0.055, Test_accy 91.070
2023-12-08 21:03:19,366 [slca.py] => CA Task 8 => Loss 0.041, Test_accy 91.070
2023-12-08 21:03:47,247 [slca.py] => CA Task 8 => Loss 0.038, Test_accy 91.220
2023-12-08 21:04:15,112 [slca.py] => CA Task 8 => Loss 0.039, Test_accy 91.180
2023-12-08 21:04:40,227 [slca.py] => Exemplar size: 0
2023-12-08 21:04:40,740 [trainer.py] => No NME accuracy.
2023-12-08 21:04:40,740 [trainer.py] => CNN: {'total': 91.18, '00-09': 82.4, '10-19': 86.4, '20-29': 92.3, '30-39': 89.9, '40-49': 91.7, '50-59': 91.9, '60-69': 94.2, '70-79': 95.7, '80-89': 96.1, 'old': 90.56, 'new': 96.1}
2023-12-08 21:04:40,740 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5, 93.32, 93.04, 91.8, 91.83, 91.94, 91.18]
2023-12-08 21:04:40,740 [trainer.py] => CNN top1 avg: 93.55666666666667
2023-12-08 21:04:40,740 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73, 99.75, 99.76, 99.62, 99.51, 99.41, 99.32]

2023-12-08 21:04:40,741 [trainer.py] => All params: 85867866
2023-12-08 21:04:40,741 [trainer.py] => Trainable params: 85867866
2023-12-08 21:04:40,742 [slca.py] => Learning on 90-100
2023-12-08 21:05:04,484 [slca.py] => Task 9, Epoch 1/20 => Loss 0.476
2023-12-08 21:05:28,378 [slca.py] => Task 9, Epoch 2/20 => Loss 0.188
2023-12-08 21:05:52,344 [slca.py] => Task 9, Epoch 3/20 => Loss 0.146
2023-12-08 21:06:16,250 [slca.py] => Task 9, Epoch 4/20 => Loss 0.146
2023-12-08 21:07:12,696 [slca.py] => Task 9, Epoch 5/20 => Loss 0.138, Train_accy 83.420, Test_accy 91.340
2023-12-08 21:07:36,676 [slca.py] => Task 9, Epoch 6/20 => Loss 0.131
2023-12-08 21:08:00,651 [slca.py] => Task 9, Epoch 7/20 => Loss 0.122
2023-12-08 21:08:24,644 [slca.py] => Task 9, Epoch 8/20 => Loss 0.137
2023-12-08 21:08:48,528 [slca.py] => Task 9, Epoch 9/20 => Loss 0.126
2023-12-08 21:09:45,025 [slca.py] => Task 9, Epoch 10/20 => Loss 0.126, Train_accy 85.020, Test_accy 91.250
2023-12-08 21:10:08,782 [slca.py] => Task 9, Epoch 11/20 => Loss 0.138
2023-12-08 21:10:32,731 [slca.py] => Task 9, Epoch 12/20 => Loss 0.140
2023-12-08 21:10:56,497 [slca.py] => Task 9, Epoch 13/20 => Loss 0.121
2023-12-08 21:11:20,398 [slca.py] => Task 9, Epoch 14/20 => Loss 0.110
2023-12-08 21:12:16,670 [slca.py] => Task 9, Epoch 15/20 => Loss 0.108, Train_accy 87.320, Test_accy 91.270
2023-12-08 21:12:40,599 [slca.py] => Task 9, Epoch 16/20 => Loss 0.113
2023-12-08 21:13:04,580 [slca.py] => Task 9, Epoch 17/20 => Loss 0.110
2023-12-08 21:13:28,556 [slca.py] => Task 9, Epoch 18/20 => Loss 0.108
2023-12-08 21:13:52,431 [slca.py] => Task 9, Epoch 19/20 => Loss 0.103
2023-12-08 21:14:48,999 [slca.py] => Task 9, Epoch 20/20 => Loss 0.114, Train_accy 87.840, Test_accy 91.340
2023-12-08 21:15:39,518 [slca.py] => CA Task 9 => Loss 0.070, Test_accy 91.650
2023-12-08 21:16:10,901 [slca.py] => CA Task 9 => Loss 0.048, Test_accy 91.750
2023-12-08 21:16:41,771 [slca.py] => CA Task 9 => Loss 0.039, Test_accy 91.620
2023-12-08 21:17:12,738 [slca.py] => CA Task 9 => Loss 0.034, Test_accy 91.720
2023-12-08 21:17:43,869 [slca.py] => CA Task 9 => Loss 0.032, Test_accy 91.690
2023-12-08 21:18:11,457 [slca.py] => Exemplar size: 0
2023-12-08 21:18:11,983 [trainer.py] => No NME accuracy.
2023-12-08 21:18:11,983 [trainer.py] => CNN: {'total': 91.69, '00-09': 82.4, '10-19': 88.6, '20-29': 91.9, '30-39': 90.3, '40-49': 91.6, '50-59': 91.3, '60-69': 93.6, '70-79': 95.1, '80-89': 95.6, '90-99': 96.5, 'old': 91.16, 'new': 96.5}
2023-12-08 21:18:11,983 [trainer.py] => CNN top1 curve: [99.0, 96.4, 93.5, 93.32, 93.04, 91.8, 91.83, 91.94, 91.18, 91.69]
2023-12-08 21:18:11,983 [trainer.py] => CNN top1 avg: 93.37
2023-12-08 21:18:11,984 [trainer.py] => CNN top5 curve: [100.0, 99.95, 99.73, 99.75, 99.76, 99.62, 99.51, 99.41, 99.32, 99.34]

2023-12-08 21:18:12,020 [trainer.py] => final accs: [91.35, 90.98, 91.69]
2023-12-08 21:18:12,020 [trainer.py] => avg accs: [94.849, 94.73299999999999, 93.37]
