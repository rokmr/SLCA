2023-12-19 07:25:42,174 [trainer.py] => config: exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 07:25:42,174 [trainer.py] => test_only: False
2023-12-19 07:25:42,174 [trainer.py] => prefix: reproduce
2023-12-19 07:25:42,174 [trainer.py] => dataset: cifar100_224
2023-12-19 07:25:42,174 [trainer.py] => memory_size: 0
2023-12-19 07:25:42,174 [trainer.py] => memory_per_class: 0
2023-12-19 07:25:42,174 [trainer.py] => fixed_memory: False
2023-12-19 07:25:42,175 [trainer.py] => shuffle: False
2023-12-19 07:25:42,175 [trainer.py] => init_cls: 10
2023-12-19 07:25:42,175 [trainer.py] => increment: 10
2023-12-19 07:25:42,175 [trainer.py] => model_name: slca_cifar
2023-12-19 07:25:42,175 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 07:25:42,175 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 07:25:42,175 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-12-19 07:25:42,175 [trainer.py] => seed: 0
2023-12-19 07:25:42,175 [trainer.py] => epochs: 20
2023-12-19 07:25:42,176 [trainer.py] => ca_epochs: 5
2023-12-19 07:25:42,176 [trainer.py] => ca_with_logit_norm: 0.01
2023-12-19 07:25:42,176 [trainer.py] => milestones: [18]
2023-12-19 07:25:42,176 [trainer.py] => lr: 0.005
2023-12-19 07:25:42,176 [trainer.py] => lr_decay: 0.1
2023-12-19 07:25:42,177 [trainer.py] => weight_decay: 0.0005
2023-12-19 07:25:42,177 [trainer.py] => u_batch_size: 256
2023-12-19 07:25:42,177 [trainer.py] => s_batch_size: 3
2023-12-19 07:25:42,177 [trainer.py] => multicrop: 2
2023-12-19 07:25:42,177 [trainer.py] => us_multicrop: 2
2023-12-19 07:25:42,177 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 07:25:42,178 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 07:25:42,178 [trainer.py] => buffer_size: 500
2023-12-19 07:25:42,178 [trainer.py] => run_id: 0
2023-12-19 07:25:44,595 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 07:25:47,847 [trainer.py] => All params: 85798656
2023-12-19 07:25:47,849 [trainer.py] => Trainable params: 85798656
2023-12-19 07:25:47,850 [slca.py] => Learning on 0-10
2023-12-19 07:26:12,330 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 07:26:14,081 [slca.py] => Task 0, Epoch 2/20 => Loss 2.383
2023-12-19 07:26:15,804 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 07:26:17,527 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 07:26:25,298 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 40.070
2023-12-19 07:26:26,999 [slca.py] => Task 0, Epoch 6/20 => Loss 1.701
2023-12-19 07:26:28,775 [slca.py] => Task 0, Epoch 7/20 => Loss 1.405
2023-12-19 07:26:30,481 [slca.py] => Task 0, Epoch 8/20 => Loss 1.354
2023-12-19 07:26:32,209 [slca.py] => Task 0, Epoch 9/20 => Loss 1.157
2023-12-19 07:26:39,725 [slca.py] => Task 0, Epoch 10/20 => Loss 0.951, Train_accy 92.500, Test_accy 69.750
2023-12-19 07:26:41,432 [slca.py] => Task 0, Epoch 11/20 => Loss 0.774
2023-12-19 07:26:43,226 [slca.py] => Task 0, Epoch 12/20 => Loss 0.792
2023-12-19 07:26:44,954 [slca.py] => Task 0, Epoch 13/20 => Loss 0.584
2023-12-19 07:26:46,678 [slca.py] => Task 0, Epoch 14/20 => Loss 0.457
2023-12-19 07:26:54,513 [slca.py] => Task 0, Epoch 15/20 => Loss 0.483, Train_accy 95.000, Test_accy 80.800
2023-12-19 07:26:56,187 [slca.py] => Task 0, Epoch 16/20 => Loss 0.336
2023-12-19 07:26:57,900 [slca.py] => Task 0, Epoch 17/20 => Loss 0.355
2023-12-19 07:26:59,619 [slca.py] => Task 0, Epoch 18/20 => Loss 0.293
2023-12-19 07:27:01,346 [slca.py] => Task 0, Epoch 19/20 => Loss 0.215
2023-12-19 07:27:09,083 [slca.py] => Task 0, Epoch 20/20 => Loss 0.295, Train_accy 97.500, Test_accy 83.590
2023-12-19 07:27:24,010 [slca.py] => CA Task 0 => Loss 0.014, Test_accy 83.590
2023-12-19 07:27:31,445 [slca.py] => CA Task 0 => Loss 0.019, Test_accy 83.590
2023-12-19 07:27:38,872 [slca.py] => CA Task 0 => Loss 0.017, Test_accy 82.250
2023-12-19 07:27:46,252 [slca.py] => CA Task 0 => Loss 0.017, Test_accy 82.810
2023-12-19 07:27:53,555 [slca.py] => CA Task 0 => Loss 0.000, Test_accy 83.590
2023-12-19 07:27:59,159 [slca.py] => Exemplar size: 0
2023-12-19 07:28:01,260 [trainer.py] => No NME accuracy.
2023-12-19 07:28:01,261 [trainer.py] => CNN: {'total': 83.59, '00-09': 83.59, 'old': 0, 'new': 83.59}
2023-12-19 07:28:01,261 [trainer.py] => CNN top1 curve: [83.59]
2023-12-19 07:28:01,263 [trainer.py] => CNN top1 avg: 83.59
2023-12-19 07:28:01,263 [trainer.py] => CNN top5 curve: [99.0]

2023-12-19 07:28:01,265 [trainer.py] => All params: 85806346
2023-12-19 07:28:01,268 [trainer.py] => Trainable params: 85806346
2023-12-19 07:28:01,271 [slca.py] => Learning on 10-20
2023-12-19 07:28:03,535 [slca.py] => Task 1, Epoch 1/20 => Loss 2.520
2023-12-19 07:28:05,629 [slca.py] => Task 1, Epoch 2/20 => Loss 2.484
2023-12-19 07:28:07,698 [slca.py] => Task 1, Epoch 3/20 => Loss 2.335
2023-12-19 07:28:09,699 [slca.py] => Task 1, Epoch 4/20 => Loss 2.200
2023-12-19 07:28:21,719 [slca.py] => Task 1, Epoch 5/20 => Loss 1.965, Train_accy 53.750, Test_accy 42.660
2023-12-19 07:28:23,765 [slca.py] => Task 1, Epoch 6/20 => Loss 1.633
2023-12-19 07:28:25,778 [slca.py] => Task 1, Epoch 7/20 => Loss 1.522
2023-12-19 07:28:27,838 [slca.py] => Task 1, Epoch 8/20 => Loss 1.283
2023-12-19 07:28:29,936 [slca.py] => Task 1, Epoch 9/20 => Loss 1.065
2023-12-19 07:28:41,439 [slca.py] => Task 1, Epoch 10/20 => Loss 0.805, Train_accy 82.500, Test_accy 55.890
2023-12-19 07:28:43,524 [slca.py] => Task 1, Epoch 11/20 => Loss 0.778
2023-12-19 07:28:45,585 [slca.py] => Task 1, Epoch 12/20 => Loss 0.623
2023-12-19 07:28:47,674 [slca.py] => Task 1, Epoch 13/20 => Loss 0.433
2023-12-19 07:28:49,726 [slca.py] => Task 1, Epoch 14/20 => Loss 0.471
2023-12-19 07:29:01,256 [slca.py] => Task 1, Epoch 15/20 => Loss 0.410, Train_accy 96.250, Test_accy 67.660
2023-12-19 07:29:03,384 [slca.py] => Task 1, Epoch 16/20 => Loss 0.243
2023-12-19 07:29:05,448 [slca.py] => Task 1, Epoch 17/20 => Loss 0.277
2023-12-19 07:29:07,486 [slca.py] => Task 1, Epoch 18/20 => Loss 0.248
2023-12-19 07:29:09,547 [slca.py] => Task 1, Epoch 19/20 => Loss 0.137
2023-12-19 07:29:21,100 [slca.py] => Task 1, Epoch 20/20 => Loss 0.220, Train_accy 91.250, Test_accy 71.040
2023-12-19 07:29:41,929 [slca.py] => CA Task 1 => Loss 0.051, Test_accy 68.590
2023-12-19 07:29:54,794 [slca.py] => CA Task 1 => Loss 0.054, Test_accy 71.670
2023-12-19 07:30:07,661 [slca.py] => CA Task 1 => Loss 0.029, Test_accy 71.090
2023-12-19 07:30:20,451 [slca.py] => CA Task 1 => Loss 0.007, Test_accy 70.470
2023-12-19 07:30:33,385 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 70.680
2023-12-19 07:30:43,503 [slca.py] => Exemplar size: 0
2023-12-19 07:30:45,750 [trainer.py] => No NME accuracy.
2023-12-19 07:30:45,750 [trainer.py] => CNN: {'total': 70.68, '00-09': 77.71, '10-19': 63.65, 'old': 77.71, 'new': 63.65}
2023-12-19 07:30:45,751 [trainer.py] => CNN top1 curve: [83.59, 70.68]
2023-12-19 07:30:45,751 [trainer.py] => CNN top1 avg: 77.135
2023-12-19 07:30:45,751 [trainer.py] => CNN top5 curve: [99.0, 95.05]

2023-12-19 07:30:45,753 [trainer.py] => All params: 85814036
2023-12-19 07:30:45,755 [trainer.py] => Trainable params: 85814036
2023-12-19 07:30:45,757 [slca.py] => Learning on 20-30
2023-12-19 07:30:48,418 [slca.py] => Task 2, Epoch 1/20 => Loss 2.423
2023-12-19 07:30:50,871 [slca.py] => Task 2, Epoch 2/20 => Loss 2.272
2023-12-19 07:30:53,285 [slca.py] => Task 2, Epoch 3/20 => Loss 1.995
2023-12-19 07:30:55,765 [slca.py] => Task 2, Epoch 4/20 => Loss 1.926
2023-12-19 07:31:11,393 [slca.py] => Task 2, Epoch 5/20 => Loss 1.656, Train_accy 61.670, Test_accy 47.760
2023-12-19 07:31:13,795 [slca.py] => Task 2, Epoch 6/20 => Loss 1.490
2023-12-19 07:31:16,332 [slca.py] => Task 2, Epoch 7/20 => Loss 1.150
2023-12-19 07:31:18,834 [slca.py] => Task 2, Epoch 8/20 => Loss 0.952
2023-12-19 07:31:21,232 [slca.py] => Task 2, Epoch 9/20 => Loss 0.761
2023-12-19 07:31:36,743 [slca.py] => Task 2, Epoch 10/20 => Loss 0.747, Train_accy 84.170, Test_accy 59.000
2023-12-19 07:31:39,205 [slca.py] => Task 2, Epoch 11/20 => Loss 0.483
2023-12-19 07:31:41,677 [slca.py] => Task 2, Epoch 12/20 => Loss 0.299
2023-12-19 07:31:44,156 [slca.py] => Task 2, Epoch 13/20 => Loss 0.244
2023-12-19 07:31:46,621 [slca.py] => Task 2, Epoch 14/20 => Loss 0.317
2023-12-19 07:32:02,010 [slca.py] => Task 2, Epoch 15/20 => Loss 0.320, Train_accy 88.330, Test_accy 67.430
2023-12-19 07:32:04,479 [slca.py] => Task 2, Epoch 16/20 => Loss 0.205
2023-12-19 07:32:06,854 [slca.py] => Task 2, Epoch 17/20 => Loss 0.353
2023-12-19 07:32:09,309 [slca.py] => Task 2, Epoch 18/20 => Loss 0.223
2023-12-19 07:32:11,788 [slca.py] => Task 2, Epoch 19/20 => Loss 0.178
2023-12-19 07:32:27,287 [slca.py] => Task 2, Epoch 20/20 => Loss 0.140, Train_accy 92.500, Test_accy 68.610
2023-12-19 07:32:53,680 [slca.py] => CA Task 2 => Loss 0.098, Test_accy 65.900
2023-12-19 07:33:12,392 [slca.py] => CA Task 2 => Loss 0.035, Test_accy 68.610
2023-12-19 07:33:31,048 [slca.py] => CA Task 2 => Loss 0.023, Test_accy 70.010
2023-12-19 07:33:49,755 [slca.py] => CA Task 2 => Loss 0.006, Test_accy 70.450
2023-12-19 07:34:08,491 [slca.py] => CA Task 2 => Loss 0.000, Test_accy 70.520
2023-12-19 07:34:23,200 [slca.py] => Exemplar size: 0
2023-12-19 07:34:25,189 [trainer.py] => No NME accuracy.
2023-12-19 07:34:25,189 [trainer.py] => CNN: {'total': 70.52, '00-09': 75.87, '10-19': 64.29, '20-29': 71.41, 'old': 70.07, 'new': 71.41}
2023-12-19 07:34:25,189 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52]
2023-12-19 07:34:25,190 [trainer.py] => CNN top1 avg: 74.93
2023-12-19 07:34:25,190 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87]

2023-12-19 07:34:25,193 [trainer.py] => All params: 85821726
2023-12-19 07:34:25,194 [trainer.py] => Trainable params: 85821726
2023-12-19 07:34:25,197 [slca.py] => Learning on 30-40
2023-12-19 07:34:28,506 [slca.py] => Task 3, Epoch 1/20 => Loss 2.325
2023-12-19 07:34:31,650 [slca.py] => Task 3, Epoch 2/20 => Loss 2.049
2023-12-19 07:34:34,961 [slca.py] => Task 3, Epoch 3/20 => Loss 1.714
2023-12-19 07:34:37,881 [slca.py] => Task 3, Epoch 4/20 => Loss 1.409
2023-12-19 07:34:58,199 [slca.py] => Task 3, Epoch 5/20 => Loss 0.884, Train_accy 78.750, Test_accy 58.870
2023-12-19 07:35:01,401 [slca.py] => Task 3, Epoch 6/20 => Loss 0.743
2023-12-19 07:35:04,581 [slca.py] => Task 3, Epoch 7/20 => Loss 0.450
2023-12-19 07:35:07,227 [slca.py] => Task 3, Epoch 8/20 => Loss 0.199
2023-12-19 07:35:10,441 [slca.py] => Task 3, Epoch 9/20 => Loss 0.373
2023-12-19 07:35:30,105 [slca.py] => Task 3, Epoch 10/20 => Loss 0.177, Train_accy 80.620, Test_accy 68.070
2023-12-19 07:35:33,195 [slca.py] => Task 3, Epoch 11/20 => Loss 0.267
2023-12-19 07:35:36,070 [slca.py] => Task 3, Epoch 12/20 => Loss 0.117
2023-12-19 07:35:38,993 [slca.py] => Task 3, Epoch 13/20 => Loss 0.071
2023-12-19 07:35:41,807 [slca.py] => Task 3, Epoch 14/20 => Loss 0.058
2023-12-19 07:36:01,536 [slca.py] => Task 3, Epoch 15/20 => Loss 0.132, Train_accy 86.880, Test_accy 68.470
2023-12-19 07:36:04,436 [slca.py] => Task 3, Epoch 16/20 => Loss 0.080
2023-12-19 07:36:07,390 [slca.py] => Task 3, Epoch 17/20 => Loss 0.104
2023-12-19 07:36:10,209 [slca.py] => Task 3, Epoch 18/20 => Loss 0.118
2023-12-19 07:36:13,037 [slca.py] => Task 3, Epoch 19/20 => Loss 0.090
2023-12-19 07:36:32,651 [slca.py] => Task 3, Epoch 20/20 => Loss 0.027, Train_accy 85.620, Test_accy 67.820
2023-12-19 07:37:05,118 [slca.py] => CA Task 3 => Loss 0.209, Test_accy 73.540
2023-12-19 07:37:29,667 [slca.py] => CA Task 3 => Loss 0.050, Test_accy 74.950
2023-12-19 07:37:54,112 [slca.py] => CA Task 3 => Loss 0.010, Test_accy 74.700
2023-12-19 07:38:18,640 [slca.py] => CA Task 3 => Loss 0.001, Test_accy 74.820
2023-12-19 07:38:42,955 [slca.py] => CA Task 3 => Loss 0.004, Test_accy 74.750
2023-12-19 07:39:02,082 [slca.py] => Exemplar size: 0
2023-12-19 07:39:03,984 [trainer.py] => No NME accuracy.
2023-12-19 07:39:03,985 [trainer.py] => CNN: {'total': 74.75, '00-09': 68.48, '10-19': 70.87, '20-29': 79.33, '30-39': 80.28, 'old': 72.9, 'new': 80.28}
2023-12-19 07:39:03,985 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52, 74.75]
2023-12-19 07:39:03,986 [trainer.py] => CNN top1 avg: 74.885
2023-12-19 07:39:03,986 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87, 93.98]

2023-12-19 07:39:03,988 [trainer.py] => All params: 85829416
2023-12-19 07:39:03,990 [trainer.py] => Trainable params: 85829416
2023-12-19 07:39:03,992 [slca.py] => Learning on 40-50
2023-12-19 07:39:07,500 [slca.py] => Task 4, Epoch 1/20 => Loss 2.327
2023-12-19 07:39:10,407 [slca.py] => Task 4, Epoch 2/20 => Loss 1.969
2023-12-19 07:39:13,398 [slca.py] => Task 4, Epoch 3/20 => Loss 1.466
2023-12-19 07:39:16,371 [slca.py] => Task 4, Epoch 4/20 => Loss 1.002
2023-12-19 07:39:39,176 [slca.py] => Task 4, Epoch 5/20 => Loss 0.529, Train_accy 78.000, Test_accy 58.950
2023-12-19 07:39:42,131 [slca.py] => Task 4, Epoch 6/20 => Loss 0.394
2023-12-19 07:39:45,091 [slca.py] => Task 4, Epoch 7/20 => Loss 0.200
2023-12-19 07:39:48,014 [slca.py] => Task 4, Epoch 8/20 => Loss 0.374
2023-12-19 07:39:51,165 [slca.py] => Task 4, Epoch 9/20 => Loss 0.160
2023-12-19 07:40:13,503 [slca.py] => Task 4, Epoch 10/20 => Loss 0.179, Train_accy 84.000, Test_accy 65.360
2023-12-19 07:40:16,482 [slca.py] => Task 4, Epoch 11/20 => Loss 0.147
2023-12-19 07:40:19,433 [slca.py] => Task 4, Epoch 12/20 => Loss 0.074
2023-12-19 07:40:22,418 [slca.py] => Task 4, Epoch 13/20 => Loss 0.106
2023-12-19 07:40:25,357 [slca.py] => Task 4, Epoch 14/20 => Loss 0.023
2023-12-19 07:40:47,955 [slca.py] => Task 4, Epoch 15/20 => Loss 0.090, Train_accy 85.000, Test_accy 64.800
2023-12-19 07:40:50,884 [slca.py] => Task 4, Epoch 16/20 => Loss 0.161
2023-12-19 07:40:53,879 [slca.py] => Task 4, Epoch 17/20 => Loss 0.058
2023-12-19 07:40:56,803 [slca.py] => Task 4, Epoch 18/20 => Loss 0.092
2023-12-19 07:40:59,739 [slca.py] => Task 4, Epoch 19/20 => Loss 0.035
2023-12-19 07:41:22,273 [slca.py] => Task 4, Epoch 20/20 => Loss 0.130, Train_accy 84.500, Test_accy 63.800
2023-12-19 07:42:00,149 [slca.py] => CA Task 4 => Loss 0.186, Test_accy 70.730
2023-12-19 07:42:30,372 [slca.py] => CA Task 4 => Loss 0.021, Test_accy 69.270
2023-12-19 07:43:00,567 [slca.py] => CA Task 4 => Loss 0.010, Test_accy 70.630
2023-12-19 07:43:30,809 [slca.py] => CA Task 4 => Loss 0.008, Test_accy 71.110
2023-12-19 07:44:01,071 [slca.py] => CA Task 4 => Loss 0.003, Test_accy 71.250
2023-12-19 07:44:25,282 [slca.py] => Exemplar size: 0
2023-12-19 07:44:27,248 [trainer.py] => No NME accuracy.
2023-12-19 07:44:27,249 [trainer.py] => CNN: {'total': 71.25, '00-09': 74.05, '10-19': 67.37, '20-29': 66.73, '30-39': 74.72, '40-49': 73.4, 'old': 70.72, 'new': 73.4}
2023-12-19 07:44:27,249 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52, 74.75, 71.25]
2023-12-19 07:44:27,249 [trainer.py] => CNN top1 avg: 74.158
2023-12-19 07:44:27,250 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87, 93.98, 93.67]

2023-12-19 07:44:27,252 [trainer.py] => All params: 85837106
2023-12-19 07:44:27,254 [trainer.py] => Trainable params: 85837106
2023-12-19 07:44:27,256 [slca.py] => Learning on 50-60
2023-12-19 07:44:30,862 [slca.py] => Task 5, Epoch 1/20 => Loss 2.416
2023-12-19 07:44:34,051 [slca.py] => Task 5, Epoch 2/20 => Loss 2.047
2023-12-19 07:44:37,240 [slca.py] => Task 5, Epoch 3/20 => Loss 1.457
2023-12-19 07:44:40,357 [slca.py] => Task 5, Epoch 4/20 => Loss 1.041
2023-12-19 07:45:06,077 [slca.py] => Task 5, Epoch 5/20 => Loss 0.538, Train_accy 74.170, Test_accy 57.000
2023-12-19 07:45:09,208 [slca.py] => Task 5, Epoch 6/20 => Loss 0.347
2023-12-19 07:45:12,520 [slca.py] => Task 5, Epoch 7/20 => Loss 0.261
2023-12-19 07:45:15,686 [slca.py] => Task 5, Epoch 8/20 => Loss 0.125
2023-12-19 07:45:18,937 [slca.py] => Task 5, Epoch 9/20 => Loss 0.306
2023-12-19 07:45:45,055 [slca.py] => Task 5, Epoch 10/20 => Loss 0.117, Train_accy 85.420, Test_accy 61.630
2023-12-19 07:45:48,491 [slca.py] => Task 5, Epoch 11/20 => Loss 0.163
2023-12-19 07:45:51,756 [slca.py] => Task 5, Epoch 12/20 => Loss 0.066
2023-12-19 07:45:55,051 [slca.py] => Task 5, Epoch 13/20 => Loss 0.041
2023-12-19 07:45:58,329 [slca.py] => Task 5, Epoch 14/20 => Loss 0.185
2023-12-19 07:46:24,776 [slca.py] => Task 5, Epoch 15/20 => Loss 0.032, Train_accy 84.170, Test_accy 61.670
2023-12-19 07:46:27,973 [slca.py] => Task 5, Epoch 16/20 => Loss 0.072
2023-12-19 07:46:31,134 [slca.py] => Task 5, Epoch 17/20 => Loss 0.015
2023-12-19 07:46:34,351 [slca.py] => Task 5, Epoch 18/20 => Loss 0.044
2023-12-19 07:46:37,590 [slca.py] => Task 5, Epoch 19/20 => Loss 0.079
2023-12-19 07:47:03,343 [slca.py] => Task 5, Epoch 20/20 => Loss 0.025, Train_accy 81.670, Test_accy 61.430
2023-12-19 07:47:46,824 [slca.py] => CA Task 5 => Loss 0.161, Test_accy 68.430
2023-12-19 07:48:22,423 [slca.py] => CA Task 5 => Loss 0.023, Test_accy 69.580
2023-12-19 07:48:58,032 [slca.py] => CA Task 5 => Loss 0.004, Test_accy 70.160
2023-12-19 07:49:34,329 [slca.py] => CA Task 5 => Loss 0.001, Test_accy 70.140
2023-12-19 07:50:10,656 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 70.230
2023-12-19 07:50:38,726 [slca.py] => Exemplar size: 0
2023-12-19 07:50:40,625 [trainer.py] => No NME accuracy.
2023-12-19 07:50:40,627 [trainer.py] => CNN: {'total': 70.23, '00-09': 64.53, '10-19': 65.72, '20-29': 72.11, '30-39': 76.36, '40-49': 70.0, '50-59': 72.72, 'old': 69.73, 'new': 72.72}
2023-12-19 07:50:40,627 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52, 74.75, 71.25, 70.23]
2023-12-19 07:50:40,628 [trainer.py] => CNN top1 avg: 73.50333333333334
2023-12-19 07:50:40,628 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87, 93.98, 93.67, 92.31]

2023-12-19 07:50:40,631 [trainer.py] => All params: 85844796
2023-12-19 07:50:40,633 [trainer.py] => Trainable params: 85844796
2023-12-19 07:50:40,636 [slca.py] => Learning on 60-70
2023-12-19 07:50:44,476 [slca.py] => Task 6, Epoch 1/20 => Loss 2.296
2023-12-19 07:50:47,982 [slca.py] => Task 6, Epoch 2/20 => Loss 1.340
2023-12-19 07:50:51,509 [slca.py] => Task 6, Epoch 3/20 => Loss 0.898
2023-12-19 07:50:55,036 [slca.py] => Task 6, Epoch 4/20 => Loss 0.776
2023-12-19 07:51:24,877 [slca.py] => Task 6, Epoch 5/20 => Loss 0.300, Train_accy 81.430, Test_accy 58.960
2023-12-19 07:51:28,344 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 07:51:31,803 [slca.py] => Task 6, Epoch 7/20 => Loss 0.132
2023-12-19 07:51:35,596 [slca.py] => Task 6, Epoch 8/20 => Loss 0.074
2023-12-19 07:51:39,139 [slca.py] => Task 6, Epoch 9/20 => Loss 0.103
2023-12-19 07:52:08,819 [slca.py] => Task 6, Epoch 10/20 => Loss 0.047, Train_accy 82.500, Test_accy 60.100
2023-12-19 07:52:12,359 [slca.py] => Task 6, Epoch 11/20 => Loss 0.053
2023-12-19 07:52:15,955 [slca.py] => Task 6, Epoch 12/20 => Loss 0.101
2023-12-19 07:52:19,492 [slca.py] => Task 6, Epoch 13/20 => Loss 0.042
2023-12-19 07:52:22,991 [slca.py] => Task 6, Epoch 14/20 => Loss 0.099
2023-12-19 07:52:52,173 [slca.py] => Task 6, Epoch 15/20 => Loss 0.046, Train_accy 80.000, Test_accy 60.560
2023-12-19 07:52:55,448 [slca.py] => Task 6, Epoch 16/20 => Loss 0.146
2023-12-19 07:52:59,115 [slca.py] => Task 6, Epoch 17/20 => Loss 0.100
2023-12-19 07:53:02,690 [slca.py] => Task 6, Epoch 18/20 => Loss 0.146
2023-12-19 07:53:06,246 [slca.py] => Task 6, Epoch 19/20 => Loss 0.018
2023-12-19 07:53:36,177 [slca.py] => Task 6, Epoch 20/20 => Loss 0.036, Train_accy 80.000, Test_accy 60.520
2023-12-19 07:54:25,630 [slca.py] => CA Task 6 => Loss 0.169, Test_accy 70.080
2023-12-19 07:55:07,108 [slca.py] => CA Task 6 => Loss 0.022, Test_accy 70.070
2023-12-19 07:55:48,715 [slca.py] => CA Task 6 => Loss 0.010, Test_accy 71.060
2023-12-19 07:56:30,076 [slca.py] => CA Task 6 => Loss 0.007, Test_accy 71.700
2023-12-19 07:57:11,426 [slca.py] => CA Task 6 => Loss 0.003, Test_accy 71.660
2023-12-19 07:57:44,485 [slca.py] => Exemplar size: 0
2023-12-19 07:57:46,488 [trainer.py] => No NME accuracy.
2023-12-19 07:57:46,492 [trainer.py] => CNN: {'total': 71.66, '00-09': 65.12, '10-19': 65.05, '20-29': 73.33, '30-39': 79.96, '40-49': 70.98, '50-59': 76.54, '60-69': 70.66, 'old': 71.82, 'new': 70.66}
2023-12-19 07:57:46,492 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52, 74.75, 71.25, 70.23, 71.66]
2023-12-19 07:57:46,493 [trainer.py] => CNN top1 avg: 73.24000000000001
2023-12-19 07:57:46,493 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87, 93.98, 93.67, 92.31, 91.71]

2023-12-19 07:57:46,495 [trainer.py] => All params: 85852486
2023-12-19 07:57:46,497 [trainer.py] => Trainable params: 85852486
2023-12-19 07:57:46,499 [slca.py] => Learning on 70-80
2023-12-19 07:57:50,418 [slca.py] => Task 7, Epoch 1/20 => Loss 2.488
2023-12-19 07:57:54,224 [slca.py] => Task 7, Epoch 2/20 => Loss 1.760
2023-12-19 07:57:58,234 [slca.py] => Task 7, Epoch 3/20 => Loss 0.996
2023-12-19 07:58:02,261 [slca.py] => Task 7, Epoch 4/20 => Loss 0.460
2023-12-19 07:58:35,911 [slca.py] => Task 7, Epoch 5/20 => Loss 0.272, Train_accy 75.940, Test_accy 57.860
2023-12-19 07:58:39,560 [slca.py] => Task 7, Epoch 6/20 => Loss 0.192
2023-12-19 07:58:43,472 [slca.py] => Task 7, Epoch 7/20 => Loss 0.207
2023-12-19 07:58:47,323 [slca.py] => Task 7, Epoch 8/20 => Loss 0.199
2023-12-19 07:58:51,038 [slca.py] => Task 7, Epoch 9/20 => Loss 0.096
2023-12-19 07:59:24,930 [slca.py] => Task 7, Epoch 10/20 => Loss 0.205, Train_accy 84.060, Test_accy 60.240
2023-12-19 07:59:28,623 [slca.py] => Task 7, Epoch 11/20 => Loss 0.177
2023-12-19 07:59:32,545 [slca.py] => Task 7, Epoch 12/20 => Loss 0.097
2023-12-19 07:59:36,260 [slca.py] => Task 7, Epoch 13/20 => Loss 0.137
2023-12-19 07:59:40,208 [slca.py] => Task 7, Epoch 14/20 => Loss 0.126
2023-12-19 08:00:14,145 [slca.py] => Task 7, Epoch 15/20 => Loss 0.019, Train_accy 80.620, Test_accy 59.700
2023-12-19 08:00:17,970 [slca.py] => Task 7, Epoch 16/20 => Loss 0.109
2023-12-19 08:00:21,825 [slca.py] => Task 7, Epoch 17/20 => Loss 0.025
2023-12-19 08:00:25,516 [slca.py] => Task 7, Epoch 18/20 => Loss 0.044
2023-12-19 08:00:29,217 [slca.py] => Task 7, Epoch 19/20 => Loss 0.024
2023-12-19 08:01:02,463 [slca.py] => Task 7, Epoch 20/20 => Loss 0.043, Train_accy 77.810, Test_accy 59.360
2023-12-19 08:01:59,410 [slca.py] => CA Task 7 => Loss 0.151, Test_accy 67.160
2023-12-19 08:02:46,857 [slca.py] => CA Task 7 => Loss 0.025, Test_accy 68.620
2023-12-19 08:03:34,315 [slca.py] => CA Task 7 => Loss 0.005, Test_accy 68.840
2023-12-19 08:04:21,944 [slca.py] => CA Task 7 => Loss 0.004, Test_accy 69.100
2023-12-19 08:05:09,550 [slca.py] => CA Task 7 => Loss 0.004, Test_accy 69.180
2023-12-19 08:05:47,083 [slca.py] => Exemplar size: 0
2023-12-19 08:05:49,008 [trainer.py] => No NME accuracy.
2023-12-19 08:05:49,009 [trainer.py] => CNN: {'total': 69.18, '00-09': 63.77, '10-19': 55.8, '20-29': 65.73, '30-39': 80.55, '40-49': 69.01, '50-59': 74.8, '60-69': 65.93, '70-79': 77.92, 'old': 67.93, 'new': 77.92}
2023-12-19 08:05:49,009 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52, 74.75, 71.25, 70.23, 71.66, 69.18]
2023-12-19 08:05:49,010 [trainer.py] => CNN top1 avg: 72.7325
2023-12-19 08:05:49,010 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87, 93.98, 93.67, 92.31, 91.71, 90.81]

2023-12-19 08:05:49,012 [trainer.py] => All params: 85860176
2023-12-19 08:05:49,014 [trainer.py] => Trainable params: 85860176
2023-12-19 08:05:49,017 [slca.py] => Learning on 80-90
2023-12-19 08:05:53,114 [slca.py] => Task 8, Epoch 1/20 => Loss 2.447
2023-12-19 08:05:56,916 [slca.py] => Task 8, Epoch 2/20 => Loss 1.588
2023-12-19 08:06:00,607 [slca.py] => Task 8, Epoch 3/20 => Loss 0.838
2023-12-19 08:06:04,463 [slca.py] => Task 8, Epoch 4/20 => Loss 0.276
2023-12-19 08:06:40,967 [slca.py] => Task 8, Epoch 5/20 => Loss 0.171, Train_accy 79.170, Test_accy 57.760
2023-12-19 08:06:44,749 [slca.py] => Task 8, Epoch 6/20 => Loss 0.210
2023-12-19 08:06:48,601 [slca.py] => Task 8, Epoch 7/20 => Loss 0.117
2023-12-19 08:06:52,429 [slca.py] => Task 8, Epoch 8/20 => Loss 0.063
2023-12-19 08:06:56,248 [slca.py] => Task 8, Epoch 9/20 => Loss 0.032
2023-12-19 08:07:32,660 [slca.py] => Task 8, Epoch 10/20 => Loss 0.095, Train_accy 79.720, Test_accy 60.410
2023-12-19 08:07:36,437 [slca.py] => Task 8, Epoch 11/20 => Loss 0.014
2023-12-19 08:07:40,341 [slca.py] => Task 8, Epoch 12/20 => Loss 0.057
2023-12-19 08:07:44,203 [slca.py] => Task 8, Epoch 13/20 => Loss 0.024
2023-12-19 08:07:48,043 [slca.py] => Task 8, Epoch 14/20 => Loss 0.150
2023-12-19 08:08:24,516 [slca.py] => Task 8, Epoch 15/20 => Loss 0.019, Train_accy 83.330, Test_accy 60.290
2023-12-19 08:08:28,389 [slca.py] => Task 8, Epoch 16/20 => Loss 0.033
2023-12-19 08:08:32,146 [slca.py] => Task 8, Epoch 17/20 => Loss 0.044
2023-12-19 08:08:35,946 [slca.py] => Task 8, Epoch 18/20 => Loss 0.081
2023-12-19 08:08:39,813 [slca.py] => Task 8, Epoch 19/20 => Loss 0.016
2023-12-19 08:09:16,460 [slca.py] => Task 8, Epoch 20/20 => Loss 0.095, Train_accy 79.440, Test_accy 60.060
2023-12-19 08:10:17,900 [slca.py] => CA Task 8 => Loss 0.201, Test_accy 68.560
2023-12-19 08:11:10,850 [slca.py] => CA Task 8 => Loss 0.028, Test_accy 69.490
2023-12-19 08:12:04,098 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 70.220
2023-12-19 08:12:57,244 [slca.py] => CA Task 8 => Loss 0.008, Test_accy 70.360
2023-12-19 08:13:50,358 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 70.350
2023-12-19 08:14:30,969 [slca.py] => Exemplar size: 0
2023-12-19 08:14:32,847 [trainer.py] => No NME accuracy.
2023-12-19 08:14:32,847 [trainer.py] => CNN: {'total': 70.35, '00-09': 62.21, '10-19': 59.44, '20-29': 67.4, '30-39': 83.73, '40-49': 67.24, '50-59': 70.04, '60-69': 71.04, '70-79': 77.54, '80-89': 74.47, 'old': 69.83, 'new': 74.47}
2023-12-19 08:14:32,848 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52, 74.75, 71.25, 70.23, 71.66, 69.18, 70.35]
2023-12-19 08:14:32,848 [trainer.py] => CNN top1 avg: 72.46777777777778
2023-12-19 08:14:32,848 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87, 93.98, 93.67, 92.31, 91.71, 90.81, 91.0]

2023-12-19 08:14:32,850 [trainer.py] => All params: 85867866
2023-12-19 08:14:32,851 [trainer.py] => Trainable params: 85867866
2023-12-19 08:14:32,853 [slca.py] => Learning on 90-100
2023-12-19 08:14:37,395 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 08:14:41,593 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 08:14:45,752 [slca.py] => Task 9, Epoch 3/20 => Loss 0.664
2023-12-19 08:14:49,953 [slca.py] => Task 9, Epoch 4/20 => Loss 0.255
2023-12-19 08:15:30,298 [slca.py] => Task 9, Epoch 5/20 => Loss 0.103, Train_accy 80.500, Test_accy 56.750
2023-12-19 08:15:34,622 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 08:15:38,695 [slca.py] => Task 9, Epoch 7/20 => Loss 0.113
2023-12-19 08:15:43,253 [slca.py] => Task 9, Epoch 8/20 => Loss 0.112
2023-12-19 08:15:47,556 [slca.py] => Task 9, Epoch 9/20 => Loss 0.094
2023-12-19 08:16:28,383 [slca.py] => Task 9, Epoch 10/20 => Loss 0.028, Train_accy 81.250, Test_accy 58.040
2023-12-19 08:16:32,610 [slca.py] => Task 9, Epoch 11/20 => Loss 0.021
2023-12-19 08:16:36,848 [slca.py] => Task 9, Epoch 12/20 => Loss 0.032
2023-12-19 08:16:41,264 [slca.py] => Task 9, Epoch 13/20 => Loss 0.027
2023-12-19 08:16:45,538 [slca.py] => Task 9, Epoch 14/20 => Loss 0.053
2023-12-19 08:17:26,080 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 81.000, Test_accy 57.860
2023-12-19 08:17:30,321 [slca.py] => Task 9, Epoch 16/20 => Loss 0.016
2023-12-19 08:17:35,012 [slca.py] => Task 9, Epoch 17/20 => Loss 0.089
2023-12-19 08:17:39,229 [slca.py] => Task 9, Epoch 18/20 => Loss 0.009
2023-12-19 08:17:43,577 [slca.py] => Task 9, Epoch 19/20 => Loss 0.048
2023-12-19 08:18:24,304 [slca.py] => Task 9, Epoch 20/20 => Loss 0.103, Train_accy 80.750, Test_accy 58.640
2023-12-19 08:19:32,303 [slca.py] => CA Task 9 => Loss 0.160, Test_accy 65.770
2023-12-19 08:20:31,116 [slca.py] => CA Task 9 => Loss 0.019, Test_accy 66.380
2023-12-19 08:21:29,784 [slca.py] => CA Task 9 => Loss 0.010, Test_accy 66.440
2023-12-19 08:22:28,705 [slca.py] => CA Task 9 => Loss 0.003, Test_accy 66.910
2023-12-19 08:23:27,789 [slca.py] => CA Task 9 => Loss 0.005, Test_accy 66.860
2023-12-19 08:24:12,981 [slca.py] => Exemplar size: 0
2023-12-19 08:24:15,035 [trainer.py] => No NME accuracy.
2023-12-19 08:24:15,036 [trainer.py] => CNN: {'total': 66.86, '00-09': 58.36, '10-19': 56.06, '20-29': 62.06, '30-39': 75.25, '40-49': 66.4, '50-59': 73.65, '60-69': 68.37, '70-79': 68.61, '80-89': 70.64, '90-99': 69.21, 'old': 66.6, 'new': 69.21}
2023-12-19 08:24:15,036 [trainer.py] => CNN top1 curve: [83.59, 70.68, 70.52, 74.75, 71.25, 70.23, 71.66, 69.18, 70.35, 66.86]
2023-12-19 08:24:15,037 [trainer.py] => CNN top1 avg: 71.90700000000001
2023-12-19 08:24:15,037 [trainer.py] => CNN top5 curve: [99.0, 95.05, 94.87, 93.98, 93.67, 92.31, 91.71, 90.81, 91.0, 90.75]

2023-12-19 08:24:15,081 [trainer.py] => final accs: [66.86]
2023-12-19 08:24:15,081 [trainer.py] => avg accs: [71.90700000000001]
