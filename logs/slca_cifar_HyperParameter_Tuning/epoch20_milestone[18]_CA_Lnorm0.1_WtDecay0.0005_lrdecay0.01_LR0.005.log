2023-12-19 06:22:07,124 [trainer.py] => config: exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 06:22:07,124 [trainer.py] => test_only: False
2023-12-19 06:22:07,124 [trainer.py] => prefix: reproduce
2023-12-19 06:22:07,124 [trainer.py] => dataset: cifar100_224
2023-12-19 06:22:07,124 [trainer.py] => memory_size: 0
2023-12-19 06:22:07,124 [trainer.py] => memory_per_class: 0
2023-12-19 06:22:07,124 [trainer.py] => fixed_memory: False
2023-12-19 06:22:07,124 [trainer.py] => shuffle: False
2023-12-19 06:22:07,124 [trainer.py] => init_cls: 10
2023-12-19 06:22:07,124 [trainer.py] => increment: 10
2023-12-19 06:22:07,124 [trainer.py] => model_name: slca_cifar
2023-12-19 06:22:07,124 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 06:22:07,124 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 06:22:07,125 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-12-19 06:22:07,125 [trainer.py] => seed: 0
2023-12-19 06:22:07,125 [trainer.py] => epochs: 20
2023-12-19 06:22:07,125 [trainer.py] => ca_epochs: 5
2023-12-19 06:22:07,125 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-19 06:22:07,125 [trainer.py] => milestones: [18]
2023-12-19 06:22:07,125 [trainer.py] => lr: 0.005
2023-12-19 06:22:07,125 [trainer.py] => lr_decay: 0.01
2023-12-19 06:22:07,125 [trainer.py] => weight_decay: 0.0005
2023-12-19 06:22:07,125 [trainer.py] => u_batch_size: 256
2023-12-19 06:22:07,125 [trainer.py] => s_batch_size: 3
2023-12-19 06:22:07,125 [trainer.py] => multicrop: 2
2023-12-19 06:22:07,125 [trainer.py] => us_multicrop: 2
2023-12-19 06:22:07,125 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 06:22:07,125 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 06:22:07,125 [trainer.py] => buffer_size: 500
2023-12-19 06:22:07,125 [trainer.py] => run_id: 0
2023-12-19 06:22:09,417 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 06:22:12,293 [trainer.py] => All params: 85798656
2023-12-19 06:22:12,294 [trainer.py] => Trainable params: 85798656
2023-12-19 06:22:12,294 [slca.py] => Learning on 0-10
2023-12-19 06:22:37,165 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 06:22:39,065 [slca.py] => Task 0, Epoch 2/20 => Loss 2.383
2023-12-19 06:22:40,995 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 06:22:42,646 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 06:22:50,255 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 40.070
2023-12-19 06:22:52,008 [slca.py] => Task 0, Epoch 6/20 => Loss 1.701
2023-12-19 06:22:53,727 [slca.py] => Task 0, Epoch 7/20 => Loss 1.405
2023-12-19 06:22:55,428 [slca.py] => Task 0, Epoch 8/20 => Loss 1.354
2023-12-19 06:22:57,101 [slca.py] => Task 0, Epoch 9/20 => Loss 1.157
2023-12-19 06:23:04,437 [slca.py] => Task 0, Epoch 10/20 => Loss 0.951, Train_accy 92.500, Test_accy 69.750
2023-12-19 06:23:06,056 [slca.py] => Task 0, Epoch 11/20 => Loss 0.774
2023-12-19 06:23:07,842 [slca.py] => Task 0, Epoch 12/20 => Loss 0.792
2023-12-19 06:23:09,664 [slca.py] => Task 0, Epoch 13/20 => Loss 0.584
2023-12-19 06:23:11,345 [slca.py] => Task 0, Epoch 14/20 => Loss 0.457
2023-12-19 06:23:19,704 [slca.py] => Task 0, Epoch 15/20 => Loss 0.483, Train_accy 95.000, Test_accy 80.800
2023-12-19 06:23:21,615 [slca.py] => Task 0, Epoch 16/20 => Loss 0.336
2023-12-19 06:23:23,319 [slca.py] => Task 0, Epoch 17/20 => Loss 0.355
2023-12-19 06:23:25,092 [slca.py] => Task 0, Epoch 18/20 => Loss 0.293
2023-12-19 06:23:27,032 [slca.py] => Task 0, Epoch 19/20 => Loss 0.215
2023-12-19 06:23:35,083 [slca.py] => Task 0, Epoch 20/20 => Loss 0.298, Train_accy 97.500, Test_accy 83.590
2023-12-19 06:23:49,586 [slca.py] => CA Task 0 => Loss 0.024, Test_accy 85.040
2023-12-19 06:23:56,652 [slca.py] => CA Task 0 => Loss 0.010, Test_accy 86.610
2023-12-19 06:24:03,704 [slca.py] => CA Task 0 => Loss 0.005, Test_accy 86.160
2023-12-19 06:24:10,621 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 86.270
2023-12-19 06:24:17,487 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 86.270
2023-12-19 06:24:22,906 [slca.py] => Exemplar size: 0
2023-12-19 06:24:25,023 [trainer.py] => No NME accuracy.
2023-12-19 06:24:25,023 [trainer.py] => CNN: {'total': 86.27, '00-09': 86.27, 'old': 0, 'new': 86.27}
2023-12-19 06:24:25,023 [trainer.py] => CNN top1 curve: [86.27]
2023-12-19 06:24:25,024 [trainer.py] => CNN top1 avg: 86.27
2023-12-19 06:24:25,024 [trainer.py] => CNN top5 curve: [99.33]

2023-12-19 06:24:25,026 [trainer.py] => All params: 85806346
2023-12-19 06:24:25,027 [trainer.py] => Trainable params: 85806346
2023-12-19 06:24:25,029 [slca.py] => Learning on 10-20
2023-12-19 06:24:27,353 [slca.py] => Task 1, Epoch 1/20 => Loss 2.521
2023-12-19 06:24:29,348 [slca.py] => Task 1, Epoch 2/20 => Loss 2.484
2023-12-19 06:24:31,303 [slca.py] => Task 1, Epoch 3/20 => Loss 2.335
2023-12-19 06:24:33,369 [slca.py] => Task 1, Epoch 4/20 => Loss 2.200
2023-12-19 06:24:45,095 [slca.py] => Task 1, Epoch 5/20 => Loss 1.966, Train_accy 52.500, Test_accy 42.810
2023-12-19 06:24:47,165 [slca.py] => Task 1, Epoch 6/20 => Loss 1.634
2023-12-19 06:24:49,116 [slca.py] => Task 1, Epoch 7/20 => Loss 1.525
2023-12-19 06:24:51,104 [slca.py] => Task 1, Epoch 8/20 => Loss 1.282
2023-12-19 06:24:53,089 [slca.py] => Task 1, Epoch 9/20 => Loss 1.071
2023-12-19 06:25:04,632 [slca.py] => Task 1, Epoch 10/20 => Loss 0.815, Train_accy 83.750, Test_accy 56.460
2023-12-19 06:25:06,634 [slca.py] => Task 1, Epoch 11/20 => Loss 0.805
2023-12-19 06:25:08,651 [slca.py] => Task 1, Epoch 12/20 => Loss 0.646
2023-12-19 06:25:10,660 [slca.py] => Task 1, Epoch 13/20 => Loss 0.442
2023-12-19 06:25:12,731 [slca.py] => Task 1, Epoch 14/20 => Loss 0.443
2023-12-19 06:25:24,230 [slca.py] => Task 1, Epoch 15/20 => Loss 0.402, Train_accy 97.500, Test_accy 67.500
2023-12-19 06:25:26,239 [slca.py] => Task 1, Epoch 16/20 => Loss 0.225
2023-12-19 06:25:28,287 [slca.py] => Task 1, Epoch 17/20 => Loss 0.257
2023-12-19 06:25:30,277 [slca.py] => Task 1, Epoch 18/20 => Loss 0.225
2023-12-19 06:25:32,297 [slca.py] => Task 1, Epoch 19/20 => Loss 0.145
2023-12-19 06:25:43,675 [slca.py] => Task 1, Epoch 20/20 => Loss 0.269, Train_accy 91.250, Test_accy 70.520
2023-12-19 06:26:03,903 [slca.py] => CA Task 1 => Loss 0.032, Test_accy 72.860
2023-12-19 06:26:16,720 [slca.py] => CA Task 1 => Loss 0.008, Test_accy 74.010
2023-12-19 06:26:29,508 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 74.380
2023-12-19 06:26:42,413 [slca.py] => CA Task 1 => Loss 0.004, Test_accy 74.380
2023-12-19 06:26:55,206 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 74.480
2023-12-19 06:27:05,116 [slca.py] => Exemplar size: 0
2023-12-19 06:27:07,213 [trainer.py] => No NME accuracy.
2023-12-19 06:27:07,213 [trainer.py] => CNN: {'total': 74.48, '00-09': 82.81, '10-19': 66.15, 'old': 82.81, 'new': 66.15}
2023-12-19 06:27:07,214 [trainer.py] => CNN top1 curve: [86.27, 74.48]
2023-12-19 06:27:07,214 [trainer.py] => CNN top1 avg: 80.375
2023-12-19 06:27:07,214 [trainer.py] => CNN top5 curve: [99.33, 96.98]

2023-12-19 06:27:07,216 [trainer.py] => All params: 85814036
2023-12-19 06:27:07,217 [trainer.py] => Trainable params: 85814036
2023-12-19 06:27:07,220 [slca.py] => Learning on 20-30
2023-12-19 06:27:09,960 [slca.py] => Task 2, Epoch 1/20 => Loss 2.395
2023-12-19 06:27:12,655 [slca.py] => Task 2, Epoch 2/20 => Loss 2.268
2023-12-19 06:27:15,156 [slca.py] => Task 2, Epoch 3/20 => Loss 2.003
2023-12-19 06:27:17,650 [slca.py] => Task 2, Epoch 4/20 => Loss 1.933
2023-12-19 06:27:33,236 [slca.py] => Task 2, Epoch 5/20 => Loss 1.663, Train_accy 61.670, Test_accy 47.890
2023-12-19 06:27:35,659 [slca.py] => Task 2, Epoch 6/20 => Loss 1.504
2023-12-19 06:27:37,986 [slca.py] => Task 2, Epoch 7/20 => Loss 1.186
2023-12-19 06:27:40,609 [slca.py] => Task 2, Epoch 8/20 => Loss 0.971
2023-12-19 06:27:42,987 [slca.py] => Task 2, Epoch 9/20 => Loss 0.767
2023-12-19 06:27:58,347 [slca.py] => Task 2, Epoch 10/20 => Loss 0.761, Train_accy 80.830, Test_accy 59.340
2023-12-19 06:28:00,711 [slca.py] => Task 2, Epoch 11/20 => Loss 0.498
2023-12-19 06:28:03,045 [slca.py] => Task 2, Epoch 12/20 => Loss 0.310
2023-12-19 06:28:05,390 [slca.py] => Task 2, Epoch 13/20 => Loss 0.262
2023-12-19 06:28:07,764 [slca.py] => Task 2, Epoch 14/20 => Loss 0.333
2023-12-19 06:28:23,317 [slca.py] => Task 2, Epoch 15/20 => Loss 0.343, Train_accy 91.670, Test_accy 67.530
2023-12-19 06:28:25,617 [slca.py] => Task 2, Epoch 16/20 => Loss 0.227
2023-12-19 06:28:28,061 [slca.py] => Task 2, Epoch 17/20 => Loss 0.378
2023-12-19 06:28:30,448 [slca.py] => Task 2, Epoch 18/20 => Loss 0.231
2023-12-19 06:28:32,806 [slca.py] => Task 2, Epoch 19/20 => Loss 0.184
2023-12-19 06:28:48,258 [slca.py] => Task 2, Epoch 20/20 => Loss 0.161, Train_accy 94.170, Test_accy 67.930
2023-12-19 06:29:14,456 [slca.py] => CA Task 2 => Loss 0.038, Test_accy 70.350
2023-12-19 06:29:33,218 [slca.py] => CA Task 2 => Loss 0.009, Test_accy 71.230
2023-12-19 06:29:52,050 [slca.py] => CA Task 2 => Loss 0.005, Test_accy 71.880
2023-12-19 06:30:10,781 [slca.py] => CA Task 2 => Loss 0.004, Test_accy 72.040
2023-12-19 06:30:29,398 [slca.py] => CA Task 2 => Loss 0.003, Test_accy 72.150
2023-12-19 06:30:43,836 [slca.py] => Exemplar size: 0
2023-12-19 06:30:45,893 [trainer.py] => No NME accuracy.
2023-12-19 06:30:45,894 [trainer.py] => CNN: {'total': 72.15, '00-09': 77.3, '10-19': 62.16, '20-29': 77.01, 'old': 69.71, 'new': 77.01}
2023-12-19 06:30:45,894 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15]
2023-12-19 06:30:45,895 [trainer.py] => CNN top1 avg: 77.63333333333334
2023-12-19 06:30:45,895 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38]

2023-12-19 06:30:45,897 [trainer.py] => All params: 85821726
2023-12-19 06:30:45,899 [trainer.py] => Trainable params: 85821726
2023-12-19 06:30:45,901 [slca.py] => Learning on 30-40
2023-12-19 06:30:49,124 [slca.py] => Task 3, Epoch 1/20 => Loss 2.316
2023-12-19 06:30:51,930 [slca.py] => Task 3, Epoch 2/20 => Loss 2.058
2023-12-19 06:30:54,720 [slca.py] => Task 3, Epoch 3/20 => Loss 1.698
2023-12-19 06:30:57,829 [slca.py] => Task 3, Epoch 4/20 => Loss 1.444
2023-12-19 06:31:17,640 [slca.py] => Task 3, Epoch 5/20 => Loss 0.858, Train_accy 79.380, Test_accy 59.350
2023-12-19 06:31:20,449 [slca.py] => Task 3, Epoch 6/20 => Loss 0.700
2023-12-19 06:31:23,189 [slca.py] => Task 3, Epoch 7/20 => Loss 0.450
2023-12-19 06:31:26,034 [slca.py] => Task 3, Epoch 8/20 => Loss 0.193
2023-12-19 06:31:28,805 [slca.py] => Task 3, Epoch 9/20 => Loss 0.312
2023-12-19 06:31:48,109 [slca.py] => Task 3, Epoch 10/20 => Loss 0.185, Train_accy 82.500, Test_accy 67.890
2023-12-19 06:31:51,119 [slca.py] => Task 3, Epoch 11/20 => Loss 0.268
2023-12-19 06:31:53,897 [slca.py] => Task 3, Epoch 12/20 => Loss 0.126
2023-12-19 06:31:56,712 [slca.py] => Task 3, Epoch 13/20 => Loss 0.059
2023-12-19 06:31:59,519 [slca.py] => Task 3, Epoch 14/20 => Loss 0.059
2023-12-19 06:32:18,941 [slca.py] => Task 3, Epoch 15/20 => Loss 0.136, Train_accy 86.250, Test_accy 68.220
2023-12-19 06:32:21,714 [slca.py] => Task 3, Epoch 16/20 => Loss 0.076
2023-12-19 06:32:24,507 [slca.py] => Task 3, Epoch 17/20 => Loss 0.087
2023-12-19 06:32:27,310 [slca.py] => Task 3, Epoch 18/20 => Loss 0.115
2023-12-19 06:32:30,061 [slca.py] => Task 3, Epoch 19/20 => Loss 0.049
2023-12-19 06:32:49,248 [slca.py] => Task 3, Epoch 20/20 => Loss 0.039, Train_accy 86.880, Test_accy 67.010
2023-12-19 06:33:21,008 [slca.py] => CA Task 3 => Loss 0.057, Test_accy 72.450
2023-12-19 06:33:45,679 [slca.py] => CA Task 3 => Loss 0.010, Test_accy 73.390
2023-12-19 06:34:10,418 [slca.py] => CA Task 3 => Loss 0.006, Test_accy 73.590
2023-12-19 06:34:34,744 [slca.py] => CA Task 3 => Loss 0.005, Test_accy 73.820
2023-12-19 06:34:59,258 [slca.py] => CA Task 3 => Loss 0.005, Test_accy 73.840
2023-12-19 06:35:18,588 [slca.py] => Exemplar size: 0
2023-12-19 06:35:20,609 [trainer.py] => No NME accuracy.
2023-12-19 06:35:20,610 [trainer.py] => CNN: {'total': 73.84, '00-09': 73.64, '10-19': 59.88, '20-29': 77.02, '30-39': 84.81, 'old': 70.17, 'new': 84.81}
2023-12-19 06:35:20,610 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15, 73.84]
2023-12-19 06:35:20,610 [trainer.py] => CNN top1 avg: 76.685
2023-12-19 06:35:20,610 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38, 94.86]

2023-12-19 06:35:20,612 [trainer.py] => All params: 85829416
2023-12-19 06:35:20,614 [trainer.py] => Trainable params: 85829416
2023-12-19 06:35:20,616 [slca.py] => Learning on 40-50
2023-12-19 06:35:23,832 [slca.py] => Task 4, Epoch 1/20 => Loss 2.342
2023-12-19 06:35:26,802 [slca.py] => Task 4, Epoch 2/20 => Loss 1.965
2023-12-19 06:35:29,766 [slca.py] => Task 4, Epoch 3/20 => Loss 1.445
2023-12-19 06:35:32,644 [slca.py] => Task 4, Epoch 4/20 => Loss 0.996
2023-12-19 06:35:55,161 [slca.py] => Task 4, Epoch 5/20 => Loss 0.553, Train_accy 76.500, Test_accy 57.950
2023-12-19 06:35:58,065 [slca.py] => Task 4, Epoch 6/20 => Loss 0.424
2023-12-19 06:36:00,978 [slca.py] => Task 4, Epoch 7/20 => Loss 0.196
2023-12-19 06:36:03,853 [slca.py] => Task 4, Epoch 8/20 => Loss 0.408
2023-12-19 06:36:06,803 [slca.py] => Task 4, Epoch 9/20 => Loss 0.156
2023-12-19 06:36:29,557 [slca.py] => Task 4, Epoch 10/20 => Loss 0.173, Train_accy 84.000, Test_accy 64.860
2023-12-19 06:36:32,518 [slca.py] => Task 4, Epoch 11/20 => Loss 0.115
2023-12-19 06:36:35,513 [slca.py] => Task 4, Epoch 12/20 => Loss 0.077
2023-12-19 06:36:38,448 [slca.py] => Task 4, Epoch 13/20 => Loss 0.142
2023-12-19 06:36:41,418 [slca.py] => Task 4, Epoch 14/20 => Loss 0.022
2023-12-19 06:37:04,042 [slca.py] => Task 4, Epoch 15/20 => Loss 0.124, Train_accy 83.000, Test_accy 63.780
2023-12-19 06:37:06,977 [slca.py] => Task 4, Epoch 16/20 => Loss 0.121
2023-12-19 06:37:09,922 [slca.py] => Task 4, Epoch 17/20 => Loss 0.080
2023-12-19 06:37:12,771 [slca.py] => Task 4, Epoch 18/20 => Loss 0.094
2023-12-19 06:37:15,681 [slca.py] => Task 4, Epoch 19/20 => Loss 0.115
2023-12-19 06:37:38,416 [slca.py] => Task 4, Epoch 20/20 => Loss 0.119, Train_accy 82.500, Test_accy 62.940
2023-12-19 06:38:16,134 [slca.py] => CA Task 4 => Loss 0.053, Test_accy 69.110
2023-12-19 06:38:46,207 [slca.py] => CA Task 4 => Loss 0.010, Test_accy 70.270
2023-12-19 06:39:16,520 [slca.py] => CA Task 4 => Loss 0.007, Test_accy 70.710
2023-12-19 06:39:46,514 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 70.830
2023-12-19 06:40:16,799 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 70.830
2023-12-19 06:40:40,451 [slca.py] => Exemplar size: 0
2023-12-19 06:40:42,233 [trainer.py] => No NME accuracy.
2023-12-19 06:40:42,234 [trainer.py] => CNN: {'total': 70.83, '00-09': 68.94, '10-19': 57.06, '20-29': 67.94, '30-39': 81.95, '40-49': 78.3, 'old': 68.96, 'new': 78.3}
2023-12-19 06:40:42,234 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15, 73.84, 70.83]
2023-12-19 06:40:42,235 [trainer.py] => CNN top1 avg: 75.514
2023-12-19 06:40:42,235 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38, 94.86, 93.53]

2023-12-19 06:40:42,237 [trainer.py] => All params: 85837106
2023-12-19 06:40:42,239 [trainer.py] => Trainable params: 85837106
2023-12-19 06:40:42,241 [slca.py] => Learning on 50-60
2023-12-19 06:40:45,457 [slca.py] => Task 5, Epoch 1/20 => Loss 2.426
2023-12-19 06:40:48,462 [slca.py] => Task 5, Epoch 2/20 => Loss 2.064
2023-12-19 06:40:51,490 [slca.py] => Task 5, Epoch 3/20 => Loss 1.485
2023-12-19 06:40:54,668 [slca.py] => Task 5, Epoch 4/20 => Loss 1.070
2023-12-19 06:41:20,501 [slca.py] => Task 5, Epoch 5/20 => Loss 0.546, Train_accy 73.330, Test_accy 56.010
2023-12-19 06:41:23,608 [slca.py] => Task 5, Epoch 6/20 => Loss 0.349
2023-12-19 06:41:26,691 [slca.py] => Task 5, Epoch 7/20 => Loss 0.264
2023-12-19 06:41:29,815 [slca.py] => Task 5, Epoch 8/20 => Loss 0.128
2023-12-19 06:41:32,831 [slca.py] => Task 5, Epoch 9/20 => Loss 0.274
2023-12-19 06:41:58,403 [slca.py] => Task 5, Epoch 10/20 => Loss 0.116, Train_accy 82.920, Test_accy 61.060
2023-12-19 06:42:01,519 [slca.py] => Task 5, Epoch 11/20 => Loss 0.174
2023-12-19 06:42:04,524 [slca.py] => Task 5, Epoch 12/20 => Loss 0.053
2023-12-19 06:42:07,498 [slca.py] => Task 5, Epoch 13/20 => Loss 0.038
2023-12-19 06:42:10,516 [slca.py] => Task 5, Epoch 14/20 => Loss 0.164
2023-12-19 06:42:36,452 [slca.py] => Task 5, Epoch 15/20 => Loss 0.038, Train_accy 84.580, Test_accy 60.780
2023-12-19 06:42:39,566 [slca.py] => Task 5, Epoch 16/20 => Loss 0.056
2023-12-19 06:42:42,610 [slca.py] => Task 5, Epoch 17/20 => Loss 0.017
2023-12-19 06:42:45,708 [slca.py] => Task 5, Epoch 18/20 => Loss 0.036
2023-12-19 06:42:48,804 [slca.py] => Task 5, Epoch 19/20 => Loss 0.047
2023-12-19 06:43:14,552 [slca.py] => Task 5, Epoch 20/20 => Loss 0.019, Train_accy 79.580, Test_accy 60.240
2023-12-19 06:43:58,517 [slca.py] => CA Task 5 => Loss 0.055, Test_accy 67.950
2023-12-19 06:44:33,919 [slca.py] => CA Task 5 => Loss 0.012, Test_accy 68.440
2023-12-19 06:45:09,447 [slca.py] => CA Task 5 => Loss 0.008, Test_accy 68.780
2023-12-19 06:45:44,999 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 68.970
2023-12-19 06:46:20,639 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 68.990
2023-12-19 06:46:48,562 [slca.py] => Exemplar size: 0
2023-12-19 06:46:50,577 [trainer.py] => No NME accuracy.
2023-12-19 06:46:50,577 [trainer.py] => CNN: {'total': 68.99, '00-09': 65.14, '10-19': 54.56, '20-29': 65.92, '30-39': 81.58, '40-49': 72.04, '50-59': 74.87, 'old': 67.82, 'new': 74.87}
2023-12-19 06:46:50,577 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15, 73.84, 70.83, 68.99]
2023-12-19 06:46:50,578 [trainer.py] => CNN top1 avg: 74.42666666666666
2023-12-19 06:46:50,578 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38, 94.86, 93.53, 92.44]

2023-12-19 06:46:50,579 [trainer.py] => All params: 85844796
2023-12-19 06:46:50,580 [trainer.py] => Trainable params: 85844796
2023-12-19 06:46:50,582 [slca.py] => Learning on 60-70
2023-12-19 06:46:54,375 [slca.py] => Task 6, Epoch 1/20 => Loss 2.281
2023-12-19 06:46:58,054 [slca.py] => Task 6, Epoch 2/20 => Loss 1.316
2023-12-19 06:47:01,846 [slca.py] => Task 6, Epoch 3/20 => Loss 0.954
2023-12-19 06:47:05,626 [slca.py] => Task 6, Epoch 4/20 => Loss 0.717
2023-12-19 06:47:36,179 [slca.py] => Task 6, Epoch 5/20 => Loss 0.344, Train_accy 77.860, Test_accy 58.040
2023-12-19 06:47:39,962 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 06:47:43,445 [slca.py] => Task 6, Epoch 7/20 => Loss 0.159
2023-12-19 06:47:46,959 [slca.py] => Task 6, Epoch 8/20 => Loss 0.069
2023-12-19 06:47:50,572 [slca.py] => Task 6, Epoch 9/20 => Loss 0.080
2023-12-19 06:48:20,633 [slca.py] => Task 6, Epoch 10/20 => Loss 0.054, Train_accy 81.070, Test_accy 59.060
2023-12-19 06:48:24,068 [slca.py] => Task 6, Epoch 11/20 => Loss 0.053
2023-12-19 06:48:27,526 [slca.py] => Task 6, Epoch 12/20 => Loss 0.095
2023-12-19 06:48:31,069 [slca.py] => Task 6, Epoch 13/20 => Loss 0.044
2023-12-19 06:48:34,607 [slca.py] => Task 6, Epoch 14/20 => Loss 0.092
2023-12-19 06:49:04,041 [slca.py] => Task 6, Epoch 15/20 => Loss 0.047, Train_accy 80.000, Test_accy 59.400
2023-12-19 06:49:07,562 [slca.py] => Task 6, Epoch 16/20 => Loss 0.114
2023-12-19 06:49:11,104 [slca.py] => Task 6, Epoch 17/20 => Loss 0.083
2023-12-19 06:49:14,610 [slca.py] => Task 6, Epoch 18/20 => Loss 0.121
2023-12-19 06:49:18,124 [slca.py] => Task 6, Epoch 19/20 => Loss 0.018
2023-12-19 06:49:48,092 [slca.py] => Task 6, Epoch 20/20 => Loss 0.029, Train_accy 81.790, Test_accy 59.610
2023-12-19 06:50:37,052 [slca.py] => CA Task 6 => Loss 0.057, Test_accy 67.770
2023-12-19 06:51:18,717 [slca.py] => CA Task 6 => Loss 0.012, Test_accy 68.500
2023-12-19 06:52:00,437 [slca.py] => CA Task 6 => Loss 0.008, Test_accy 68.940
2023-12-19 06:52:42,056 [slca.py] => CA Task 6 => Loss 0.006, Test_accy 69.130
2023-12-19 06:53:23,583 [slca.py] => CA Task 6 => Loss 0.006, Test_accy 69.200
2023-12-19 06:53:56,562 [slca.py] => Exemplar size: 0
2023-12-19 06:53:58,535 [trainer.py] => No NME accuracy.
2023-12-19 06:53:58,535 [trainer.py] => CNN: {'total': 69.2, '00-09': 61.98, '10-19': 52.89, '20-29': 64.91, '30-39': 81.08, '40-49': 70.78, '50-59': 74.22, '60-69': 78.58, 'old': 67.64, 'new': 78.58}
2023-12-19 06:53:58,536 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15, 73.84, 70.83, 68.99, 69.2]
2023-12-19 06:53:58,536 [trainer.py] => CNN top1 avg: 73.67999999999999
2023-12-19 06:53:58,537 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38, 94.86, 93.53, 92.44, 91.59]

2023-12-19 06:53:58,539 [trainer.py] => All params: 85852486
2023-12-19 06:53:58,540 [trainer.py] => Trainable params: 85852486
2023-12-19 06:53:58,542 [slca.py] => Learning on 70-80
2023-12-19 06:54:02,651 [slca.py] => Task 7, Epoch 1/20 => Loss 2.467
2023-12-19 06:54:06,549 [slca.py] => Task 7, Epoch 2/20 => Loss 1.754
2023-12-19 06:54:10,467 [slca.py] => Task 7, Epoch 3/20 => Loss 1.001
2023-12-19 06:54:14,223 [slca.py] => Task 7, Epoch 4/20 => Loss 0.465
2023-12-19 06:54:47,340 [slca.py] => Task 7, Epoch 5/20 => Loss 0.292, Train_accy 76.250, Test_accy 56.430
2023-12-19 06:54:51,224 [slca.py] => Task 7, Epoch 6/20 => Loss 0.207
2023-12-19 06:54:55,006 [slca.py] => Task 7, Epoch 7/20 => Loss 0.218
2023-12-19 06:54:58,667 [slca.py] => Task 7, Epoch 8/20 => Loss 0.233
2023-12-19 06:55:02,458 [slca.py] => Task 7, Epoch 9/20 => Loss 0.081
2023-12-19 06:55:35,701 [slca.py] => Task 7, Epoch 10/20 => Loss 0.200, Train_accy 83.120, Test_accy 58.850
2023-12-19 06:55:39,597 [slca.py] => Task 7, Epoch 11/20 => Loss 0.177
2023-12-19 06:55:43,184 [slca.py] => Task 7, Epoch 12/20 => Loss 0.115
2023-12-19 06:55:46,586 [slca.py] => Task 7, Epoch 13/20 => Loss 0.113
2023-12-19 06:55:50,225 [slca.py] => Task 7, Epoch 14/20 => Loss 0.143
2023-12-19 06:56:23,647 [slca.py] => Task 7, Epoch 15/20 => Loss 0.022, Train_accy 79.690, Test_accy 59.030
2023-12-19 06:56:27,289 [slca.py] => Task 7, Epoch 16/20 => Loss 0.128
2023-12-19 06:56:31,147 [slca.py] => Task 7, Epoch 17/20 => Loss 0.021
2023-12-19 06:56:34,843 [slca.py] => Task 7, Epoch 18/20 => Loss 0.047
2023-12-19 06:56:38,492 [slca.py] => Task 7, Epoch 19/20 => Loss 0.023
2023-12-19 06:57:11,727 [slca.py] => Task 7, Epoch 20/20 => Loss 0.057, Train_accy 77.810, Test_accy 58.730
2023-12-19 06:58:05,761 [slca.py] => CA Task 7 => Loss 0.055, Test_accy 65.880
2023-12-19 06:58:53,142 [slca.py] => CA Task 7 => Loss 0.012, Test_accy 67.040
2023-12-19 06:59:40,181 [slca.py] => CA Task 7 => Loss 0.008, Test_accy 67.310
2023-12-19 07:00:27,146 [slca.py] => CA Task 7 => Loss 0.007, Test_accy 67.410
2023-12-19 07:01:14,682 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 67.500
2023-12-19 07:01:52,024 [slca.py] => Exemplar size: 0
2023-12-19 07:01:53,935 [trainer.py] => No NME accuracy.
2023-12-19 07:01:53,935 [trainer.py] => CNN: {'total': 67.5, '00-09': 57.42, '10-19': 50.05, '20-29': 60.32, '30-39': 80.45, '40-49': 70.22, '50-59': 71.66, '60-69': 72.06, '70-79': 77.92, 'old': 66.01, 'new': 77.92}
2023-12-19 07:01:53,936 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15, 73.84, 70.83, 68.99, 69.2, 67.5]
2023-12-19 07:01:53,936 [trainer.py] => CNN top1 avg: 72.9075
2023-12-19 07:01:53,937 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38, 94.86, 93.53, 92.44, 91.59, 90.94]

2023-12-19 07:01:53,939 [trainer.py] => All params: 85860176
2023-12-19 07:01:53,941 [trainer.py] => Trainable params: 85860176
2023-12-19 07:01:53,943 [slca.py] => Learning on 80-90
2023-12-19 07:01:58,184 [slca.py] => Task 8, Epoch 1/20 => Loss 2.426
2023-12-19 07:02:02,047 [slca.py] => Task 8, Epoch 2/20 => Loss 1.570
2023-12-19 07:02:05,937 [slca.py] => Task 8, Epoch 3/20 => Loss 0.842
2023-12-19 07:02:09,691 [slca.py] => Task 8, Epoch 4/20 => Loss 0.263
2023-12-19 07:02:46,243 [slca.py] => Task 8, Epoch 5/20 => Loss 0.181, Train_accy 78.890, Test_accy 56.910
2023-12-19 07:02:50,139 [slca.py] => Task 8, Epoch 6/20 => Loss 0.223
2023-12-19 07:02:53,984 [slca.py] => Task 8, Epoch 7/20 => Loss 0.089
2023-12-19 07:02:57,789 [slca.py] => Task 8, Epoch 8/20 => Loss 0.057
2023-12-19 07:03:01,602 [slca.py] => Task 8, Epoch 9/20 => Loss 0.038
2023-12-19 07:03:38,157 [slca.py] => Task 8, Epoch 10/20 => Loss 0.081, Train_accy 79.440, Test_accy 59.080
2023-12-19 07:03:42,007 [slca.py] => Task 8, Epoch 11/20 => Loss 0.012
2023-12-19 07:03:45,798 [slca.py] => Task 8, Epoch 12/20 => Loss 0.038
2023-12-19 07:03:49,632 [slca.py] => Task 8, Epoch 13/20 => Loss 0.018
2023-12-19 07:03:53,478 [slca.py] => Task 8, Epoch 14/20 => Loss 0.161
2023-12-19 07:04:29,927 [slca.py] => Task 8, Epoch 15/20 => Loss 0.009, Train_accy 81.670, Test_accy 58.660
2023-12-19 07:04:33,778 [slca.py] => Task 8, Epoch 16/20 => Loss 0.061
2023-12-19 07:04:37,559 [slca.py] => Task 8, Epoch 17/20 => Loss 0.048
2023-12-19 07:04:41,388 [slca.py] => Task 8, Epoch 18/20 => Loss 0.084
2023-12-19 07:04:45,097 [slca.py] => Task 8, Epoch 19/20 => Loss 0.011
2023-12-19 07:05:21,397 [slca.py] => Task 8, Epoch 20/20 => Loss 0.093, Train_accy 75.280, Test_accy 58.070
2023-12-19 07:06:22,217 [slca.py] => CA Task 8 => Loss 0.054, Test_accy 64.630
2023-12-19 07:07:16,032 [slca.py] => CA Task 8 => Loss 0.012, Test_accy 65.520
2023-12-19 07:08:09,723 [slca.py] => CA Task 8 => Loss 0.008, Test_accy 66.080
2023-12-19 07:09:02,701 [slca.py] => CA Task 8 => Loss 0.007, Test_accy 66.240
2023-12-19 07:09:55,871 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 66.290
2023-12-19 07:10:37,997 [slca.py] => Exemplar size: 0
2023-12-19 07:10:39,834 [trainer.py] => No NME accuracy.
2023-12-19 07:10:39,835 [trainer.py] => CNN: {'total': 66.29, '00-09': 53.17, '10-19': 44.88, '20-29': 56.47, '30-39': 78.41, '40-49': 66.13, '50-59': 69.84, '60-69': 70.74, '70-79': 76.23, '80-89': 80.8, 'old': 64.48, 'new': 80.8}
2023-12-19 07:10:39,835 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15, 73.84, 70.83, 68.99, 69.2, 67.5, 66.29]
2023-12-19 07:10:39,836 [trainer.py] => CNN top1 avg: 72.17222222222222
2023-12-19 07:10:39,836 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38, 94.86, 93.53, 92.44, 91.59, 90.94, 90.18]

2023-12-19 07:10:39,838 [trainer.py] => All params: 85867866
2023-12-19 07:10:39,840 [trainer.py] => Trainable params: 85867866
2023-12-19 07:10:39,843 [slca.py] => Learning on 90-100
2023-12-19 07:10:44,436 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 07:10:48,754 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 07:10:53,003 [slca.py] => Task 9, Epoch 3/20 => Loss 0.682
2023-12-19 07:10:57,158 [slca.py] => Task 9, Epoch 4/20 => Loss 0.253
2023-12-19 07:11:37,768 [slca.py] => Task 9, Epoch 5/20 => Loss 0.111, Train_accy 76.500, Test_accy 54.440
2023-12-19 07:11:42,027 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 07:11:46,328 [slca.py] => Task 9, Epoch 7/20 => Loss 0.116
2023-12-19 07:11:50,547 [slca.py] => Task 9, Epoch 8/20 => Loss 0.118
2023-12-19 07:11:54,795 [slca.py] => Task 9, Epoch 9/20 => Loss 0.091
2023-12-19 07:12:35,316 [slca.py] => Task 9, Epoch 10/20 => Loss 0.016, Train_accy 79.000, Test_accy 56.030
2023-12-19 07:12:39,596 [slca.py] => Task 9, Epoch 11/20 => Loss 0.039
2023-12-19 07:12:43,904 [slca.py] => Task 9, Epoch 12/20 => Loss 0.043
2023-12-19 07:12:48,291 [slca.py] => Task 9, Epoch 13/20 => Loss 0.040
2023-12-19 07:12:52,584 [slca.py] => Task 9, Epoch 14/20 => Loss 0.088
2023-12-19 07:13:33,669 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 78.750, Test_accy 55.500
2023-12-19 07:13:37,921 [slca.py] => Task 9, Epoch 16/20 => Loss 0.022
2023-12-19 07:13:42,322 [slca.py] => Task 9, Epoch 17/20 => Loss 0.100
2023-12-19 07:13:46,736 [slca.py] => Task 9, Epoch 18/20 => Loss 0.006
2023-12-19 07:13:51,013 [slca.py] => Task 9, Epoch 19/20 => Loss 0.050
2023-12-19 07:14:31,755 [slca.py] => Task 9, Epoch 20/20 => Loss 0.027, Train_accy 78.000, Test_accy 56.100
2023-12-19 07:15:39,341 [slca.py] => CA Task 9 => Loss 0.052, Test_accy 62.600
2023-12-19 07:16:38,493 [slca.py] => CA Task 9 => Loss 0.014, Test_accy 63.520
2023-12-19 07:17:37,634 [slca.py] => CA Task 9 => Loss 0.009, Test_accy 64.030
2023-12-19 07:18:36,463 [slca.py] => CA Task 9 => Loss 0.007, Test_accy 64.170
2023-12-19 07:19:35,443 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 64.160
2023-12-19 07:20:21,175 [slca.py] => Exemplar size: 0
2023-12-19 07:20:23,122 [trainer.py] => No NME accuracy.
2023-12-19 07:20:23,122 [trainer.py] => CNN: {'total': 64.16, '00-09': 47.35, '10-19': 44.64, '20-29': 55.36, '30-39': 73.45, '40-49': 65.9, '50-59': 67.94, '60-69': 70.87, '70-79': 73.52, '80-89': 77.25, '90-99': 65.4, 'old': 64.03, 'new': 65.4}
2023-12-19 07:20:23,123 [trainer.py] => CNN top1 curve: [86.27, 74.48, 72.15, 73.84, 70.83, 68.99, 69.2, 67.5, 66.29, 64.16]
2023-12-19 07:20:23,123 [trainer.py] => CNN top1 avg: 71.371
2023-12-19 07:20:23,124 [trainer.py] => CNN top5 curve: [99.33, 96.98, 95.38, 94.86, 93.53, 92.44, 91.59, 90.94, 90.18, 89.64]

2023-12-19 07:20:23,155 [trainer.py] => final accs: [64.16]
2023-12-19 07:20:23,156 [trainer.py] => avg accs: [71.371]
