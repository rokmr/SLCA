2023-12-19 06:32:01,592 [trainer.py] => config: /home/gayathri/rohit/SLCA/exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 06:32:01,592 [trainer.py] => test_only: False
2023-12-19 06:32:01,592 [trainer.py] => prefix: reproduce
2023-12-19 06:32:01,592 [trainer.py] => dataset: cifar100_224
2023-12-19 06:32:01,592 [trainer.py] => memory_size: 0
2023-12-19 06:32:01,592 [trainer.py] => memory_per_class: 0
2023-12-19 06:32:01,592 [trainer.py] => fixed_memory: False
2023-12-19 06:32:01,592 [trainer.py] => shuffle: False
2023-12-19 06:32:01,592 [trainer.py] => init_cls: 10
2023-12-19 06:32:01,592 [trainer.py] => increment: 10
2023-12-19 06:32:01,592 [trainer.py] => model_name: slca_cifar
2023-12-19 06:32:01,592 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 06:32:01,592 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 06:32:01,592 [trainer.py] => device: [device(type='cuda', index=1)]
2023-12-19 06:32:01,592 [trainer.py] => seed: 0
2023-12-19 06:32:01,592 [trainer.py] => epochs: 20
2023-12-19 06:32:01,592 [trainer.py] => ca_epochs: 5
2023-12-19 06:32:01,592 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-19 06:32:01,592 [trainer.py] => milestones: [18]
2023-12-19 06:32:01,592 [trainer.py] => lr: 0.005
2023-12-19 06:32:01,592 [trainer.py] => lr_decay: 0.1
2023-12-19 06:32:01,592 [trainer.py] => weight_decay: 5e-05
2023-12-19 06:32:01,593 [trainer.py] => u_batch_size: 256
2023-12-19 06:32:01,593 [trainer.py] => s_batch_size: 3
2023-12-19 06:32:01,593 [trainer.py] => multicrop: 2
2023-12-19 06:32:01,593 [trainer.py] => us_multicrop: 2
2023-12-19 06:32:01,593 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 06:32:01,593 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 06:32:01,593 [trainer.py] => buffer_size: 500
2023-12-19 06:32:01,593 [trainer.py] => run_id: 0
2023-12-19 06:32:03,095 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 06:32:04,873 [trainer.py] => All params: 85798656
2023-12-19 06:32:04,873 [trainer.py] => Trainable params: 85798656
2023-12-19 06:32:04,874 [slca.py] => Learning on 0-10
2023-12-19 06:32:10,272 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 06:32:11,353 [slca.py] => Task 0, Epoch 2/20 => Loss 2.382
2023-12-19 06:32:12,463 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 06:32:13,540 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 06:32:18,317 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 39.960
2023-12-19 06:32:19,395 [slca.py] => Task 0, Epoch 6/20 => Loss 1.699
2023-12-19 06:32:20,497 [slca.py] => Task 0, Epoch 7/20 => Loss 1.405
2023-12-19 06:32:21,593 [slca.py] => Task 0, Epoch 8/20 => Loss 1.355
2023-12-19 06:32:22,673 [slca.py] => Task 0, Epoch 9/20 => Loss 1.156
2023-12-19 06:32:27,634 [slca.py] => Task 0, Epoch 10/20 => Loss 0.952, Train_accy 92.500, Test_accy 69.750
2023-12-19 06:32:28,756 [slca.py] => Task 0, Epoch 11/20 => Loss 0.774
2023-12-19 06:32:29,883 [slca.py] => Task 0, Epoch 12/20 => Loss 0.796
2023-12-19 06:32:30,957 [slca.py] => Task 0, Epoch 13/20 => Loss 0.573
2023-12-19 06:32:32,023 [slca.py] => Task 0, Epoch 14/20 => Loss 0.458
2023-12-19 06:32:36,753 [slca.py] => Task 0, Epoch 15/20 => Loss 0.482, Train_accy 95.000, Test_accy 81.140
2023-12-19 06:32:37,842 [slca.py] => Task 0, Epoch 16/20 => Loss 0.330
2023-12-19 06:32:38,942 [slca.py] => Task 0, Epoch 17/20 => Loss 0.352
2023-12-19 06:32:40,033 [slca.py] => Task 0, Epoch 18/20 => Loss 0.293
2023-12-19 06:32:41,096 [slca.py] => Task 0, Epoch 19/20 => Loss 0.215
2023-12-19 06:32:45,957 [slca.py] => Task 0, Epoch 20/20 => Loss 0.298, Train_accy 97.500, Test_accy 84.040
2023-12-19 06:32:53,821 [slca.py] => CA Task 0 => Loss 0.026, Test_accy 85.710
2023-12-19 06:32:56,672 [slca.py] => CA Task 0 => Loss 0.010, Test_accy 86.610
2023-12-19 06:32:59,480 [slca.py] => CA Task 0 => Loss 0.005, Test_accy 87.390
2023-12-19 06:33:02,371 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 87.610
2023-12-19 06:33:05,201 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 87.610
2023-12-19 06:33:08,051 [slca.py] => Exemplar size: 0
2023-12-19 06:33:08,759 [trainer.py] => No NME accuracy.
2023-12-19 06:33:08,760 [trainer.py] => CNN: {'total': 87.61, '00-09': 87.61, 'old': 0, 'new': 87.61}
2023-12-19 06:33:08,760 [trainer.py] => CNN top1 curve: [87.61]
2023-12-19 06:33:08,760 [trainer.py] => CNN top1 avg: 87.61
2023-12-19 06:33:08,760 [trainer.py] => CNN top5 curve: [99.33]

2023-12-19 06:33:08,762 [trainer.py] => All params: 85806346
2023-12-19 06:33:08,763 [trainer.py] => Trainable params: 85806346
2023-12-19 06:33:08,764 [slca.py] => Learning on 10-20
2023-12-19 06:33:10,325 [slca.py] => Task 1, Epoch 1/20 => Loss 2.531
2023-12-19 06:33:11,647 [slca.py] => Task 1, Epoch 2/20 => Loss 2.489
2023-12-19 06:33:12,969 [slca.py] => Task 1, Epoch 3/20 => Loss 2.332
2023-12-19 06:33:14,328 [slca.py] => Task 1, Epoch 4/20 => Loss 2.197
2023-12-19 06:33:21,584 [slca.py] => Task 1, Epoch 5/20 => Loss 1.952, Train_accy 52.500, Test_accy 42.710
2023-12-19 06:33:23,093 [slca.py] => Task 1, Epoch 6/20 => Loss 1.627
2023-12-19 06:33:24,504 [slca.py] => Task 1, Epoch 7/20 => Loss 1.507
2023-12-19 06:33:25,890 [slca.py] => Task 1, Epoch 8/20 => Loss 1.269
2023-12-19 06:33:27,261 [slca.py] => Task 1, Epoch 9/20 => Loss 1.054
2023-12-19 06:33:35,957 [slca.py] => Task 1, Epoch 10/20 => Loss 0.813, Train_accy 80.000, Test_accy 55.520
2023-12-19 06:33:37,418 [slca.py] => Task 1, Epoch 11/20 => Loss 0.767
2023-12-19 06:33:38,888 [slca.py] => Task 1, Epoch 12/20 => Loss 0.610
2023-12-19 06:33:40,280 [slca.py] => Task 1, Epoch 13/20 => Loss 0.423
2023-12-19 06:33:41,691 [slca.py] => Task 1, Epoch 14/20 => Loss 0.455
2023-12-19 06:33:49,062 [slca.py] => Task 1, Epoch 15/20 => Loss 0.397, Train_accy 96.250, Test_accy 67.190
2023-12-19 06:33:50,473 [slca.py] => Task 1, Epoch 16/20 => Loss 0.235
2023-12-19 06:33:51,850 [slca.py] => Task 1, Epoch 17/20 => Loss 0.268
2023-12-19 06:33:53,251 [slca.py] => Task 1, Epoch 18/20 => Loss 0.238
2023-12-19 06:33:54,607 [slca.py] => Task 1, Epoch 19/20 => Loss 0.141
2023-12-19 06:34:01,945 [slca.py] => Task 1, Epoch 20/20 => Loss 0.229, Train_accy 91.250, Test_accy 70.830
2023-12-19 06:34:13,373 [slca.py] => CA Task 1 => Loss 0.032, Test_accy 72.600
2023-12-19 06:34:18,370 [slca.py] => CA Task 1 => Loss 0.009, Test_accy 74.170
2023-12-19 06:34:24,635 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 74.740
2023-12-19 06:34:29,702 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 74.740
2023-12-19 06:34:36,273 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 74.790
2023-12-19 06:34:41,126 [slca.py] => Exemplar size: 0
2023-12-19 06:34:41,821 [trainer.py] => No NME accuracy.
2023-12-19 06:34:41,822 [trainer.py] => CNN: {'total': 74.79, '00-09': 82.5, '10-19': 67.08, 'old': 82.5, 'new': 67.08}
2023-12-19 06:34:41,822 [trainer.py] => CNN top1 curve: [87.61, 74.79]
2023-12-19 06:34:41,822 [trainer.py] => CNN top1 avg: 81.2
2023-12-19 06:34:41,822 [trainer.py] => CNN top5 curve: [99.33, 97.4]

2023-12-19 06:34:41,823 [trainer.py] => All params: 85814036
2023-12-19 06:34:41,824 [trainer.py] => Trainable params: 85814036
2023-12-19 06:34:41,825 [slca.py] => Learning on 20-30
2023-12-19 06:34:43,591 [slca.py] => Task 2, Epoch 1/20 => Loss 2.422
2023-12-19 06:34:45,314 [slca.py] => Task 2, Epoch 2/20 => Loss 2.275
2023-12-19 06:34:46,988 [slca.py] => Task 2, Epoch 3/20 => Loss 1.993
2023-12-19 06:34:48,681 [slca.py] => Task 2, Epoch 4/20 => Loss 1.918
2023-12-19 06:34:58,528 [slca.py] => Task 2, Epoch 5/20 => Loss 1.651, Train_accy 63.330, Test_accy 48.130
2023-12-19 06:35:00,261 [slca.py] => Task 2, Epoch 6/20 => Loss 1.484
2023-12-19 06:35:01,942 [slca.py] => Task 2, Epoch 7/20 => Loss 1.142
2023-12-19 06:35:03,654 [slca.py] => Task 2, Epoch 8/20 => Loss 0.945
2023-12-19 06:35:05,386 [slca.py] => Task 2, Epoch 9/20 => Loss 0.763
2023-12-19 06:35:15,216 [slca.py] => Task 2, Epoch 10/20 => Loss 0.738, Train_accy 84.170, Test_accy 60.120
2023-12-19 06:35:16,938 [slca.py] => Task 2, Epoch 11/20 => Loss 0.482
2023-12-19 06:35:18,678 [slca.py] => Task 2, Epoch 12/20 => Loss 0.295
2023-12-19 06:35:20,336 [slca.py] => Task 2, Epoch 13/20 => Loss 0.237
2023-12-19 06:35:22,023 [slca.py] => Task 2, Epoch 14/20 => Loss 0.308
2023-12-19 06:35:31,905 [slca.py] => Task 2, Epoch 15/20 => Loss 0.319, Train_accy 89.170, Test_accy 68.580
2023-12-19 06:35:33,862 [slca.py] => Task 2, Epoch 16/20 => Loss 0.222
2023-12-19 06:35:35,820 [slca.py] => Task 2, Epoch 17/20 => Loss 0.352
2023-12-19 06:35:37,824 [slca.py] => Task 2, Epoch 18/20 => Loss 0.208
2023-12-19 06:35:39,802 [slca.py] => Task 2, Epoch 19/20 => Loss 0.180
2023-12-19 06:35:50,661 [slca.py] => Task 2, Epoch 20/20 => Loss 0.141, Train_accy 91.670, Test_accy 69.330
2023-12-19 06:36:03,742 [slca.py] => CA Task 2 => Loss 0.038, Test_accy 71.330
2023-12-19 06:36:10,905 [slca.py] => CA Task 2 => Loss 0.008, Test_accy 72.110
2023-12-19 06:36:18,669 [slca.py] => CA Task 2 => Loss 0.005, Test_accy 72.660
2023-12-19 06:36:25,850 [slca.py] => CA Task 2 => Loss 0.004, Test_accy 72.930
2023-12-19 06:36:33,228 [slca.py] => CA Task 2 => Loss 0.004, Test_accy 73.060
2023-12-19 06:36:40,188 [slca.py] => Exemplar size: 0
2023-12-19 06:36:40,897 [trainer.py] => No NME accuracy.
2023-12-19 06:36:40,897 [trainer.py] => CNN: {'total': 73.06, '00-09': 77.91, '10-19': 63.78, '20-29': 77.52, 'old': 70.83, 'new': 77.52}
2023-12-19 06:36:40,898 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06]
2023-12-19 06:36:40,898 [trainer.py] => CNN top1 avg: 78.48666666666666
2023-12-19 06:36:40,898 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65]

2023-12-19 06:36:40,899 [trainer.py] => All params: 85821726
2023-12-19 06:36:40,901 [trainer.py] => Trainable params: 85821726
2023-12-19 06:36:40,902 [slca.py] => Learning on 30-40
2023-12-19 06:36:43,006 [slca.py] => Task 3, Epoch 1/20 => Loss 2.308
2023-12-19 06:36:44,913 [slca.py] => Task 3, Epoch 2/20 => Loss 2.054
2023-12-19 06:36:46,824 [slca.py] => Task 3, Epoch 3/20 => Loss 1.707
2023-12-19 06:36:48,797 [slca.py] => Task 3, Epoch 4/20 => Loss 1.412
2023-12-19 06:37:01,074 [slca.py] => Task 3, Epoch 5/20 => Loss 0.886, Train_accy 80.000, Test_accy 60.380
2023-12-19 06:37:03,059 [slca.py] => Task 3, Epoch 6/20 => Loss 0.668
2023-12-19 06:37:05,057 [slca.py] => Task 3, Epoch 7/20 => Loss 0.424
2023-12-19 06:37:07,031 [slca.py] => Task 3, Epoch 8/20 => Loss 0.184
2023-12-19 06:37:08,894 [slca.py] => Task 3, Epoch 9/20 => Loss 0.320
2023-12-19 06:37:21,352 [slca.py] => Task 3, Epoch 10/20 => Loss 0.163, Train_accy 81.880, Test_accy 68.570
2023-12-19 06:37:23,255 [slca.py] => Task 3, Epoch 11/20 => Loss 0.261
2023-12-19 06:37:25,179 [slca.py] => Task 3, Epoch 12/20 => Loss 0.112
2023-12-19 06:37:27,085 [slca.py] => Task 3, Epoch 13/20 => Loss 0.065
2023-12-19 06:37:29,065 [slca.py] => Task 3, Epoch 14/20 => Loss 0.059
2023-12-19 06:37:41,849 [slca.py] => Task 3, Epoch 15/20 => Loss 0.127, Train_accy 85.620, Test_accy 69.150
2023-12-19 06:37:43,794 [slca.py] => Task 3, Epoch 16/20 => Loss 0.111
2023-12-19 06:37:45,881 [slca.py] => Task 3, Epoch 17/20 => Loss 0.086
2023-12-19 06:37:47,835 [slca.py] => Task 3, Epoch 18/20 => Loss 0.113
2023-12-19 06:37:49,855 [slca.py] => Task 3, Epoch 19/20 => Loss 0.064
2023-12-19 06:38:03,728 [slca.py] => Task 3, Epoch 20/20 => Loss 0.049, Train_accy 86.250, Test_accy 68.040
2023-12-19 06:38:18,062 [slca.py] => CA Task 3 => Loss 0.055, Test_accy 73.110
2023-12-19 06:38:27,309 [slca.py] => CA Task 3 => Loss 0.011, Test_accy 74.400
2023-12-19 06:38:36,935 [slca.py] => CA Task 3 => Loss 0.006, Test_accy 74.800
2023-12-19 06:38:46,228 [slca.py] => CA Task 3 => Loss 0.005, Test_accy 74.750
2023-12-19 06:38:55,607 [slca.py] => CA Task 3 => Loss 0.005, Test_accy 74.770
2023-12-19 06:39:05,490 [slca.py] => Exemplar size: 0
2023-12-19 06:39:06,219 [trainer.py] => No NME accuracy.
2023-12-19 06:39:06,220 [trainer.py] => CNN: {'total': 74.77, '00-09': 72.83, '10-19': 60.69, '20-29': 78.63, '30-39': 86.92, 'old': 70.71, 'new': 86.92}
2023-12-19 06:39:06,220 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06, 74.77]
2023-12-19 06:39:06,220 [trainer.py] => CNN top1 avg: 77.5575
2023-12-19 06:39:06,220 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65, 94.86]

2023-12-19 06:39:06,221 [trainer.py] => All params: 85829416
2023-12-19 06:39:06,221 [trainer.py] => Trainable params: 85829416
2023-12-19 06:39:06,223 [slca.py] => Learning on 40-50
2023-12-19 06:39:08,416 [slca.py] => Task 4, Epoch 1/20 => Loss 2.310
2023-12-19 06:39:10,558 [slca.py] => Task 4, Epoch 2/20 => Loss 1.919
2023-12-19 06:39:12,670 [slca.py] => Task 4, Epoch 3/20 => Loss 1.466
2023-12-19 06:39:14,846 [slca.py] => Task 4, Epoch 4/20 => Loss 0.968
2023-12-19 06:39:29,560 [slca.py] => Task 4, Epoch 5/20 => Loss 0.512, Train_accy 77.500, Test_accy 58.970
2023-12-19 06:39:31,760 [slca.py] => Task 4, Epoch 6/20 => Loss 0.371
2023-12-19 06:39:33,952 [slca.py] => Task 4, Epoch 7/20 => Loss 0.196
2023-12-19 06:39:36,136 [slca.py] => Task 4, Epoch 8/20 => Loss 0.343
2023-12-19 06:39:38,304 [slca.py] => Task 4, Epoch 9/20 => Loss 0.138
2023-12-19 06:39:52,833 [slca.py] => Task 4, Epoch 10/20 => Loss 0.182, Train_accy 82.000, Test_accy 64.780
2023-12-19 06:39:54,980 [slca.py] => Task 4, Epoch 11/20 => Loss 0.158
2023-12-19 06:39:57,105 [slca.py] => Task 4, Epoch 12/20 => Loss 0.075
2023-12-19 06:39:59,214 [slca.py] => Task 4, Epoch 13/20 => Loss 0.108
2023-12-19 06:40:01,402 [slca.py] => Task 4, Epoch 14/20 => Loss 0.023
2023-12-19 06:40:16,036 [slca.py] => Task 4, Epoch 15/20 => Loss 0.110, Train_accy 80.500, Test_accy 63.120
2023-12-19 06:40:18,182 [slca.py] => Task 4, Epoch 16/20 => Loss 0.138
2023-12-19 06:40:20,382 [slca.py] => Task 4, Epoch 17/20 => Loss 0.074
2023-12-19 06:40:22,526 [slca.py] => Task 4, Epoch 18/20 => Loss 0.094
2023-12-19 06:40:24,676 [slca.py] => Task 4, Epoch 19/20 => Loss 0.057
2023-12-19 06:40:39,304 [slca.py] => Task 4, Epoch 20/20 => Loss 0.115, Train_accy 82.000, Test_accy 62.400
2023-12-19 06:40:57,434 [slca.py] => CA Task 4 => Loss 0.049, Test_accy 69.470
2023-12-19 06:41:08,937 [slca.py] => CA Task 4 => Loss 0.010, Test_accy 70.190
2023-12-19 06:41:20,586 [slca.py] => CA Task 4 => Loss 0.007, Test_accy 70.590
2023-12-19 06:41:32,317 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 70.770
2023-12-19 06:41:43,871 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 70.790
2023-12-19 06:41:54,942 [slca.py] => Exemplar size: 0
2023-12-19 06:41:55,616 [trainer.py] => No NME accuracy.
2023-12-19 06:41:55,616 [trainer.py] => CNN: {'total': 70.79, '00-09': 67.84, '10-19': 55.96, '20-29': 70.14, '30-39': 83.15, '40-49': 76.9, 'old': 69.26, 'new': 76.9}
2023-12-19 06:41:55,616 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06, 74.77, 70.79]
2023-12-19 06:41:55,616 [trainer.py] => CNN top1 avg: 76.20400000000001
2023-12-19 06:41:55,617 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65, 94.86, 93.29]

2023-12-19 06:41:55,617 [trainer.py] => All params: 85837106
2023-12-19 06:41:55,617 [trainer.py] => Trainable params: 85837106
2023-12-19 06:41:55,618 [slca.py] => Learning on 50-60
2023-12-19 06:41:57,951 [slca.py] => Task 5, Epoch 1/20 => Loss 2.417
2023-12-19 06:42:00,247 [slca.py] => Task 5, Epoch 2/20 => Loss 2.083
2023-12-19 06:42:02,573 [slca.py] => Task 5, Epoch 3/20 => Loss 1.442
2023-12-19 06:42:04,857 [slca.py] => Task 5, Epoch 4/20 => Loss 0.992
2023-12-19 06:42:21,721 [slca.py] => Task 5, Epoch 5/20 => Loss 0.519, Train_accy 75.000, Test_accy 56.100
2023-12-19 06:42:24,053 [slca.py] => Task 5, Epoch 6/20 => Loss 0.363
2023-12-19 06:42:26,379 [slca.py] => Task 5, Epoch 7/20 => Loss 0.244
2023-12-19 06:42:28,726 [slca.py] => Task 5, Epoch 8/20 => Loss 0.129
2023-12-19 06:42:30,965 [slca.py] => Task 5, Epoch 9/20 => Loss 0.304
2023-12-19 06:42:47,595 [slca.py] => Task 5, Epoch 10/20 => Loss 0.104, Train_accy 86.250, Test_accy 60.970
2023-12-19 06:42:49,893 [slca.py] => Task 5, Epoch 11/20 => Loss 0.166
2023-12-19 06:42:52,282 [slca.py] => Task 5, Epoch 12/20 => Loss 0.057
2023-12-19 06:42:54,631 [slca.py] => Task 5, Epoch 13/20 => Loss 0.040
2023-12-19 06:42:56,971 [slca.py] => Task 5, Epoch 14/20 => Loss 0.154
2023-12-19 06:43:13,686 [slca.py] => Task 5, Epoch 15/20 => Loss 0.033, Train_accy 83.330, Test_accy 60.890
2023-12-19 06:43:15,961 [slca.py] => Task 5, Epoch 16/20 => Loss 0.066
2023-12-19 06:43:18,289 [slca.py] => Task 5, Epoch 17/20 => Loss 0.020
2023-12-19 06:43:20,600 [slca.py] => Task 5, Epoch 18/20 => Loss 0.035
2023-12-19 06:43:22,908 [slca.py] => Task 5, Epoch 19/20 => Loss 0.055
2023-12-19 06:43:40,909 [slca.py] => Task 5, Epoch 20/20 => Loss 0.022, Train_accy 79.580, Test_accy 60.720
2023-12-19 06:43:59,399 [slca.py] => CA Task 5 => Loss 0.053, Test_accy 68.310
2023-12-19 06:44:14,164 [slca.py] => CA Task 5 => Loss 0.011, Test_accy 69.000
2023-12-19 06:44:29,379 [slca.py] => CA Task 5 => Loss 0.008, Test_accy 69.530
2023-12-19 06:44:43,296 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 69.600
2023-12-19 06:44:56,926 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 69.680
2023-12-19 06:45:09,885 [slca.py] => Exemplar size: 0
2023-12-19 06:45:10,634 [trainer.py] => No NME accuracy.
2023-12-19 06:45:10,635 [trainer.py] => CNN: {'total': 69.68, '00-09': 65.35, '10-19': 54.36, '20-29': 68.86, '30-39': 81.68, '40-49': 71.43, '50-59': 76.62, 'old': 68.31, 'new': 76.62}
2023-12-19 06:45:10,635 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06, 74.77, 70.79, 69.68]
2023-12-19 06:45:10,635 [trainer.py] => CNN top1 avg: 75.11666666666667
2023-12-19 06:45:10,635 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65, 94.86, 93.29, 92.61]

2023-12-19 06:45:10,635 [trainer.py] => All params: 85844796
2023-12-19 06:45:10,636 [trainer.py] => Trainable params: 85844796
2023-12-19 06:45:10,637 [slca.py] => Learning on 60-70
2023-12-19 06:45:13,349 [slca.py] => Task 6, Epoch 1/20 => Loss 2.268
2023-12-19 06:45:15,917 [slca.py] => Task 6, Epoch 2/20 => Loss 1.337
2023-12-19 06:45:18,421 [slca.py] => Task 6, Epoch 3/20 => Loss 0.958
2023-12-19 06:45:21,087 [slca.py] => Task 6, Epoch 4/20 => Loss 0.775
2023-12-19 06:45:42,098 [slca.py] => Task 6, Epoch 5/20 => Loss 0.308, Train_accy 79.640, Test_accy 58.190
2023-12-19 06:45:44,690 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 06:45:47,328 [slca.py] => Task 6, Epoch 7/20 => Loss 0.129
2023-12-19 06:45:50,123 [slca.py] => Task 6, Epoch 8/20 => Loss 0.070
2023-12-19 06:45:52,788 [slca.py] => Task 6, Epoch 9/20 => Loss 0.111
2023-12-19 06:46:11,986 [slca.py] => Task 6, Epoch 10/20 => Loss 0.045, Train_accy 83.930, Test_accy 59.840
2023-12-19 06:46:14,582 [slca.py] => Task 6, Epoch 11/20 => Loss 0.050
2023-12-19 06:46:17,161 [slca.py] => Task 6, Epoch 12/20 => Loss 0.094
2023-12-19 06:46:19,698 [slca.py] => Task 6, Epoch 13/20 => Loss 0.034
2023-12-19 06:46:22,187 [slca.py] => Task 6, Epoch 14/20 => Loss 0.066
2023-12-19 06:46:42,689 [slca.py] => Task 6, Epoch 15/20 => Loss 0.035, Train_accy 80.360, Test_accy 59.430
2023-12-19 06:46:45,248 [slca.py] => Task 6, Epoch 16/20 => Loss 0.184
2023-12-19 06:46:47,894 [slca.py] => Task 6, Epoch 17/20 => Loss 0.082
2023-12-19 06:46:50,417 [slca.py] => Task 6, Epoch 18/20 => Loss 0.162
2023-12-19 06:46:52,962 [slca.py] => Task 6, Epoch 19/20 => Loss 0.029
2023-12-19 06:47:11,984 [slca.py] => Task 6, Epoch 20/20 => Loss 0.040, Train_accy 81.790, Test_accy 60.550
2023-12-19 06:47:32,870 [slca.py] => CA Task 6 => Loss 0.055, Test_accy 68.210
2023-12-19 06:47:50,202 [slca.py] => CA Task 6 => Loss 0.012, Test_accy 68.940
2023-12-19 06:48:06,147 [slca.py] => CA Task 6 => Loss 0.008, Test_accy 69.230
2023-12-19 06:48:21,898 [slca.py] => CA Task 6 => Loss 0.006, Test_accy 69.340
2023-12-19 06:48:37,161 [slca.py] => CA Task 6 => Loss 0.006, Test_accy 69.340
2023-12-19 06:48:52,157 [slca.py] => Exemplar size: 0
2023-12-19 06:48:52,855 [trainer.py] => No NME accuracy.
2023-12-19 06:48:52,855 [trainer.py] => CNN: {'total': 69.34, '00-09': 63.3, '10-19': 50.96, '20-29': 66.02, '30-39': 80.37, '40-49': 71.18, '50-59': 75.13, '60-69': 78.48, 'old': 67.83, 'new': 78.48}
2023-12-19 06:48:52,855 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06, 74.77, 70.79, 69.68, 69.34]
2023-12-19 06:48:52,855 [trainer.py] => CNN top1 avg: 74.29142857142858
2023-12-19 06:48:52,855 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65, 94.86, 93.29, 92.61, 91.58]

2023-12-19 06:48:52,856 [trainer.py] => All params: 85852486
2023-12-19 06:48:52,856 [trainer.py] => Trainable params: 85852486
2023-12-19 06:48:52,857 [slca.py] => Learning on 70-80
2023-12-19 06:48:55,698 [slca.py] => Task 7, Epoch 1/20 => Loss 2.473
2023-12-19 06:48:58,462 [slca.py] => Task 7, Epoch 2/20 => Loss 1.783
2023-12-19 06:49:01,233 [slca.py] => Task 7, Epoch 3/20 => Loss 1.034
2023-12-19 06:49:03,969 [slca.py] => Task 7, Epoch 4/20 => Loss 0.464
2023-12-19 06:49:25,360 [slca.py] => Task 7, Epoch 5/20 => Loss 0.292, Train_accy 78.440, Test_accy 57.410
2023-12-19 06:49:28,091 [slca.py] => Task 7, Epoch 6/20 => Loss 0.207
2023-12-19 06:49:30,896 [slca.py] => Task 7, Epoch 7/20 => Loss 0.221
2023-12-19 06:49:33,675 [slca.py] => Task 7, Epoch 8/20 => Loss 0.213
2023-12-19 06:49:36,462 [slca.py] => Task 7, Epoch 9/20 => Loss 0.084
2023-12-19 06:49:59,615 [slca.py] => Task 7, Epoch 10/20 => Loss 0.191, Train_accy 82.810, Test_accy 59.680
2023-12-19 06:50:02,372 [slca.py] => Task 7, Epoch 11/20 => Loss 0.209
2023-12-19 06:50:05,177 [slca.py] => Task 7, Epoch 12/20 => Loss 0.124
2023-12-19 06:50:07,969 [slca.py] => Task 7, Epoch 13/20 => Loss 0.108
2023-12-19 06:50:10,746 [slca.py] => Task 7, Epoch 14/20 => Loss 0.142
2023-12-19 06:50:33,550 [slca.py] => Task 7, Epoch 15/20 => Loss 0.016, Train_accy 81.560, Test_accy 59.140
2023-12-19 06:50:36,298 [slca.py] => Task 7, Epoch 16/20 => Loss 0.106
2023-12-19 06:50:39,038 [slca.py] => Task 7, Epoch 17/20 => Loss 0.012
2023-12-19 06:50:41,854 [slca.py] => Task 7, Epoch 18/20 => Loss 0.057
2023-12-19 06:50:44,731 [slca.py] => Task 7, Epoch 19/20 => Loss 0.024
2023-12-19 06:51:06,429 [slca.py] => Task 7, Epoch 20/20 => Loss 0.048, Train_accy 79.060, Test_accy 58.910
2023-12-19 06:51:29,495 [slca.py] => CA Task 7 => Loss 0.054, Test_accy 66.150
2023-12-19 06:51:49,545 [slca.py] => CA Task 7 => Loss 0.012, Test_accy 66.920
2023-12-19 06:52:09,046 [slca.py] => CA Task 7 => Loss 0.008, Test_accy 67.310
2023-12-19 06:52:27,313 [slca.py] => CA Task 7 => Loss 0.007, Test_accy 67.580
2023-12-19 06:52:45,823 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 67.680
2023-12-19 06:53:05,331 [slca.py] => Exemplar size: 0
2023-12-19 06:53:06,043 [trainer.py] => No NME accuracy.
2023-12-19 06:53:06,044 [trainer.py] => CNN: {'total': 67.68, '00-09': 57.72, '10-19': 48.74, '20-29': 62.73, '30-39': 79.33, '40-49': 69.62, '50-59': 72.87, '60-69': 72.86, '70-79': 77.62, 'old': 66.26, 'new': 77.62}
2023-12-19 06:53:06,044 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06, 74.77, 70.79, 69.68, 69.34, 67.68]
2023-12-19 06:53:06,044 [trainer.py] => CNN top1 avg: 73.465
2023-12-19 06:53:06,044 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65, 94.86, 93.29, 92.61, 91.58, 90.94]

2023-12-19 06:53:06,045 [trainer.py] => All params: 85860176
2023-12-19 06:53:06,046 [trainer.py] => Trainable params: 85860176
2023-12-19 06:53:06,047 [slca.py] => Learning on 80-90
2023-12-19 06:53:09,092 [slca.py] => Task 8, Epoch 1/20 => Loss 2.452
2023-12-19 06:53:12,037 [slca.py] => Task 8, Epoch 2/20 => Loss 1.602
2023-12-19 06:53:15,981 [slca.py] => Task 8, Epoch 3/20 => Loss 0.842
2023-12-19 06:53:18,967 [slca.py] => Task 8, Epoch 4/20 => Loss 0.268
2023-12-19 06:53:44,314 [slca.py] => Task 8, Epoch 5/20 => Loss 0.176, Train_accy 80.000, Test_accy 57.440
2023-12-19 06:53:47,247 [slca.py] => Task 8, Epoch 6/20 => Loss 0.221
2023-12-19 06:53:50,254 [slca.py] => Task 8, Epoch 7/20 => Loss 0.088
2023-12-19 06:53:53,252 [slca.py] => Task 8, Epoch 8/20 => Loss 0.064
2023-12-19 06:53:56,246 [slca.py] => Task 8, Epoch 9/20 => Loss 0.045
2023-12-19 06:54:20,223 [slca.py] => Task 8, Epoch 10/20 => Loss 0.070, Train_accy 79.440, Test_accy 59.100
2023-12-19 06:54:23,306 [slca.py] => Task 8, Epoch 11/20 => Loss 0.014
2023-12-19 06:54:26,276 [slca.py] => Task 8, Epoch 12/20 => Loss 0.040
2023-12-19 06:54:29,230 [slca.py] => Task 8, Epoch 13/20 => Loss 0.024
2023-12-19 06:54:32,221 [slca.py] => Task 8, Epoch 14/20 => Loss 0.070
2023-12-19 06:54:56,095 [slca.py] => Task 8, Epoch 15/20 => Loss 0.021, Train_accy 84.170, Test_accy 59.200
2023-12-19 06:54:59,012 [slca.py] => Task 8, Epoch 16/20 => Loss 0.052
2023-12-19 06:55:01,958 [slca.py] => Task 8, Epoch 17/20 => Loss 0.045
2023-12-19 06:55:04,905 [slca.py] => Task 8, Epoch 18/20 => Loss 0.092
2023-12-19 06:55:07,869 [slca.py] => Task 8, Epoch 19/20 => Loss 0.012
2023-12-19 06:55:31,697 [slca.py] => Task 8, Epoch 20/20 => Loss 0.079, Train_accy 79.720, Test_accy 58.940
2023-12-19 06:55:57,119 [slca.py] => CA Task 8 => Loss 0.052, Test_accy 65.310
2023-12-19 06:56:17,589 [slca.py] => CA Task 8 => Loss 0.013, Test_accy 66.320
2023-12-19 06:56:37,841 [slca.py] => CA Task 8 => Loss 0.008, Test_accy 66.690
2023-12-19 06:56:58,152 [slca.py] => CA Task 8 => Loss 0.007, Test_accy 66.980
2023-12-19 06:57:19,609 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 67.010
2023-12-19 06:57:38,768 [slca.py] => Exemplar size: 0
2023-12-19 06:57:39,483 [trainer.py] => No NME accuracy.
2023-12-19 06:57:39,483 [trainer.py] => CNN: {'total': 67.01, '00-09': 54.27, '10-19': 44.38, '20-29': 59.18, '30-39': 77.71, '40-49': 66.03, '50-59': 73.05, '60-69': 71.24, '70-79': 77.64, '80-89': 79.6, 'old': 65.44, 'new': 79.6}
2023-12-19 06:57:39,483 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06, 74.77, 70.79, 69.68, 69.34, 67.68, 67.01]
2023-12-19 06:57:39,483 [trainer.py] => CNN top1 avg: 72.74777777777778
2023-12-19 06:57:39,483 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65, 94.86, 93.29, 92.61, 91.58, 90.94, 90.26]

2023-12-19 06:57:39,484 [trainer.py] => All params: 85867866
2023-12-19 06:57:39,485 [trainer.py] => Trainable params: 85867866
2023-12-19 06:57:39,485 [slca.py] => Learning on 90-100
2023-12-19 06:57:42,774 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 06:57:45,921 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 06:57:49,421 [slca.py] => Task 9, Epoch 3/20 => Loss 0.665
2023-12-19 06:57:52,639 [slca.py] => Task 9, Epoch 4/20 => Loss 0.252
2023-12-19 06:58:18,997 [slca.py] => Task 9, Epoch 5/20 => Loss 0.080, Train_accy 81.000, Test_accy 56.170
2023-12-19 06:58:22,213 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 06:58:25,418 [slca.py] => Task 9, Epoch 7/20 => Loss 0.105
2023-12-19 06:58:28,630 [slca.py] => Task 9, Epoch 8/20 => Loss 0.117
2023-12-19 06:58:31,832 [slca.py] => Task 9, Epoch 9/20 => Loss 0.097
2023-12-19 06:58:58,123 [slca.py] => Task 9, Epoch 10/20 => Loss 0.014, Train_accy 83.000, Test_accy 57.240
2023-12-19 06:59:01,375 [slca.py] => Task 9, Epoch 11/20 => Loss 0.026
2023-12-19 06:59:04,529 [slca.py] => Task 9, Epoch 12/20 => Loss 0.053
2023-12-19 06:59:07,780 [slca.py] => Task 9, Epoch 13/20 => Loss 0.041
2023-12-19 06:59:10,981 [slca.py] => Task 9, Epoch 14/20 => Loss 0.088
2023-12-19 06:59:39,021 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 81.250, Test_accy 56.590
2023-12-19 06:59:43,276 [slca.py] => Task 9, Epoch 16/20 => Loss 0.021
2023-12-19 06:59:46,504 [slca.py] => Task 9, Epoch 17/20 => Loss 0.113
2023-12-19 06:59:49,668 [slca.py] => Task 9, Epoch 18/20 => Loss 0.008
2023-12-19 06:59:52,924 [slca.py] => Task 9, Epoch 19/20 => Loss 0.031
2023-12-19 07:00:20,007 [slca.py] => Task 9, Epoch 20/20 => Loss 0.040, Train_accy 80.000, Test_accy 57.440
2023-12-19 07:00:48,120 [slca.py] => CA Task 9 => Loss 0.053, Test_accy 64.010
2023-12-19 07:01:10,623 [slca.py] => CA Task 9 => Loss 0.013, Test_accy 64.850
2023-12-19 07:01:33,221 [slca.py] => CA Task 9 => Loss 0.009, Test_accy 65.400
2023-12-19 07:01:57,120 [slca.py] => CA Task 9 => Loss 0.007, Test_accy 65.390
2023-12-19 07:02:19,719 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 65.430
2023-12-19 07:02:40,938 [slca.py] => Exemplar size: 0
2023-12-19 07:02:41,677 [trainer.py] => No NME accuracy.
2023-12-19 07:02:41,677 [trainer.py] => CNN: {'total': 65.43, '00-09': 49.85, '10-19': 45.45, '20-29': 60.96, '30-39': 73.35, '40-49': 65.4, '50-59': 70.24, '60-69': 70.77, '70-79': 75.13, '80-89': 76.55, '90-99': 66.7, 'old': 65.29, 'new': 66.7}
2023-12-19 07:02:41,677 [trainer.py] => CNN top1 curve: [87.61, 74.79, 73.06, 74.77, 70.79, 69.68, 69.34, 67.68, 67.01, 65.43]
2023-12-19 07:02:41,678 [trainer.py] => CNN top1 avg: 72.016
2023-12-19 07:02:41,678 [trainer.py] => CNN top5 curve: [99.33, 97.4, 95.65, 94.86, 93.29, 92.61, 91.58, 90.94, 90.26, 90.3]

2023-12-19 07:02:41,679 [trainer.py] => final accs: [65.43]
2023-12-19 07:02:41,679 [trainer.py] => avg accs: [72.016]
