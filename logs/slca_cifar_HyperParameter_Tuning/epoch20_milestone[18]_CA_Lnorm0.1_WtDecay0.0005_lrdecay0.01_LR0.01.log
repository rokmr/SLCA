2023-12-19 05:50:52,174 [trainer.py] => config: /home/gayathri/rohit/SLCA/exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 05:50:52,174 [trainer.py] => test_only: False
2023-12-19 05:50:52,174 [trainer.py] => prefix: reproduce
2023-12-19 05:50:52,174 [trainer.py] => dataset: cifar100_224
2023-12-19 05:50:52,174 [trainer.py] => memory_size: 0
2023-12-19 05:50:52,174 [trainer.py] => memory_per_class: 0
2023-12-19 05:50:52,174 [trainer.py] => fixed_memory: False
2023-12-19 05:50:52,174 [trainer.py] => shuffle: False
2023-12-19 05:50:52,174 [trainer.py] => init_cls: 10
2023-12-19 05:50:52,174 [trainer.py] => increment: 10
2023-12-19 05:50:52,174 [trainer.py] => model_name: slca_cifar
2023-12-19 05:50:52,174 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 05:50:52,174 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 05:50:52,174 [trainer.py] => device: [device(type='cuda', index=1)]
2023-12-19 05:50:52,174 [trainer.py] => seed: 0
2023-12-19 05:50:52,174 [trainer.py] => epochs: 20
2023-12-19 05:50:52,174 [trainer.py] => ca_epochs: 5
2023-12-19 05:50:52,174 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-19 05:50:52,174 [trainer.py] => milestones: [18]
2023-12-19 05:50:52,174 [trainer.py] => lr: 0.01
2023-12-19 05:50:52,174 [trainer.py] => lr_decay: 0.01
2023-12-19 05:50:52,174 [trainer.py] => weight_decay: 0.0005
2023-12-19 05:50:52,174 [trainer.py] => u_batch_size: 256
2023-12-19 05:50:52,174 [trainer.py] => s_batch_size: 3
2023-12-19 05:50:52,174 [trainer.py] => multicrop: 2
2023-12-19 05:50:52,174 [trainer.py] => us_multicrop: 2
2023-12-19 05:50:52,175 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 05:50:52,175 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 05:50:52,175 [trainer.py] => buffer_size: 500
2023-12-19 05:50:52,175 [trainer.py] => run_id: 0
2023-12-19 05:50:53,673 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 05:50:55,526 [trainer.py] => All params: 85798656
2023-12-19 05:50:55,526 [trainer.py] => Trainable params: 85798656
2023-12-19 05:50:55,527 [slca.py] => Learning on 0-10
2023-12-19 05:51:00,864 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 05:51:01,884 [slca.py] => Task 0, Epoch 2/20 => Loss 2.314
2023-12-19 05:51:02,982 [slca.py] => Task 0, Epoch 3/20 => Loss 2.105
2023-12-19 05:51:04,079 [slca.py] => Task 0, Epoch 4/20 => Loss 1.740
2023-12-19 05:51:08,760 [slca.py] => Task 0, Epoch 5/20 => Loss 1.514, Train_accy 72.500, Test_accy 57.920
2023-12-19 05:51:09,837 [slca.py] => Task 0, Epoch 6/20 => Loss 1.184
2023-12-19 05:51:10,885 [slca.py] => Task 0, Epoch 7/20 => Loss 0.837
2023-12-19 05:51:11,939 [slca.py] => Task 0, Epoch 8/20 => Loss 0.776
2023-12-19 05:51:13,010 [slca.py] => Task 0, Epoch 9/20 => Loss 0.625
2023-12-19 05:51:17,735 [slca.py] => Task 0, Epoch 10/20 => Loss 0.476, Train_accy 95.000, Test_accy 79.240
2023-12-19 05:51:18,802 [slca.py] => Task 0, Epoch 11/20 => Loss 0.338
2023-12-19 05:51:19,848 [slca.py] => Task 0, Epoch 12/20 => Loss 0.397
2023-12-19 05:51:20,956 [slca.py] => Task 0, Epoch 13/20 => Loss 0.224
2023-12-19 05:51:22,015 [slca.py] => Task 0, Epoch 14/20 => Loss 0.202
2023-12-19 05:51:26,687 [slca.py] => Task 0, Epoch 15/20 => Loss 0.200, Train_accy 97.500, Test_accy 86.720
2023-12-19 05:51:27,724 [slca.py] => Task 0, Epoch 16/20 => Loss 0.106
2023-12-19 05:51:28,815 [slca.py] => Task 0, Epoch 17/20 => Loss 0.237
2023-12-19 05:51:29,852 [slca.py] => Task 0, Epoch 18/20 => Loss 0.087
2023-12-19 05:51:30,907 [slca.py] => Task 0, Epoch 19/20 => Loss 0.050
2023-12-19 05:51:35,749 [slca.py] => Task 0, Epoch 20/20 => Loss 0.114, Train_accy 97.500, Test_accy 88.500
2023-12-19 05:51:43,294 [slca.py] => CA Task 0 => Loss 0.018, Test_accy 90.400
2023-12-19 05:51:46,083 [slca.py] => CA Task 0 => Loss 0.007, Test_accy 91.410
2023-12-19 05:51:49,136 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 91.630
2023-12-19 05:51:52,079 [slca.py] => CA Task 0 => Loss 0.003, Test_accy 91.630
2023-12-19 05:51:54,878 [slca.py] => CA Task 0 => Loss 0.003, Test_accy 91.630
2023-12-19 05:51:57,634 [slca.py] => Exemplar size: 0
2023-12-19 05:51:58,359 [trainer.py] => No NME accuracy.
2023-12-19 05:51:58,360 [trainer.py] => CNN: {'total': 91.63, '00-09': 91.63, 'old': 0, 'new': 91.63}
2023-12-19 05:51:58,360 [trainer.py] => CNN top1 curve: [91.63]
2023-12-19 05:51:58,360 [trainer.py] => CNN top1 avg: 91.63
2023-12-19 05:51:58,360 [trainer.py] => CNN top5 curve: [99.78]

2023-12-19 05:51:58,362 [trainer.py] => All params: 85806346
2023-12-19 05:51:58,363 [trainer.py] => Trainable params: 85806346
2023-12-19 05:51:58,365 [slca.py] => Learning on 10-20
2023-12-19 05:51:59,761 [slca.py] => Task 1, Epoch 1/20 => Loss 2.418
2023-12-19 05:52:01,103 [slca.py] => Task 1, Epoch 2/20 => Loss 2.353
2023-12-19 05:52:02,426 [slca.py] => Task 1, Epoch 3/20 => Loss 2.058
2023-12-19 05:52:03,788 [slca.py] => Task 1, Epoch 4/20 => Loss 1.756
2023-12-19 05:52:11,789 [slca.py] => Task 1, Epoch 5/20 => Loss 1.349, Train_accy 60.000, Test_accy 47.500
2023-12-19 05:52:13,155 [slca.py] => Task 1, Epoch 6/20 => Loss 0.904
2023-12-19 05:52:14,492 [slca.py] => Task 1, Epoch 7/20 => Loss 0.705
2023-12-19 05:52:15,858 [slca.py] => Task 1, Epoch 8/20 => Loss 0.603
2023-12-19 05:52:17,234 [slca.py] => Task 1, Epoch 9/20 => Loss 0.375
2023-12-19 05:52:24,676 [slca.py] => Task 1, Epoch 10/20 => Loss 0.196, Train_accy 90.000, Test_accy 68.070
2023-12-19 05:52:26,064 [slca.py] => Task 1, Epoch 11/20 => Loss 0.217
2023-12-19 05:52:27,432 [slca.py] => Task 1, Epoch 12/20 => Loss 0.211
2023-12-19 05:52:28,773 [slca.py] => Task 1, Epoch 13/20 => Loss 0.073
2023-12-19 05:52:30,101 [slca.py] => Task 1, Epoch 14/20 => Loss 0.150
2023-12-19 05:52:37,357 [slca.py] => Task 1, Epoch 15/20 => Loss 0.146, Train_accy 96.250, Test_accy 75.570
2023-12-19 05:52:38,741 [slca.py] => Task 1, Epoch 16/20 => Loss 0.058
2023-12-19 05:52:40,118 [slca.py] => Task 1, Epoch 17/20 => Loss 0.093
2023-12-19 05:52:41,484 [slca.py] => Task 1, Epoch 18/20 => Loss 0.085
2023-12-19 05:52:42,836 [slca.py] => Task 1, Epoch 19/20 => Loss 0.038
2023-12-19 05:52:50,132 [slca.py] => Task 1, Epoch 20/20 => Loss 0.073, Train_accy 95.000, Test_accy 77.760
2023-12-19 05:53:00,712 [slca.py] => CA Task 1 => Loss 0.018, Test_accy 79.320
2023-12-19 05:53:05,809 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 80.260
2023-12-19 05:53:11,504 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 80.780
2023-12-19 05:53:16,587 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 80.990
2023-12-19 05:53:21,525 [slca.py] => CA Task 1 => Loss 0.002, Test_accy 80.990
2023-12-19 05:53:27,877 [slca.py] => Exemplar size: 0
2023-12-19 05:53:28,567 [trainer.py] => No NME accuracy.
2023-12-19 05:53:28,567 [trainer.py] => CNN: {'total': 80.99, '00-09': 87.5, '10-19': 74.48, 'old': 87.5, 'new': 74.48}
2023-12-19 05:53:28,567 [trainer.py] => CNN top1 curve: [91.63, 80.99]
2023-12-19 05:53:28,568 [trainer.py] => CNN top1 avg: 86.31
2023-12-19 05:53:28,568 [trainer.py] => CNN top5 curve: [99.78, 98.28]

2023-12-19 05:53:28,569 [trainer.py] => All params: 85814036
2023-12-19 05:53:28,571 [trainer.py] => Trainable params: 85814036
2023-12-19 05:53:28,572 [slca.py] => Learning on 20-30
2023-12-19 05:53:30,373 [slca.py] => Task 2, Epoch 1/20 => Loss 2.414
2023-12-19 05:53:31,990 [slca.py] => Task 2, Epoch 2/20 => Loss 2.161
2023-12-19 05:53:33,654 [slca.py] => Task 2, Epoch 3/20 => Loss 1.709
2023-12-19 05:53:35,307 [slca.py] => Task 2, Epoch 4/20 => Loss 1.499
2023-12-19 05:53:46,694 [slca.py] => Task 2, Epoch 5/20 => Loss 1.128, Train_accy 69.170, Test_accy 53.360
2023-12-19 05:53:48,408 [slca.py] => Task 2, Epoch 6/20 => Loss 0.863
2023-12-19 05:53:50,099 [slca.py] => Task 2, Epoch 7/20 => Loss 0.475
2023-12-19 05:53:51,757 [slca.py] => Task 2, Epoch 8/20 => Loss 0.398
2023-12-19 05:53:53,426 [slca.py] => Task 2, Epoch 9/20 => Loss 0.323
2023-12-19 05:54:03,336 [slca.py] => Task 2, Epoch 10/20 => Loss 0.322, Train_accy 89.170, Test_accy 70.890
2023-12-19 05:54:05,009 [slca.py] => Task 2, Epoch 11/20 => Loss 0.124
2023-12-19 05:54:06,775 [slca.py] => Task 2, Epoch 12/20 => Loss 0.053
2023-12-19 05:54:08,455 [slca.py] => Task 2, Epoch 13/20 => Loss 0.061
2023-12-19 05:54:10,468 [slca.py] => Task 2, Epoch 14/20 => Loss 0.098
2023-12-19 05:54:20,313 [slca.py] => Task 2, Epoch 15/20 => Loss 0.175, Train_accy 92.500, Test_accy 75.820
2023-12-19 05:54:22,218 [slca.py] => Task 2, Epoch 16/20 => Loss 0.104
2023-12-19 05:54:24,172 [slca.py] => Task 2, Epoch 17/20 => Loss 0.192
2023-12-19 05:54:26,147 [slca.py] => Task 2, Epoch 18/20 => Loss 0.068
2023-12-19 05:54:28,208 [slca.py] => Task 2, Epoch 19/20 => Loss 0.052
2023-12-19 05:54:38,865 [slca.py] => Task 2, Epoch 20/20 => Loss 0.053, Train_accy 90.830, Test_accy 75.750
2023-12-19 05:54:51,711 [slca.py] => CA Task 2 => Loss 0.018, Test_accy 77.510
2023-12-19 05:54:58,845 [slca.py] => CA Task 2 => Loss 0.005, Test_accy 78.230
2023-12-19 05:55:06,901 [slca.py] => CA Task 2 => Loss 0.003, Test_accy 78.400
2023-12-19 05:55:15,158 [slca.py] => CA Task 2 => Loss 0.003, Test_accy 78.360
2023-12-19 05:55:22,335 [slca.py] => CA Task 2 => Loss 0.003, Test_accy 78.430
2023-12-19 05:55:29,788 [slca.py] => Exemplar size: 0
2023-12-19 05:55:31,433 [trainer.py] => No NME accuracy.
2023-12-19 05:55:31,433 [trainer.py] => CNN: {'total': 78.43, '00-09': 81.49, '10-19': 70.3, '20-29': 83.52, 'old': 75.88, 'new': 83.52}
2023-12-19 05:55:31,433 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43]
2023-12-19 05:55:31,434 [trainer.py] => CNN top1 avg: 83.68333333333334
2023-12-19 05:55:31,434 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08]

2023-12-19 05:55:31,435 [trainer.py] => All params: 85821726
2023-12-19 05:55:31,437 [trainer.py] => Trainable params: 85821726
2023-12-19 05:55:31,438 [slca.py] => Learning on 30-40
2023-12-19 05:55:33,532 [slca.py] => Task 3, Epoch 1/20 => Loss 2.352
2023-12-19 05:55:35,416 [slca.py] => Task 3, Epoch 2/20 => Loss 1.778
2023-12-19 05:55:37,305 [slca.py] => Task 3, Epoch 3/20 => Loss 1.212
2023-12-19 05:55:39,196 [slca.py] => Task 3, Epoch 4/20 => Loss 0.901
2023-12-19 05:55:51,658 [slca.py] => Task 3, Epoch 5/20 => Loss 0.452, Train_accy 86.880, Test_accy 70.790
2023-12-19 05:55:53,519 [slca.py] => Task 3, Epoch 6/20 => Loss 0.385
2023-12-19 05:55:55,371 [slca.py] => Task 3, Epoch 7/20 => Loss 0.268
2023-12-19 05:55:57,281 [slca.py] => Task 3, Epoch 8/20 => Loss 0.074
2023-12-19 05:55:59,188 [slca.py] => Task 3, Epoch 9/20 => Loss 0.271
2023-12-19 05:56:11,569 [slca.py] => Task 3, Epoch 10/20 => Loss 0.089, Train_accy 83.750, Test_accy 71.400
2023-12-19 05:56:13,476 [slca.py] => Task 3, Epoch 11/20 => Loss 0.139
2023-12-19 05:56:15,378 [slca.py] => Task 3, Epoch 12/20 => Loss 0.102
2023-12-19 05:56:17,286 [slca.py] => Task 3, Epoch 13/20 => Loss 0.094
2023-12-19 05:56:19,189 [slca.py] => Task 3, Epoch 14/20 => Loss 0.064
2023-12-19 05:56:31,317 [slca.py] => Task 3, Epoch 15/20 => Loss 0.081, Train_accy 84.380, Test_accy 71.420
2023-12-19 05:56:33,164 [slca.py] => Task 3, Epoch 16/20 => Loss 0.083
2023-12-19 05:56:35,009 [slca.py] => Task 3, Epoch 17/20 => Loss 0.039
2023-12-19 05:56:36,989 [slca.py] => Task 3, Epoch 18/20 => Loss 0.056
2023-12-19 05:56:38,996 [slca.py] => Task 3, Epoch 19/20 => Loss 0.045
2023-12-19 05:56:51,313 [slca.py] => Task 3, Epoch 20/20 => Loss 0.004, Train_accy 86.250, Test_accy 70.640
2023-12-19 05:57:05,432 [slca.py] => CA Task 3 => Loss 0.030, Test_accy 76.080
2023-12-19 05:57:15,992 [slca.py] => CA Task 3 => Loss 0.007, Test_accy 76.410
2023-12-19 05:57:25,293 [slca.py] => CA Task 3 => Loss 0.004, Test_accy 76.740
2023-12-19 05:57:34,766 [slca.py] => CA Task 3 => Loss 0.003, Test_accy 76.810
2023-12-19 05:57:44,106 [slca.py] => CA Task 3 => Loss 0.003, Test_accy 76.840
2023-12-19 05:57:53,105 [slca.py] => Exemplar size: 0
2023-12-19 05:57:55,060 [trainer.py] => No NME accuracy.
2023-12-19 05:57:55,060 [trainer.py] => CNN: {'total': 76.84, '00-09': 73.64, '10-19': 62.8, '20-29': 81.65, '30-39': 89.24, 'old': 72.7, 'new': 89.24}
2023-12-19 05:57:55,060 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43, 76.84]
2023-12-19 05:57:55,060 [trainer.py] => CNN top1 avg: 81.9725
2023-12-19 05:57:55,060 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08, 95.54]

2023-12-19 05:57:55,061 [trainer.py] => All params: 85829416
2023-12-19 05:57:55,062 [trainer.py] => Trainable params: 85829416
2023-12-19 05:57:55,063 [slca.py] => Learning on 40-50
2023-12-19 05:57:57,170 [slca.py] => Task 4, Epoch 1/20 => Loss 2.315
2023-12-19 05:57:59,277 [slca.py] => Task 4, Epoch 2/20 => Loss 1.683
2023-12-19 05:58:01,365 [slca.py] => Task 4, Epoch 3/20 => Loss 0.837
2023-12-19 05:58:03,410 [slca.py] => Task 4, Epoch 4/20 => Loss 0.492
2023-12-19 05:58:18,272 [slca.py] => Task 4, Epoch 5/20 => Loss 0.178, Train_accy 83.500, Test_accy 63.140
2023-12-19 05:58:20,551 [slca.py] => Task 4, Epoch 6/20 => Loss 0.128
2023-12-19 05:58:22,644 [slca.py] => Task 4, Epoch 7/20 => Loss 0.053
2023-12-19 05:58:24,820 [slca.py] => Task 4, Epoch 8/20 => Loss 0.321
2023-12-19 05:58:26,976 [slca.py] => Task 4, Epoch 9/20 => Loss 0.095
2023-12-19 05:58:41,590 [slca.py] => Task 4, Epoch 10/20 => Loss 0.100, Train_accy 83.000, Test_accy 67.630
2023-12-19 05:58:43,737 [slca.py] => Task 4, Epoch 11/20 => Loss 0.058
2023-12-19 05:58:45,872 [slca.py] => Task 4, Epoch 12/20 => Loss 0.062
2023-12-19 05:58:47,963 [slca.py] => Task 4, Epoch 13/20 => Loss 0.031
2023-12-19 05:58:50,077 [slca.py] => Task 4, Epoch 14/20 => Loss 0.004
2023-12-19 05:59:06,377 [slca.py] => Task 4, Epoch 15/20 => Loss 0.048, Train_accy 85.500, Test_accy 66.830
2023-12-19 05:59:08,500 [slca.py] => Task 4, Epoch 16/20 => Loss 0.061
2023-12-19 05:59:10,586 [slca.py] => Task 4, Epoch 17/20 => Loss 0.021
2023-12-19 05:59:12,704 [slca.py] => Task 4, Epoch 18/20 => Loss 0.040
2023-12-19 05:59:14,804 [slca.py] => Task 4, Epoch 19/20 => Loss 0.015
2023-12-19 05:59:29,279 [slca.py] => Task 4, Epoch 20/20 => Loss 0.083, Train_accy 88.000, Test_accy 66.230
2023-12-19 05:59:45,885 [slca.py] => CA Task 4 => Loss 0.031, Test_accy 71.690
2023-12-19 05:59:57,436 [slca.py] => CA Task 4 => Loss 0.006, Test_accy 72.360
2023-12-19 06:00:08,948 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 72.660
2023-12-19 06:00:20,479 [slca.py] => CA Task 4 => Loss 0.004, Test_accy 72.740
2023-12-19 06:00:32,886 [slca.py] => CA Task 4 => Loss 0.004, Test_accy 72.760
2023-12-19 06:00:43,951 [slca.py] => Exemplar size: 0
2023-12-19 06:00:46,045 [trainer.py] => No NME accuracy.
2023-12-19 06:00:46,046 [trainer.py] => CNN: {'total': 72.76, '00-09': 70.44, '10-19': 59.56, '20-29': 75.15, '30-39': 84.65, '40-49': 74.0, 'old': 72.44, 'new': 74.0}
2023-12-19 06:00:46,046 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43, 76.84, 72.76]
2023-12-19 06:00:46,046 [trainer.py] => CNN top1 avg: 80.13
2023-12-19 06:00:46,046 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08, 95.54, 94.23]

2023-12-19 06:00:46,048 [trainer.py] => All params: 85837106
2023-12-19 06:00:46,049 [trainer.py] => Trainable params: 85837106
2023-12-19 06:00:46,051 [slca.py] => Learning on 50-60
2023-12-19 06:00:48,425 [slca.py] => Task 5, Epoch 1/20 => Loss 2.345
2023-12-19 06:00:50,711 [slca.py] => Task 5, Epoch 2/20 => Loss 1.691
2023-12-19 06:00:52,965 [slca.py] => Task 5, Epoch 3/20 => Loss 0.864
2023-12-19 06:00:55,262 [slca.py] => Task 5, Epoch 4/20 => Loss 0.515
2023-12-19 06:01:11,886 [slca.py] => Task 5, Epoch 5/20 => Loss 0.157, Train_accy 75.830, Test_accy 60.360
2023-12-19 06:01:14,239 [slca.py] => Task 5, Epoch 6/20 => Loss 0.127
2023-12-19 06:01:16,599 [slca.py] => Task 5, Epoch 7/20 => Loss 0.061
2023-12-19 06:01:18,898 [slca.py] => Task 5, Epoch 8/20 => Loss 0.037
2023-12-19 06:01:21,226 [slca.py] => Task 5, Epoch 9/20 => Loss 0.222
2023-12-19 06:01:38,057 [slca.py] => Task 5, Epoch 10/20 => Loss 0.074, Train_accy 87.920, Test_accy 64.130
2023-12-19 06:01:40,319 [slca.py] => Task 5, Epoch 11/20 => Loss 0.122
2023-12-19 06:01:42,603 [slca.py] => Task 5, Epoch 12/20 => Loss 0.013
2023-12-19 06:01:44,913 [slca.py] => Task 5, Epoch 13/20 => Loss 0.007
2023-12-19 06:01:47,208 [slca.py] => Task 5, Epoch 14/20 => Loss 0.121
2023-12-19 06:02:03,889 [slca.py] => Task 5, Epoch 15/20 => Loss 0.019, Train_accy 85.830, Test_accy 64.560
2023-12-19 06:02:06,218 [slca.py] => Task 5, Epoch 16/20 => Loss 0.013
2023-12-19 06:02:08,518 [slca.py] => Task 5, Epoch 17/20 => Loss 0.005
2023-12-19 06:02:10,989 [slca.py] => Task 5, Epoch 18/20 => Loss 0.013
2023-12-19 06:02:13,312 [slca.py] => Task 5, Epoch 19/20 => Loss 0.031
2023-12-19 06:02:32,193 [slca.py] => Task 5, Epoch 20/20 => Loss 0.005, Train_accy 82.500, Test_accy 64.590
2023-12-19 06:02:50,649 [slca.py] => CA Task 5 => Loss 0.034, Test_accy 70.110
2023-12-19 06:03:05,470 [slca.py] => CA Task 5 => Loss 0.007, Test_accy 70.600
2023-12-19 06:03:20,384 [slca.py] => CA Task 5 => Loss 0.005, Test_accy 70.860
2023-12-19 06:03:34,034 [slca.py] => CA Task 5 => Loss 0.004, Test_accy 71.010
2023-12-19 06:03:47,453 [slca.py] => CA Task 5 => Loss 0.004, Test_accy 70.970
2023-12-19 06:04:02,118 [slca.py] => Exemplar size: 0
2023-12-19 06:04:02,873 [trainer.py] => No NME accuracy.
2023-12-19 06:04:02,873 [trainer.py] => CNN: {'total': 70.97, '00-09': 68.9, '10-19': 56.49, '20-29': 73.94, '30-39': 83.52, '40-49': 68.47, '50-59': 74.67, 'old': 70.24, 'new': 74.67}
2023-12-19 06:04:02,874 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43, 76.84, 72.76, 70.97]
2023-12-19 06:04:02,874 [trainer.py] => CNN top1 avg: 78.60333333333334
2023-12-19 06:04:02,874 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08, 95.54, 94.23, 93.12]

2023-12-19 06:04:02,874 [trainer.py] => All params: 85844796
2023-12-19 06:04:02,875 [trainer.py] => Trainable params: 85844796
2023-12-19 06:04:02,875 [slca.py] => Learning on 60-70
2023-12-19 06:04:05,520 [slca.py] => Task 6, Epoch 1/20 => Loss 2.244
2023-12-19 06:04:08,123 [slca.py] => Task 6, Epoch 2/20 => Loss 0.898
2023-12-19 06:04:10,748 [slca.py] => Task 6, Epoch 3/20 => Loss 0.539
2023-12-19 06:04:13,285 [slca.py] => Task 6, Epoch 4/20 => Loss 0.461
2023-12-19 06:04:33,166 [slca.py] => Task 6, Epoch 5/20 => Loss 0.247, Train_accy 82.860, Test_accy 63.300
2023-12-19 06:04:35,728 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 06:04:38,257 [slca.py] => Task 6, Epoch 7/20 => Loss 0.218
2023-12-19 06:04:40,858 [slca.py] => Task 6, Epoch 8/20 => Loss 0.009
2023-12-19 06:04:43,471 [slca.py] => Task 6, Epoch 9/20 => Loss 0.056
2023-12-19 06:05:02,567 [slca.py] => Task 6, Epoch 10/20 => Loss 0.029, Train_accy 85.360, Test_accy 62.280
2023-12-19 06:05:05,121 [slca.py] => Task 6, Epoch 11/20 => Loss 0.054
2023-12-19 06:05:07,680 [slca.py] => Task 6, Epoch 12/20 => Loss 0.029
2023-12-19 06:05:10,334 [slca.py] => Task 6, Epoch 13/20 => Loss 0.038
2023-12-19 06:05:13,035 [slca.py] => Task 6, Epoch 14/20 => Loss 0.050
2023-12-19 06:05:32,438 [slca.py] => Task 6, Epoch 15/20 => Loss 0.039, Train_accy 82.860, Test_accy 62.720
2023-12-19 06:05:35,026 [slca.py] => Task 6, Epoch 16/20 => Loss 0.082
2023-12-19 06:05:37,747 [slca.py] => Task 6, Epoch 17/20 => Loss 0.150
2023-12-19 06:05:40,629 [slca.py] => Task 6, Epoch 18/20 => Loss 0.053
2023-12-19 06:05:43,172 [slca.py] => Task 6, Epoch 19/20 => Loss 0.003
2023-12-19 06:06:02,625 [slca.py] => Task 6, Epoch 20/20 => Loss 0.039, Train_accy 85.000, Test_accy 61.920
2023-12-19 06:06:23,432 [slca.py] => CA Task 6 => Loss 0.040, Test_accy 68.750
2023-12-19 06:06:40,990 [slca.py] => CA Task 6 => Loss 0.008, Test_accy 69.500
2023-12-19 06:06:56,639 [slca.py] => CA Task 6 => Loss 0.006, Test_accy 69.850
2023-12-19 06:07:14,096 [slca.py] => CA Task 6 => Loss 0.005, Test_accy 69.980
2023-12-19 06:07:30,322 [slca.py] => CA Task 6 => Loss 0.005, Test_accy 70.010
2023-12-19 06:07:45,319 [slca.py] => Exemplar size: 0
2023-12-19 06:07:46,001 [trainer.py] => No NME accuracy.
2023-12-19 06:07:46,001 [trainer.py] => CNN: {'total': 70.01, '00-09': 63.9, '10-19': 52.68, '20-29': 70.28, '30-39': 83.83, '40-49': 66.94, '50-59': 73.21, '60-69': 79.29, 'old': 68.47, 'new': 79.29}
2023-12-19 06:07:46,002 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43, 76.84, 72.76, 70.97, 70.01]
2023-12-19 06:07:46,002 [trainer.py] => CNN top1 avg: 77.37571428571428
2023-12-19 06:07:46,002 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08, 95.54, 94.23, 93.12, 91.88]

2023-12-19 06:07:46,004 [trainer.py] => All params: 85852486
2023-12-19 06:07:46,005 [trainer.py] => Trainable params: 85852486
2023-12-19 06:07:46,006 [slca.py] => Learning on 70-80
2023-12-19 06:07:48,861 [slca.py] => Task 7, Epoch 1/20 => Loss 2.420
2023-12-19 06:07:51,598 [slca.py] => Task 7, Epoch 2/20 => Loss 1.229
2023-12-19 06:07:54,362 [slca.py] => Task 7, Epoch 3/20 => Loss 0.577
2023-12-19 06:07:57,084 [slca.py] => Task 7, Epoch 4/20 => Loss 0.179
2023-12-19 06:08:18,603 [slca.py] => Task 7, Epoch 5/20 => Loss 0.105, Train_accy 81.560, Test_accy 59.700
2023-12-19 06:08:21,313 [slca.py] => Task 7, Epoch 6/20 => Loss 0.154
2023-12-19 06:08:24,076 [slca.py] => Task 7, Epoch 7/20 => Loss 0.155
2023-12-19 06:08:26,887 [slca.py] => Task 7, Epoch 8/20 => Loss 0.154
2023-12-19 06:08:29,604 [slca.py] => Task 7, Epoch 9/20 => Loss 0.035
2023-12-19 06:08:51,029 [slca.py] => Task 7, Epoch 10/20 => Loss 0.151, Train_accy 84.690, Test_accy 60.070
2023-12-19 06:08:53,754 [slca.py] => Task 7, Epoch 11/20 => Loss 0.227
2023-12-19 06:08:56,473 [slca.py] => Task 7, Epoch 12/20 => Loss 0.092
2023-12-19 06:08:59,215 [slca.py] => Task 7, Epoch 13/20 => Loss 0.080
2023-12-19 06:09:02,002 [slca.py] => Task 7, Epoch 14/20 => Loss 0.142
2023-12-19 06:09:23,660 [slca.py] => Task 7, Epoch 15/20 => Loss 0.004, Train_accy 80.000, Test_accy 58.900
2023-12-19 06:09:26,378 [slca.py] => Task 7, Epoch 16/20 => Loss 0.134
2023-12-19 06:09:29,083 [slca.py] => Task 7, Epoch 17/20 => Loss 0.133
2023-12-19 06:09:31,893 [slca.py] => Task 7, Epoch 18/20 => Loss 0.080
2023-12-19 06:09:34,618 [slca.py] => Task 7, Epoch 19/20 => Loss 0.003
2023-12-19 06:09:56,150 [slca.py] => Task 7, Epoch 20/20 => Loss 0.058, Train_accy 80.000, Test_accy 59.490
2023-12-19 06:10:19,354 [slca.py] => CA Task 7 => Loss 0.040, Test_accy 66.340
2023-12-19 06:10:37,807 [slca.py] => CA Task 7 => Loss 0.008, Test_accy 67.100
2023-12-19 06:10:55,756 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 67.460
2023-12-19 06:11:13,757 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 67.600
2023-12-19 06:11:31,740 [slca.py] => CA Task 7 => Loss 0.004, Test_accy 67.600
2023-12-19 06:11:48,815 [slca.py] => Exemplar size: 0
2023-12-19 06:11:49,532 [trainer.py] => No NME accuracy.
2023-12-19 06:11:49,533 [trainer.py] => CNN: {'total': 67.6, '00-09': 60.95, '10-19': 52.57, '20-29': 65.03, '30-39': 81.16, '40-49': 64.08, '50-59': 70.55, '60-69': 73.77, '70-79': 72.78, 'old': 66.86, 'new': 72.78}
2023-12-19 06:11:49,533 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43, 76.84, 72.76, 70.97, 70.01, 67.6]
2023-12-19 06:11:49,533 [trainer.py] => CNN top1 avg: 76.15375
2023-12-19 06:11:49,533 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08, 95.54, 94.23, 93.12, 91.88, 90.84]

2023-12-19 06:11:49,535 [trainer.py] => All params: 85860176
2023-12-19 06:11:49,536 [trainer.py] => Trainable params: 85860176
2023-12-19 06:11:49,538 [slca.py] => Learning on 80-90
2023-12-19 06:11:52,561 [slca.py] => Task 8, Epoch 1/20 => Loss 2.407
2023-12-19 06:11:55,500 [slca.py] => Task 8, Epoch 2/20 => Loss 1.046
2023-12-19 06:11:58,477 [slca.py] => Task 8, Epoch 3/20 => Loss 0.345
2023-12-19 06:12:01,407 [slca.py] => Task 8, Epoch 4/20 => Loss 0.063
2023-12-19 06:12:25,259 [slca.py] => Task 8, Epoch 5/20 => Loss 0.059, Train_accy 80.830, Test_accy 58.080
2023-12-19 06:12:28,205 [slca.py] => Task 8, Epoch 6/20 => Loss 0.079
2023-12-19 06:12:31,203 [slca.py] => Task 8, Epoch 7/20 => Loss 0.035
2023-12-19 06:12:34,219 [slca.py] => Task 8, Epoch 8/20 => Loss 0.068
2023-12-19 06:12:37,170 [slca.py] => Task 8, Epoch 9/20 => Loss 0.039
2023-12-19 06:13:01,706 [slca.py] => Task 8, Epoch 10/20 => Loss 0.030, Train_accy 81.110, Test_accy 58.870
2023-12-19 06:13:04,818 [slca.py] => Task 8, Epoch 11/20 => Loss 0.005
2023-12-19 06:13:07,757 [slca.py] => Task 8, Epoch 12/20 => Loss 0.055
2023-12-19 06:13:10,736 [slca.py] => Task 8, Epoch 13/20 => Loss 0.036
2023-12-19 06:13:13,711 [slca.py] => Task 8, Epoch 14/20 => Loss 0.023
2023-12-19 06:13:39,026 [slca.py] => Task 8, Epoch 15/20 => Loss 0.007, Train_accy 83.060, Test_accy 58.420
2023-12-19 06:13:41,983 [slca.py] => Task 8, Epoch 16/20 => Loss 0.103
2023-12-19 06:13:44,928 [slca.py] => Task 8, Epoch 17/20 => Loss 0.056
2023-12-19 06:13:47,860 [slca.py] => Task 8, Epoch 18/20 => Loss 0.071
2023-12-19 06:13:50,812 [slca.py] => Task 8, Epoch 19/20 => Loss 0.011
2023-12-19 06:14:14,633 [slca.py] => Task 8, Epoch 20/20 => Loss 0.084, Train_accy 81.390, Test_accy 58.870
2023-12-19 06:14:40,263 [slca.py] => CA Task 8 => Loss 0.040, Test_accy 65.520
2023-12-19 06:15:00,486 [slca.py] => CA Task 8 => Loss 0.009, Test_accy 66.420
2023-12-19 06:15:20,879 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 66.960
2023-12-19 06:15:41,082 [slca.py] => CA Task 8 => Loss 0.005, Test_accy 67.140
2023-12-19 06:16:01,336 [slca.py] => CA Task 8 => Loss 0.004, Test_accy 67.150
2023-12-19 06:16:20,725 [slca.py] => Exemplar size: 0
2023-12-19 06:16:21,428 [trainer.py] => No NME accuracy.
2023-12-19 06:16:21,428 [trainer.py] => CNN: {'total': 67.15, '00-09': 56.48, '10-19': 48.39, '20-29': 63.69, '30-39': 81.53, '40-49': 58.67, '50-59': 69.64, '60-69': 71.84, '70-79': 74.32, '80-89': 79.8, 'old': 65.57, 'new': 79.8}
2023-12-19 06:16:21,428 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43, 76.84, 72.76, 70.97, 70.01, 67.6, 67.15]
2023-12-19 06:16:21,428 [trainer.py] => CNN top1 avg: 75.15333333333334
2023-12-19 06:16:21,428 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08, 95.54, 94.23, 93.12, 91.88, 90.84, 90.73]

2023-12-19 06:16:21,429 [trainer.py] => All params: 85867866
2023-12-19 06:16:21,429 [trainer.py] => Trainable params: 85867866
2023-12-19 06:16:21,430 [slca.py] => Learning on 90-100
2023-12-19 06:16:24,640 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 06:16:27,861 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 06:16:31,039 [slca.py] => Task 9, Epoch 3/20 => Loss 0.233
2023-12-19 06:16:34,248 [slca.py] => Task 9, Epoch 4/20 => Loss 0.152
2023-12-19 06:17:00,766 [slca.py] => Task 9, Epoch 5/20 => Loss 0.101, Train_accy 81.750, Test_accy 56.820
2023-12-19 06:17:03,898 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 06:17:07,086 [slca.py] => Task 9, Epoch 7/20 => Loss 0.110
2023-12-19 06:17:10,282 [slca.py] => Task 9, Epoch 8/20 => Loss 0.091
2023-12-19 06:17:13,514 [slca.py] => Task 9, Epoch 9/20 => Loss 0.054
2023-12-19 06:17:39,982 [slca.py] => Task 9, Epoch 10/20 => Loss 0.047, Train_accy 83.000, Test_accy 58.530
2023-12-19 06:17:43,381 [slca.py] => Task 9, Epoch 11/20 => Loss 0.021
2023-12-19 06:17:46,690 [slca.py] => Task 9, Epoch 12/20 => Loss 0.040
2023-12-19 06:17:49,899 [slca.py] => Task 9, Epoch 13/20 => Loss 0.022
2023-12-19 06:17:53,165 [slca.py] => Task 9, Epoch 14/20 => Loss 0.045
2023-12-19 06:18:19,643 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 83.750, Test_accy 59.280
2023-12-19 06:18:22,879 [slca.py] => Task 9, Epoch 16/20 => Loss 0.036
2023-12-19 06:18:26,102 [slca.py] => Task 9, Epoch 17/20 => Loss 0.043
2023-12-19 06:18:29,274 [slca.py] => Task 9, Epoch 18/20 => Loss 0.002
2023-12-19 06:18:32,460 [slca.py] => Task 9, Epoch 19/20 => Loss 0.094
2023-12-19 06:18:58,734 [slca.py] => Task 9, Epoch 20/20 => Loss 0.006, Train_accy 83.000, Test_accy 58.300
2023-12-19 06:19:26,597 [slca.py] => CA Task 9 => Loss 0.041, Test_accy 64.550
2023-12-19 06:19:50,857 [slca.py] => CA Task 9 => Loss 0.009, Test_accy 65.310
2023-12-19 06:20:13,449 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 65.680
2023-12-19 06:20:37,189 [slca.py] => CA Task 9 => Loss 0.005, Test_accy 65.830
2023-12-19 06:21:01,722 [slca.py] => CA Task 9 => Loss 0.005, Test_accy 65.890
2023-12-19 06:21:23,180 [slca.py] => Exemplar size: 0
2023-12-19 06:21:23,916 [trainer.py] => No NME accuracy.
2023-12-19 06:21:23,916 [trainer.py] => CNN: {'total': 65.89, '00-09': 55.06, '10-19': 49.65, '20-29': 63.86, '30-39': 76.05, '40-49': 61.6, '50-59': 66.93, '60-69': 71.87, '70-79': 71.01, '80-89': 77.35, '90-99': 65.5, 'old': 65.93, 'new': 65.5}
2023-12-19 06:21:23,916 [trainer.py] => CNN top1 curve: [91.63, 80.99, 78.43, 76.84, 72.76, 70.97, 70.01, 67.6, 67.15, 65.89]
2023-12-19 06:21:23,916 [trainer.py] => CNN top1 avg: 74.227
2023-12-19 06:21:23,916 [trainer.py] => CNN top5 curve: [99.78, 98.28, 97.08, 95.54, 94.23, 93.12, 91.88, 90.84, 90.73, 90.68]

2023-12-19 06:21:23,917 [trainer.py] => final accs: [65.89]
2023-12-19 06:21:23,917 [trainer.py] => avg accs: [74.227]
