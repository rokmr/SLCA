2023-12-19 05:20:48,751 [trainer.py] => config: exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 05:20:48,751 [trainer.py] => test_only: False
2023-12-19 05:20:48,751 [trainer.py] => prefix: reproduce
2023-12-19 05:20:48,751 [trainer.py] => dataset: cifar100_224
2023-12-19 05:20:48,751 [trainer.py] => memory_size: 0
2023-12-19 05:20:48,751 [trainer.py] => memory_per_class: 0
2023-12-19 05:20:48,752 [trainer.py] => fixed_memory: False
2023-12-19 05:20:48,752 [trainer.py] => shuffle: False
2023-12-19 05:20:48,752 [trainer.py] => init_cls: 10
2023-12-19 05:20:48,752 [trainer.py] => increment: 10
2023-12-19 05:20:48,752 [trainer.py] => model_name: slca_cifar
2023-12-19 05:20:48,752 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 05:20:48,752 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 05:20:48,752 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]
2023-12-19 05:20:48,752 [trainer.py] => seed: 0
2023-12-19 05:20:48,752 [trainer.py] => epochs: 20
2023-12-19 05:20:48,752 [trainer.py] => ca_epochs: 5
2023-12-19 05:20:48,752 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-19 05:20:48,752 [trainer.py] => milestones: [18]
2023-12-19 05:20:48,752 [trainer.py] => lr: 0.005
2023-12-19 05:20:48,752 [trainer.py] => lr_decay: 0.1
2023-12-19 05:20:48,752 [trainer.py] => weight_decay: 0.0005
2023-12-19 05:20:48,752 [trainer.py] => u_batch_size: 256
2023-12-19 05:20:48,752 [trainer.py] => s_batch_size: 3
2023-12-19 05:20:48,753 [trainer.py] => multicrop: 2
2023-12-19 05:20:48,753 [trainer.py] => us_multicrop: 2
2023-12-19 05:20:48,753 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 05:20:48,753 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 05:20:48,753 [trainer.py] => buffer_size: 500
2023-12-19 05:20:48,753 [trainer.py] => run_id: 0
2023-12-19 05:21:14,095 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 05:21:16,655 [trainer.py] => All params: 85798656
2023-12-19 05:21:16,656 [trainer.py] => Trainable params: 85798656
2023-12-19 05:21:16,657 [slca.py] => Learning on 0-10
2023-12-19 05:21:41,126 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 05:21:42,751 [slca.py] => Task 0, Epoch 2/20 => Loss 2.383
2023-12-19 05:21:44,373 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 05:21:45,987 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 05:21:53,468 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 40.070
2023-12-19 05:21:55,130 [slca.py] => Task 0, Epoch 6/20 => Loss 1.701
2023-12-19 05:21:56,778 [slca.py] => Task 0, Epoch 7/20 => Loss 1.405
2023-12-19 05:21:58,386 [slca.py] => Task 0, Epoch 8/20 => Loss 1.354
2023-12-19 05:21:59,975 [slca.py] => Task 0, Epoch 9/20 => Loss 1.157
2023-12-19 05:22:07,382 [slca.py] => Task 0, Epoch 10/20 => Loss 0.951, Train_accy 92.500, Test_accy 69.750
2023-12-19 05:22:08,989 [slca.py] => Task 0, Epoch 11/20 => Loss 0.774
2023-12-19 05:22:10,620 [slca.py] => Task 0, Epoch 12/20 => Loss 0.792
2023-12-19 05:22:12,260 [slca.py] => Task 0, Epoch 13/20 => Loss 0.584
2023-12-19 05:22:13,891 [slca.py] => Task 0, Epoch 14/20 => Loss 0.457
2023-12-19 05:22:21,417 [slca.py] => Task 0, Epoch 15/20 => Loss 0.483, Train_accy 95.000, Test_accy 80.800
2023-12-19 05:22:23,035 [slca.py] => Task 0, Epoch 16/20 => Loss 0.336
2023-12-19 05:22:24,682 [slca.py] => Task 0, Epoch 17/20 => Loss 0.355
2023-12-19 05:22:26,288 [slca.py] => Task 0, Epoch 18/20 => Loss 0.293
2023-12-19 05:22:27,918 [slca.py] => Task 0, Epoch 19/20 => Loss 0.215
2023-12-19 05:22:35,346 [slca.py] => Task 0, Epoch 20/20 => Loss 0.295, Train_accy 97.500, Test_accy 83.590
2023-12-19 05:22:49,289 [slca.py] => CA Task 0 => Loss 0.024, Test_accy 85.270
2023-12-19 05:22:56,341 [slca.py] => CA Task 0 => Loss 0.010, Test_accy 86.500
2023-12-19 05:23:03,336 [slca.py] => CA Task 0 => Loss 0.005, Test_accy 86.610
2023-12-19 05:23:10,307 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 86.500
2023-12-19 05:23:17,486 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 86.380
2023-12-19 05:23:22,910 [slca.py] => Exemplar size: 0
2023-12-19 05:23:23,686 [trainer.py] => No NME accuracy.
2023-12-19 05:23:23,686 [trainer.py] => CNN: {'total': 86.38, '00-09': 86.38, 'old': 0, 'new': 86.38}
2023-12-19 05:23:23,686 [trainer.py] => CNN top1 curve: [86.38]
2023-12-19 05:23:23,687 [trainer.py] => CNN top1 avg: 86.38
2023-12-19 05:23:23,687 [trainer.py] => CNN top5 curve: [99.33]

2023-12-19 05:23:23,687 [trainer.py] => All params: 85806346
2023-12-19 05:23:23,688 [trainer.py] => Trainable params: 85806346
2023-12-19 05:23:23,689 [slca.py] => Learning on 10-20
2023-12-19 05:23:25,731 [slca.py] => Task 1, Epoch 1/20 => Loss 2.520
2023-12-19 05:23:27,723 [slca.py] => Task 1, Epoch 2/20 => Loss 2.484
2023-12-19 05:23:29,691 [slca.py] => Task 1, Epoch 3/20 => Loss 2.335
2023-12-19 05:23:31,726 [slca.py] => Task 1, Epoch 4/20 => Loss 2.200
2023-12-19 05:23:43,036 [slca.py] => Task 1, Epoch 5/20 => Loss 1.965, Train_accy 53.750, Test_accy 42.660
2023-12-19 05:23:45,015 [slca.py] => Task 1, Epoch 6/20 => Loss 1.633
2023-12-19 05:23:46,955 [slca.py] => Task 1, Epoch 7/20 => Loss 1.522
2023-12-19 05:23:48,947 [slca.py] => Task 1, Epoch 8/20 => Loss 1.283
2023-12-19 05:23:50,967 [slca.py] => Task 1, Epoch 9/20 => Loss 1.065
2023-12-19 05:24:02,338 [slca.py] => Task 1, Epoch 10/20 => Loss 0.805, Train_accy 82.500, Test_accy 55.890
2023-12-19 05:24:04,421 [slca.py] => Task 1, Epoch 11/20 => Loss 0.778
2023-12-19 05:24:06,408 [slca.py] => Task 1, Epoch 12/20 => Loss 0.623
2023-12-19 05:24:08,330 [slca.py] => Task 1, Epoch 13/20 => Loss 0.433
2023-12-19 05:24:10,282 [slca.py] => Task 1, Epoch 14/20 => Loss 0.471
2023-12-19 05:24:21,646 [slca.py] => Task 1, Epoch 15/20 => Loss 0.410, Train_accy 96.250, Test_accy 67.660
2023-12-19 05:24:23,685 [slca.py] => Task 1, Epoch 16/20 => Loss 0.243
2023-12-19 05:24:25,615 [slca.py] => Task 1, Epoch 17/20 => Loss 0.277
2023-12-19 05:24:27,541 [slca.py] => Task 1, Epoch 18/20 => Loss 0.248
2023-12-19 05:24:29,571 [slca.py] => Task 1, Epoch 19/20 => Loss 0.137
2023-12-19 05:24:40,992 [slca.py] => Task 1, Epoch 20/20 => Loss 0.220, Train_accy 91.250, Test_accy 71.040
2023-12-19 05:25:01,025 [slca.py] => CA Task 1 => Loss 0.031, Test_accy 73.440
2023-12-19 05:25:13,791 [slca.py] => CA Task 1 => Loss 0.008, Test_accy 74.580
2023-12-19 05:25:26,538 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 74.950
2023-12-19 05:25:39,304 [slca.py] => CA Task 1 => Loss 0.004, Test_accy 75.000
2023-12-19 05:25:52,038 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 75.000
2023-12-19 05:26:02,119 [slca.py] => Exemplar size: 0
2023-12-19 05:26:02,893 [trainer.py] => No NME accuracy.
2023-12-19 05:26:02,894 [trainer.py] => CNN: {'total': 75.0, '00-09': 82.71, '10-19': 67.29, 'old': 82.71, 'new': 67.29}
2023-12-19 05:26:02,894 [trainer.py] => CNN top1 curve: [86.38, 75.0]
2023-12-19 05:26:02,894 [trainer.py] => CNN top1 avg: 80.69
2023-12-19 05:26:02,894 [trainer.py] => CNN top5 curve: [99.33, 97.24]

2023-12-19 05:26:02,895 [trainer.py] => All params: 85814036
2023-12-19 05:26:02,896 [trainer.py] => Trainable params: 85814036
2023-12-19 05:26:02,897 [slca.py] => Learning on 20-30
2023-12-19 05:26:05,329 [slca.py] => Task 2, Epoch 1/20 => Loss 2.423
2023-12-19 05:26:07,587 [slca.py] => Task 2, Epoch 2/20 => Loss 2.272
2023-12-19 05:26:09,871 [slca.py] => Task 2, Epoch 3/20 => Loss 1.995
2023-12-19 05:26:12,214 [slca.py] => Task 2, Epoch 4/20 => Loss 1.926
2023-12-19 05:26:27,340 [slca.py] => Task 2, Epoch 5/20 => Loss 1.656, Train_accy 61.670, Test_accy 47.760
2023-12-19 05:26:29,663 [slca.py] => Task 2, Epoch 6/20 => Loss 1.490
2023-12-19 05:26:31,995 [slca.py] => Task 2, Epoch 7/20 => Loss 1.150
2023-12-19 05:26:34,301 [slca.py] => Task 2, Epoch 8/20 => Loss 0.952
2023-12-19 05:26:36,611 [slca.py] => Task 2, Epoch 9/20 => Loss 0.761
2023-12-19 05:26:51,819 [slca.py] => Task 2, Epoch 10/20 => Loss 0.747, Train_accy 84.170, Test_accy 59.000
2023-12-19 05:26:54,198 [slca.py] => Task 2, Epoch 11/20 => Loss 0.483
2023-12-19 05:26:56,451 [slca.py] => Task 2, Epoch 12/20 => Loss 0.299
2023-12-19 05:26:58,715 [slca.py] => Task 2, Epoch 13/20 => Loss 0.244
2023-12-19 05:27:00,995 [slca.py] => Task 2, Epoch 14/20 => Loss 0.317
2023-12-19 05:27:16,171 [slca.py] => Task 2, Epoch 15/20 => Loss 0.320, Train_accy 88.330, Test_accy 67.430
2023-12-19 05:27:18,526 [slca.py] => Task 2, Epoch 16/20 => Loss 0.205
2023-12-19 05:27:20,809 [slca.py] => Task 2, Epoch 17/20 => Loss 0.353
2023-12-19 05:27:23,067 [slca.py] => Task 2, Epoch 18/20 => Loss 0.223
2023-12-19 05:27:25,349 [slca.py] => Task 2, Epoch 19/20 => Loss 0.178
2023-12-19 05:27:40,611 [slca.py] => Task 2, Epoch 20/20 => Loss 0.140, Train_accy 92.500, Test_accy 68.610
2023-12-19 05:28:06,710 [slca.py] => CA Task 2 => Loss 0.038, Test_accy 70.210
2023-12-19 05:28:25,217 [slca.py] => CA Task 2 => Loss 0.009, Test_accy 70.990
2023-12-19 05:28:43,962 [slca.py] => CA Task 2 => Loss 0.005, Test_accy 71.710
2023-12-19 05:29:02,829 [slca.py] => CA Task 2 => Loss 0.004, Test_accy 71.770
2023-12-19 05:29:21,468 [slca.py] => CA Task 2 => Loss 0.003, Test_accy 71.940
2023-12-19 05:29:36,094 [slca.py] => Exemplar size: 0
2023-12-19 05:29:36,873 [trainer.py] => No NME accuracy.
2023-12-19 05:29:36,873 [trainer.py] => CNN: {'total': 71.94, '00-09': 77.4, '10-19': 61.04, '20-29': 77.42, 'old': 69.2, 'new': 77.42}
2023-12-19 05:29:36,874 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94]
2023-12-19 05:29:36,874 [trainer.py] => CNN top1 avg: 77.77333333333333
2023-12-19 05:29:36,874 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38]

2023-12-19 05:29:36,876 [trainer.py] => All params: 85821726
2023-12-19 05:29:36,878 [trainer.py] => Trainable params: 85821726
2023-12-19 05:29:36,881 [slca.py] => Learning on 30-40
2023-12-19 05:29:39,767 [slca.py] => Task 3, Epoch 1/20 => Loss 2.325
2023-12-19 05:29:42,542 [slca.py] => Task 3, Epoch 2/20 => Loss 2.049
2023-12-19 05:29:45,336 [slca.py] => Task 3, Epoch 3/20 => Loss 1.714
2023-12-19 05:29:48,075 [slca.py] => Task 3, Epoch 4/20 => Loss 1.409
2023-12-19 05:30:07,387 [slca.py] => Task 3, Epoch 5/20 => Loss 0.884, Train_accy 78.750, Test_accy 58.870
2023-12-19 05:30:10,086 [slca.py] => Task 3, Epoch 6/20 => Loss 0.743
2023-12-19 05:30:12,915 [slca.py] => Task 3, Epoch 7/20 => Loss 0.450
2023-12-19 05:30:15,703 [slca.py] => Task 3, Epoch 8/20 => Loss 0.199
2023-12-19 05:30:18,499 [slca.py] => Task 3, Epoch 9/20 => Loss 0.373
2023-12-19 05:30:38,000 [slca.py] => Task 3, Epoch 10/20 => Loss 0.177, Train_accy 80.620, Test_accy 68.070
2023-12-19 05:30:40,678 [slca.py] => Task 3, Epoch 11/20 => Loss 0.267
2023-12-19 05:30:43,392 [slca.py] => Task 3, Epoch 12/20 => Loss 0.117
2023-12-19 05:30:46,195 [slca.py] => Task 3, Epoch 13/20 => Loss 0.071
2023-12-19 05:30:48,977 [slca.py] => Task 3, Epoch 14/20 => Loss 0.058
2023-12-19 05:31:08,330 [slca.py] => Task 3, Epoch 15/20 => Loss 0.132, Train_accy 86.880, Test_accy 68.470
2023-12-19 05:31:11,228 [slca.py] => Task 3, Epoch 16/20 => Loss 0.080
2023-12-19 05:31:14,037 [slca.py] => Task 3, Epoch 17/20 => Loss 0.104
2023-12-19 05:31:16,836 [slca.py] => Task 3, Epoch 18/20 => Loss 0.118
2023-12-19 05:31:19,546 [slca.py] => Task 3, Epoch 19/20 => Loss 0.090
2023-12-19 05:31:38,938 [slca.py] => Task 3, Epoch 20/20 => Loss 0.027, Train_accy 85.620, Test_accy 67.820
2023-12-19 05:32:12,077 [slca.py] => CA Task 3 => Loss 0.057, Test_accy 73.240
2023-12-19 05:32:37,031 [slca.py] => CA Task 3 => Loss 0.009, Test_accy 74.270
2023-12-19 05:33:01,380 [slca.py] => CA Task 3 => Loss 0.006, Test_accy 74.750
2023-12-19 05:33:25,853 [slca.py] => CA Task 3 => Loss 0.004, Test_accy 74.820
2023-12-19 05:33:50,289 [slca.py] => CA Task 3 => Loss 0.005, Test_accy 74.870
2023-12-19 05:34:09,482 [slca.py] => Exemplar size: 0
2023-12-19 05:34:10,225 [trainer.py] => No NME accuracy.
2023-12-19 05:34:10,225 [trainer.py] => CNN: {'total': 74.87, '00-09': 73.84, '10-19': 61.79, '20-29': 77.92, '30-39': 85.92, 'old': 71.18, 'new': 85.92}
2023-12-19 05:34:10,226 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94, 74.87]
2023-12-19 05:34:10,226 [trainer.py] => CNN top1 avg: 77.0475
2023-12-19 05:34:10,226 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38, 94.81]

2023-12-19 05:34:10,227 [trainer.py] => All params: 85829416
2023-12-19 05:34:10,228 [trainer.py] => Trainable params: 85829416
2023-12-19 05:34:10,229 [slca.py] => Learning on 40-50
2023-12-19 05:34:13,408 [slca.py] => Task 4, Epoch 1/20 => Loss 2.327
2023-12-19 05:34:16,439 [slca.py] => Task 4, Epoch 2/20 => Loss 1.969
2023-12-19 05:34:19,345 [slca.py] => Task 4, Epoch 3/20 => Loss 1.466
2023-12-19 05:34:22,248 [slca.py] => Task 4, Epoch 4/20 => Loss 1.002
2023-12-19 05:34:44,644 [slca.py] => Task 4, Epoch 5/20 => Loss 0.529, Train_accy 78.000, Test_accy 58.950
2023-12-19 05:34:47,516 [slca.py] => Task 4, Epoch 6/20 => Loss 0.394
2023-12-19 05:34:50,480 [slca.py] => Task 4, Epoch 7/20 => Loss 0.200
2023-12-19 05:34:53,404 [slca.py] => Task 4, Epoch 8/20 => Loss 0.374
2023-12-19 05:34:56,305 [slca.py] => Task 4, Epoch 9/20 => Loss 0.160
2023-12-19 05:35:18,857 [slca.py] => Task 4, Epoch 10/20 => Loss 0.179, Train_accy 84.000, Test_accy 65.360
2023-12-19 05:35:21,777 [slca.py] => Task 4, Epoch 11/20 => Loss 0.147
2023-12-19 05:35:24,626 [slca.py] => Task 4, Epoch 12/20 => Loss 0.074
2023-12-19 05:35:27,538 [slca.py] => Task 4, Epoch 13/20 => Loss 0.106
2023-12-19 05:35:30,458 [slca.py] => Task 4, Epoch 14/20 => Loss 0.023
2023-12-19 05:35:52,931 [slca.py] => Task 4, Epoch 15/20 => Loss 0.090, Train_accy 85.000, Test_accy 64.800
2023-12-19 05:35:55,874 [slca.py] => Task 4, Epoch 16/20 => Loss 0.161
2023-12-19 05:35:58,809 [slca.py] => Task 4, Epoch 17/20 => Loss 0.058
2023-12-19 05:36:01,704 [slca.py] => Task 4, Epoch 18/20 => Loss 0.092
2023-12-19 05:36:04,635 [slca.py] => Task 4, Epoch 19/20 => Loss 0.035
2023-12-19 05:36:27,501 [slca.py] => Task 4, Epoch 20/20 => Loss 0.130, Train_accy 84.500, Test_accy 63.800
2023-12-19 05:37:04,717 [slca.py] => CA Task 4 => Loss 0.052, Test_accy 70.770
2023-12-19 05:37:35,043 [slca.py] => CA Task 4 => Loss 0.009, Test_accy 71.630
2023-12-19 05:38:04,905 [slca.py] => CA Task 4 => Loss 0.006, Test_accy 72.020
2023-12-19 05:38:34,956 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 72.180
2023-12-19 05:39:05,172 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 72.180
2023-12-19 05:39:29,298 [slca.py] => Exemplar size: 0
2023-12-19 05:39:30,107 [trainer.py] => No NME accuracy.
2023-12-19 05:39:30,107 [trainer.py] => CNN: {'total': 72.18, '00-09': 69.64, '10-19': 59.06, '20-29': 71.64, '30-39': 82.65, '40-49': 77.9, 'old': 70.74, 'new': 77.9}
2023-12-19 05:39:30,107 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94, 74.87, 72.18]
2023-12-19 05:39:30,108 [trainer.py] => CNN top1 avg: 76.074
2023-12-19 05:39:30,108 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38, 94.81, 93.97]

2023-12-19 05:39:30,109 [trainer.py] => All params: 85837106
2023-12-19 05:39:30,109 [trainer.py] => Trainable params: 85837106
2023-12-19 05:39:30,111 [slca.py] => Learning on 50-60
2023-12-19 05:39:33,395 [slca.py] => Task 5, Epoch 1/20 => Loss 2.416
2023-12-19 05:39:36,454 [slca.py] => Task 5, Epoch 2/20 => Loss 2.047
2023-12-19 05:39:39,540 [slca.py] => Task 5, Epoch 3/20 => Loss 1.457
2023-12-19 05:39:42,492 [slca.py] => Task 5, Epoch 4/20 => Loss 1.041
2023-12-19 05:40:08,119 [slca.py] => Task 5, Epoch 5/20 => Loss 0.538, Train_accy 74.170, Test_accy 57.000
2023-12-19 05:40:11,116 [slca.py] => Task 5, Epoch 6/20 => Loss 0.347
2023-12-19 05:40:14,033 [slca.py] => Task 5, Epoch 7/20 => Loss 0.261
2023-12-19 05:40:17,016 [slca.py] => Task 5, Epoch 8/20 => Loss 0.125
2023-12-19 05:40:19,995 [slca.py] => Task 5, Epoch 9/20 => Loss 0.306
2023-12-19 05:40:45,213 [slca.py] => Task 5, Epoch 10/20 => Loss 0.117, Train_accy 85.420, Test_accy 61.630
2023-12-19 05:40:48,143 [slca.py] => Task 5, Epoch 11/20 => Loss 0.163
2023-12-19 05:40:51,211 [slca.py] => Task 5, Epoch 12/20 => Loss 0.066
2023-12-19 05:40:54,094 [slca.py] => Task 5, Epoch 13/20 => Loss 0.041
2023-12-19 05:40:57,139 [slca.py] => Task 5, Epoch 14/20 => Loss 0.185
2023-12-19 05:41:22,678 [slca.py] => Task 5, Epoch 15/20 => Loss 0.032, Train_accy 84.170, Test_accy 61.670
2023-12-19 05:41:25,858 [slca.py] => Task 5, Epoch 16/20 => Loss 0.072
2023-12-19 05:41:28,871 [slca.py] => Task 5, Epoch 17/20 => Loss 0.015
2023-12-19 05:41:31,829 [slca.py] => Task 5, Epoch 18/20 => Loss 0.044
2023-12-19 05:41:34,817 [slca.py] => Task 5, Epoch 19/20 => Loss 0.079
2023-12-19 05:42:00,176 [slca.py] => Task 5, Epoch 20/20 => Loss 0.025, Train_accy 81.670, Test_accy 61.430
2023-12-19 05:42:42,992 [slca.py] => CA Task 5 => Loss 0.055, Test_accy 68.800
2023-12-19 05:43:18,797 [slca.py] => CA Task 5 => Loss 0.011, Test_accy 69.040
2023-12-19 05:43:54,107 [slca.py] => CA Task 5 => Loss 0.007, Test_accy 69.380
2023-12-19 05:44:29,561 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 69.500
2023-12-19 05:45:05,018 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 69.510
2023-12-19 05:45:32,905 [slca.py] => Exemplar size: 0
2023-12-19 05:45:33,711 [trainer.py] => No NME accuracy.
2023-12-19 05:45:33,711 [trainer.py] => CNN: {'total': 69.51, '00-09': 64.63, '10-19': 56.39, '20-29': 66.23, '30-39': 81.68, '40-49': 72.04, '50-59': 76.31, 'old': 68.17, 'new': 76.31}
2023-12-19 05:45:33,711 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94, 74.87, 72.18, 69.51]
2023-12-19 05:45:33,712 [trainer.py] => CNN top1 avg: 74.98
2023-12-19 05:45:33,712 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38, 94.81, 93.97, 92.92]

2023-12-19 05:45:33,713 [trainer.py] => All params: 85844796
2023-12-19 05:45:33,713 [trainer.py] => Trainable params: 85844796
2023-12-19 05:45:33,714 [slca.py] => Learning on 60-70
2023-12-19 05:45:37,384 [slca.py] => Task 6, Epoch 1/20 => Loss 2.296
2023-12-19 05:45:40,881 [slca.py] => Task 6, Epoch 2/20 => Loss 1.340
2023-12-19 05:45:44,617 [slca.py] => Task 6, Epoch 3/20 => Loss 0.898
2023-12-19 05:45:48,137 [slca.py] => Task 6, Epoch 4/20 => Loss 0.776
2023-12-19 05:46:17,545 [slca.py] => Task 6, Epoch 5/20 => Loss 0.300, Train_accy 81.430, Test_accy 58.960
2023-12-19 05:46:21,033 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 05:46:24,790 [slca.py] => Task 6, Epoch 7/20 => Loss 0.132
2023-12-19 05:46:28,234 [slca.py] => Task 6, Epoch 8/20 => Loss 0.074
2023-12-19 05:46:31,871 [slca.py] => Task 6, Epoch 9/20 => Loss 0.103
2023-12-19 05:47:02,058 [slca.py] => Task 6, Epoch 10/20 => Loss 0.047, Train_accy 82.500, Test_accy 60.100
2023-12-19 05:47:05,724 [slca.py] => Task 6, Epoch 11/20 => Loss 0.053
2023-12-19 05:47:09,452 [slca.py] => Task 6, Epoch 12/20 => Loss 0.101
2023-12-19 05:47:13,193 [slca.py] => Task 6, Epoch 13/20 => Loss 0.042
2023-12-19 05:47:16,889 [slca.py] => Task 6, Epoch 14/20 => Loss 0.099
2023-12-19 05:47:47,443 [slca.py] => Task 6, Epoch 15/20 => Loss 0.046, Train_accy 80.000, Test_accy 60.560
2023-12-19 05:47:50,969 [slca.py] => Task 6, Epoch 16/20 => Loss 0.146
2023-12-19 05:47:54,449 [slca.py] => Task 6, Epoch 17/20 => Loss 0.100
2023-12-19 05:47:57,957 [slca.py] => Task 6, Epoch 18/20 => Loss 0.146
2023-12-19 05:48:01,426 [slca.py] => Task 6, Epoch 19/20 => Loss 0.018
2023-12-19 05:48:31,460 [slca.py] => Task 6, Epoch 20/20 => Loss 0.036, Train_accy 80.000, Test_accy 60.520
2023-12-19 05:49:21,193 [slca.py] => CA Task 6 => Loss 0.057, Test_accy 68.320
2023-12-19 05:50:02,755 [slca.py] => CA Task 6 => Loss 0.012, Test_accy 69.010
2023-12-19 05:50:44,358 [slca.py] => CA Task 6 => Loss 0.007, Test_accy 69.340
2023-12-19 05:51:25,990 [slca.py] => CA Task 6 => Loss 0.006, Test_accy 69.420
2023-12-19 05:52:07,434 [slca.py] => CA Task 6 => Loss 0.006, Test_accy 69.460
2023-12-19 05:52:39,881 [slca.py] => Exemplar size: 0
2023-12-19 05:52:40,599 [trainer.py] => No NME accuracy.
2023-12-19 05:52:40,599 [trainer.py] => CNN: {'total': 69.46, '00-09': 61.88, '10-19': 52.48, '20-29': 66.63, '30-39': 81.18, '40-49': 70.17, '50-59': 75.73, '60-69': 78.17, 'old': 68.01, 'new': 78.17}
2023-12-19 05:52:40,599 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94, 74.87, 72.18, 69.51, 69.46]
2023-12-19 05:52:40,600 [trainer.py] => CNN top1 avg: 74.19142857142857
2023-12-19 05:52:40,600 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38, 94.81, 93.97, 92.92, 91.65]

2023-12-19 05:52:40,601 [trainer.py] => All params: 85852486
2023-12-19 05:52:40,601 [trainer.py] => Trainable params: 85852486
2023-12-19 05:52:40,603 [slca.py] => Learning on 70-80
2023-12-19 05:52:44,583 [slca.py] => Task 7, Epoch 1/20 => Loss 2.488
2023-12-19 05:52:48,151 [slca.py] => Task 7, Epoch 2/20 => Loss 1.760
2023-12-19 05:52:51,764 [slca.py] => Task 7, Epoch 3/20 => Loss 0.996
2023-12-19 05:52:55,350 [slca.py] => Task 7, Epoch 4/20 => Loss 0.460
2023-12-19 05:53:28,125 [slca.py] => Task 7, Epoch 5/20 => Loss 0.272, Train_accy 75.940, Test_accy 57.860
2023-12-19 05:53:31,754 [slca.py] => Task 7, Epoch 6/20 => Loss 0.192
2023-12-19 05:53:35,313 [slca.py] => Task 7, Epoch 7/20 => Loss 0.207
2023-12-19 05:53:38,934 [slca.py] => Task 7, Epoch 8/20 => Loss 0.199
2023-12-19 05:53:42,810 [slca.py] => Task 7, Epoch 9/20 => Loss 0.096
2023-12-19 05:54:16,183 [slca.py] => Task 7, Epoch 10/20 => Loss 0.205, Train_accy 84.060, Test_accy 60.240
2023-12-19 05:54:19,766 [slca.py] => Task 7, Epoch 11/20 => Loss 0.177
2023-12-19 05:54:23,401 [slca.py] => Task 7, Epoch 12/20 => Loss 0.097
2023-12-19 05:54:27,300 [slca.py] => Task 7, Epoch 13/20 => Loss 0.137
2023-12-19 05:54:30,883 [slca.py] => Task 7, Epoch 14/20 => Loss 0.126
2023-12-19 05:55:04,059 [slca.py] => Task 7, Epoch 15/20 => Loss 0.019, Train_accy 80.620, Test_accy 59.700
2023-12-19 05:55:07,725 [slca.py] => Task 7, Epoch 16/20 => Loss 0.109
2023-12-19 05:55:11,394 [slca.py] => Task 7, Epoch 17/20 => Loss 0.025
2023-12-19 05:55:15,190 [slca.py] => Task 7, Epoch 18/20 => Loss 0.044
2023-12-19 05:55:18,766 [slca.py] => Task 7, Epoch 19/20 => Loss 0.024
2023-12-19 05:55:51,804 [slca.py] => Task 7, Epoch 20/20 => Loss 0.043, Train_accy 77.810, Test_accy 59.360
2023-12-19 05:56:46,459 [slca.py] => CA Task 7 => Loss 0.054, Test_accy 66.850
2023-12-19 05:57:33,702 [slca.py] => CA Task 7 => Loss 0.012, Test_accy 67.790
2023-12-19 05:58:21,229 [slca.py] => CA Task 7 => Loss 0.008, Test_accy 68.170
2023-12-19 05:59:08,600 [slca.py] => CA Task 7 => Loss 0.007, Test_accy 68.260
2023-12-19 05:59:56,058 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 68.310
2023-12-19 06:00:33,733 [slca.py] => Exemplar size: 0
2023-12-19 06:00:34,573 [trainer.py] => No NME accuracy.
2023-12-19 06:00:34,574 [trainer.py] => CNN: {'total': 68.31, '00-09': 57.21, '10-19': 49.95, '20-29': 61.72, '30-39': 79.94, '40-49': 70.93, '50-59': 73.48, '60-69': 74.97, '70-79': 78.33, 'old': 66.88, 'new': 78.33}
2023-12-19 06:00:34,574 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94, 74.87, 72.18, 69.51, 69.46, 68.31]
2023-12-19 06:00:34,574 [trainer.py] => CNN top1 avg: 73.45625
2023-12-19 06:00:34,574 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38, 94.81, 93.97, 92.92, 91.65, 91.09]

2023-12-19 06:00:34,575 [trainer.py] => All params: 85860176
2023-12-19 06:00:34,576 [trainer.py] => Trainable params: 85860176
2023-12-19 06:00:34,577 [slca.py] => Learning on 80-90
2023-12-19 06:00:38,705 [slca.py] => Task 8, Epoch 1/20 => Loss 2.447
2023-12-19 06:00:42,445 [slca.py] => Task 8, Epoch 2/20 => Loss 1.588
2023-12-19 06:00:46,182 [slca.py] => Task 8, Epoch 3/20 => Loss 0.838
2023-12-19 06:00:50,130 [slca.py] => Task 8, Epoch 4/20 => Loss 0.276
2023-12-19 06:01:27,068 [slca.py] => Task 8, Epoch 5/20 => Loss 0.171, Train_accy 79.170, Test_accy 57.760
2023-12-19 06:01:30,850 [slca.py] => Task 8, Epoch 6/20 => Loss 0.210
2023-12-19 06:01:34,840 [slca.py] => Task 8, Epoch 7/20 => Loss 0.117
2023-12-19 06:01:38,526 [slca.py] => Task 8, Epoch 8/20 => Loss 0.063
2023-12-19 06:01:42,408 [slca.py] => Task 8, Epoch 9/20 => Loss 0.032
2023-12-19 06:02:18,717 [slca.py] => Task 8, Epoch 10/20 => Loss 0.095, Train_accy 79.720, Test_accy 60.410
2023-12-19 06:02:22,452 [slca.py] => Task 8, Epoch 11/20 => Loss 0.014
2023-12-19 06:02:26,154 [slca.py] => Task 8, Epoch 12/20 => Loss 0.057
2023-12-19 06:02:29,856 [slca.py] => Task 8, Epoch 13/20 => Loss 0.024
2023-12-19 06:02:33,674 [slca.py] => Task 8, Epoch 14/20 => Loss 0.150
2023-12-19 06:03:10,049 [slca.py] => Task 8, Epoch 15/20 => Loss 0.019, Train_accy 83.330, Test_accy 60.290
2023-12-19 06:03:13,793 [slca.py] => Task 8, Epoch 16/20 => Loss 0.033
2023-12-19 06:03:17,573 [slca.py] => Task 8, Epoch 17/20 => Loss 0.044
2023-12-19 06:03:21,271 [slca.py] => Task 8, Epoch 18/20 => Loss 0.081
2023-12-19 06:03:24,974 [slca.py] => Task 8, Epoch 19/20 => Loss 0.016
2023-12-19 06:04:01,986 [slca.py] => Task 8, Epoch 20/20 => Loss 0.095, Train_accy 79.440, Test_accy 60.060
2023-12-19 06:05:03,009 [slca.py] => CA Task 8 => Loss 0.053, Test_accy 66.550
2023-12-19 06:05:56,099 [slca.py] => CA Task 8 => Loss 0.012, Test_accy 67.560
2023-12-19 06:06:49,124 [slca.py] => CA Task 8 => Loss 0.007, Test_accy 67.870
2023-12-19 06:07:41,627 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 68.060
2023-12-19 06:08:34,402 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 68.150
2023-12-19 06:09:16,839 [slca.py] => Exemplar size: 0
2023-12-19 06:09:17,578 [trainer.py] => No NME accuracy.
2023-12-19 06:09:17,579 [trainer.py] => CNN: {'total': 68.15, '00-09': 55.48, '10-19': 47.69, '20-29': 59.98, '30-39': 80.32, '40-49': 67.14, '50-59': 72.55, '60-69': 73.25, '70-79': 76.23, '80-89': 80.7, 'old': 66.58, 'new': 80.7}
2023-12-19 06:09:17,579 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94, 74.87, 72.18, 69.51, 69.46, 68.31, 68.15]
2023-12-19 06:09:17,579 [trainer.py] => CNN top1 avg: 72.86666666666666
2023-12-19 06:09:17,579 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38, 94.81, 93.97, 92.92, 91.65, 91.09, 91.05]

2023-12-19 06:09:17,580 [trainer.py] => All params: 85867866
2023-12-19 06:09:17,581 [trainer.py] => Trainable params: 85867866
2023-12-19 06:09:17,582 [slca.py] => Learning on 90-100
2023-12-19 06:09:22,266 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 06:09:26,619 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 06:09:30,839 [slca.py] => Task 9, Epoch 3/20 => Loss 0.664
2023-12-19 06:09:34,925 [slca.py] => Task 9, Epoch 4/20 => Loss 0.255
2023-12-19 06:10:15,884 [slca.py] => Task 9, Epoch 5/20 => Loss 0.103, Train_accy 80.500, Test_accy 56.750
2023-12-19 06:10:20,059 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 06:10:24,162 [slca.py] => Task 9, Epoch 7/20 => Loss 0.113
2023-12-19 06:10:28,630 [slca.py] => Task 9, Epoch 8/20 => Loss 0.112
2023-12-19 06:10:32,768 [slca.py] => Task 9, Epoch 9/20 => Loss 0.094
2023-12-19 06:11:13,216 [slca.py] => Task 9, Epoch 10/20 => Loss 0.028, Train_accy 81.250, Test_accy 58.040
2023-12-19 06:11:17,528 [slca.py] => Task 9, Epoch 11/20 => Loss 0.021
2023-12-19 06:11:21,689 [slca.py] => Task 9, Epoch 12/20 => Loss 0.032
2023-12-19 06:11:26,001 [slca.py] => Task 9, Epoch 13/20 => Loss 0.027
2023-12-19 06:11:30,164 [slca.py] => Task 9, Epoch 14/20 => Loss 0.053
2023-12-19 06:12:10,362 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 81.000, Test_accy 57.860
2023-12-19 06:12:14,615 [slca.py] => Task 9, Epoch 16/20 => Loss 0.016
2023-12-19 06:12:18,782 [slca.py] => Task 9, Epoch 17/20 => Loss 0.089
2023-12-19 06:12:22,969 [slca.py] => Task 9, Epoch 18/20 => Loss 0.009
2023-12-19 06:12:27,355 [slca.py] => Task 9, Epoch 19/20 => Loss 0.048
2023-12-19 06:13:07,605 [slca.py] => Task 9, Epoch 20/20 => Loss 0.103, Train_accy 80.750, Test_accy 58.640
2023-12-19 06:14:13,894 [slca.py] => CA Task 9 => Loss 0.051, Test_accy 64.840
2023-12-19 06:15:12,676 [slca.py] => CA Task 9 => Loss 0.013, Test_accy 65.780
2023-12-19 06:16:11,222 [slca.py] => CA Task 9 => Loss 0.009, Test_accy 66.340
2023-12-19 06:17:09,774 [slca.py] => CA Task 9 => Loss 0.007, Test_accy 66.480
2023-12-19 06:18:08,598 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 66.510
2023-12-19 06:18:53,916 [slca.py] => Exemplar size: 0
2023-12-19 06:18:54,640 [trainer.py] => No NME accuracy.
2023-12-19 06:18:54,641 [trainer.py] => CNN: {'total': 66.51, '00-09': 51.45, '10-19': 48.45, '20-29': 59.36, '30-39': 76.35, '40-49': 65.0, '50-59': 72.34, '60-69': 73.17, '70-79': 74.32, '80-89': 78.86, '90-99': 65.8, 'old': 66.59, 'new': 65.8}
2023-12-19 06:18:54,641 [trainer.py] => CNN top1 curve: [86.38, 75.0, 71.94, 74.87, 72.18, 69.51, 69.46, 68.31, 68.15, 66.51]
2023-12-19 06:18:54,641 [trainer.py] => CNN top1 avg: 72.231
2023-12-19 06:18:54,641 [trainer.py] => CNN top5 curve: [99.33, 97.24, 95.38, 94.81, 93.97, 92.92, 91.65, 91.09, 91.05, 90.64]

2023-12-19 06:18:54,662 [trainer.py] => final accs: [66.51]
2023-12-19 06:18:54,662 [trainer.py] => avg accs: [72.231]
