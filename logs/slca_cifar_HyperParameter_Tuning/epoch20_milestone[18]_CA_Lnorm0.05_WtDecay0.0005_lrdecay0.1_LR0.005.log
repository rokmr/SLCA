2023-12-19 07:11:50,324 [trainer.py] => config: exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 07:11:50,324 [trainer.py] => test_only: False
2023-12-19 07:11:50,324 [trainer.py] => prefix: reproduce
2023-12-19 07:11:50,324 [trainer.py] => dataset: cifar100_224
2023-12-19 07:11:50,324 [trainer.py] => memory_size: 0
2023-12-19 07:11:50,324 [trainer.py] => memory_per_class: 0
2023-12-19 07:11:50,324 [trainer.py] => fixed_memory: False
2023-12-19 07:11:50,324 [trainer.py] => shuffle: False
2023-12-19 07:11:50,324 [trainer.py] => init_cls: 10
2023-12-19 07:11:50,324 [trainer.py] => increment: 10
2023-12-19 07:11:50,324 [trainer.py] => model_name: slca_cifar
2023-12-19 07:11:50,324 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 07:11:50,324 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 07:11:50,324 [trainer.py] => device: [device(type='cuda', index=0)]
2023-12-19 07:11:50,324 [trainer.py] => seed: 0
2023-12-19 07:11:50,324 [trainer.py] => epochs: 20
2023-12-19 07:11:50,324 [trainer.py] => ca_epochs: 5
2023-12-19 07:11:50,324 [trainer.py] => ca_with_logit_norm: 0.05
2023-12-19 07:11:50,324 [trainer.py] => milestones: [18]
2023-12-19 07:11:50,324 [trainer.py] => lr: 0.005
2023-12-19 07:11:50,324 [trainer.py] => lr_decay: 0.1
2023-12-19 07:11:50,324 [trainer.py] => weight_decay: 0.0005
2023-12-19 07:11:50,324 [trainer.py] => u_batch_size: 256
2023-12-19 07:11:50,324 [trainer.py] => s_batch_size: 3
2023-12-19 07:11:50,324 [trainer.py] => multicrop: 2
2023-12-19 07:11:50,324 [trainer.py] => us_multicrop: 2
2023-12-19 07:11:50,324 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 07:11:50,324 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 07:11:50,324 [trainer.py] => buffer_size: 500
2023-12-19 07:11:50,324 [trainer.py] => run_id: 0
2023-12-19 07:11:51,929 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 07:11:53,820 [trainer.py] => All params: 85798656
2023-12-19 07:11:53,820 [trainer.py] => Trainable params: 85798656
2023-12-19 07:11:53,821 [slca.py] => Learning on 0-10
2023-12-19 07:12:00,472 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 07:12:01,640 [slca.py] => Task 0, Epoch 2/20 => Loss 2.382
2023-12-19 07:12:02,793 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 07:12:03,944 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 07:12:09,129 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 39.730
2023-12-19 07:12:10,187 [slca.py] => Task 0, Epoch 6/20 => Loss 1.699
2023-12-19 07:12:11,360 [slca.py] => Task 0, Epoch 7/20 => Loss 1.406
2023-12-19 07:12:12,545 [slca.py] => Task 0, Epoch 8/20 => Loss 1.355
2023-12-19 07:12:13,698 [slca.py] => Task 0, Epoch 9/20 => Loss 1.157
2023-12-19 07:12:18,833 [slca.py] => Task 0, Epoch 10/20 => Loss 0.953, Train_accy 92.500, Test_accy 69.750
2023-12-19 07:12:20,049 [slca.py] => Task 0, Epoch 11/20 => Loss 0.777
2023-12-19 07:12:21,212 [slca.py] => Task 0, Epoch 12/20 => Loss 0.796
2023-12-19 07:12:22,323 [slca.py] => Task 0, Epoch 13/20 => Loss 0.581
2023-12-19 07:12:23,470 [slca.py] => Task 0, Epoch 14/20 => Loss 0.462
2023-12-19 07:12:28,595 [slca.py] => Task 0, Epoch 15/20 => Loss 0.487, Train_accy 95.000, Test_accy 80.690
2023-12-19 07:12:29,777 [slca.py] => Task 0, Epoch 16/20 => Loss 0.337
2023-12-19 07:12:30,940 [slca.py] => Task 0, Epoch 17/20 => Loss 0.359
2023-12-19 07:12:32,024 [slca.py] => Task 0, Epoch 18/20 => Loss 0.296
2023-12-19 07:12:33,215 [slca.py] => Task 0, Epoch 19/20 => Loss 0.216
2023-12-19 07:12:38,350 [slca.py] => Task 0, Epoch 20/20 => Loss 0.299, Train_accy 100.000, Test_accy 83.710
2023-12-19 07:12:46,678 [slca.py] => CA Task 0 => Loss 0.012, Test_accy 84.600
2023-12-19 07:12:49,690 [slca.py] => CA Task 0 => Loss 0.002, Test_accy 85.380
2023-12-19 07:12:52,695 [slca.py] => CA Task 0 => Loss 0.001, Test_accy 85.490
2023-12-19 07:12:55,677 [slca.py] => CA Task 0 => Loss 0.001, Test_accy 85.490
2023-12-19 07:12:58,687 [slca.py] => CA Task 0 => Loss 0.001, Test_accy 85.490
2023-12-19 07:13:01,744 [slca.py] => Exemplar size: 0
2023-12-19 07:13:02,519 [trainer.py] => No NME accuracy.
2023-12-19 07:13:02,519 [trainer.py] => CNN: {'total': 85.49, '00-09': 85.49, 'old': 0, 'new': 85.49}
2023-12-19 07:13:02,519 [trainer.py] => CNN top1 curve: [85.49]
2023-12-19 07:13:02,519 [trainer.py] => CNN top1 avg: 85.49
2023-12-19 07:13:02,519 [trainer.py] => CNN top5 curve: [99.44]

2023-12-19 07:13:02,520 [trainer.py] => All params: 85806346
2023-12-19 07:13:02,520 [trainer.py] => Trainable params: 85806346
2023-12-19 07:13:02,521 [slca.py] => Learning on 10-20
2023-12-19 07:13:04,121 [slca.py] => Task 1, Epoch 1/20 => Loss 2.526
2023-12-19 07:13:05,599 [slca.py] => Task 1, Epoch 2/20 => Loss 2.488
2023-12-19 07:13:07,065 [slca.py] => Task 1, Epoch 3/20 => Loss 2.333
2023-12-19 07:13:08,569 [slca.py] => Task 1, Epoch 4/20 => Loss 2.198
2023-12-19 07:13:16,238 [slca.py] => Task 1, Epoch 5/20 => Loss 1.963, Train_accy 53.750, Test_accy 42.710
2023-12-19 07:13:17,781 [slca.py] => Task 1, Epoch 6/20 => Loss 1.638
2023-12-19 07:13:19,330 [slca.py] => Task 1, Epoch 7/20 => Loss 1.524
2023-12-19 07:13:20,770 [slca.py] => Task 1, Epoch 8/20 => Loss 1.288
2023-12-19 07:13:22,362 [slca.py] => Task 1, Epoch 9/20 => Loss 1.082
2023-12-19 07:13:30,175 [slca.py] => Task 1, Epoch 10/20 => Loss 0.816, Train_accy 80.000, Test_accy 56.350
2023-12-19 07:13:31,721 [slca.py] => Task 1, Epoch 11/20 => Loss 0.796
2023-12-19 07:13:33,250 [slca.py] => Task 1, Epoch 12/20 => Loss 0.634
2023-12-19 07:13:34,812 [slca.py] => Task 1, Epoch 13/20 => Loss 0.437
2023-12-19 07:13:36,336 [slca.py] => Task 1, Epoch 14/20 => Loss 0.469
2023-12-19 07:13:44,026 [slca.py] => Task 1, Epoch 15/20 => Loss 0.405, Train_accy 96.250, Test_accy 66.770
2023-12-19 07:13:45,578 [slca.py] => Task 1, Epoch 16/20 => Loss 0.236
2023-12-19 07:13:47,045 [slca.py] => Task 1, Epoch 17/20 => Loss 0.273
2023-12-19 07:13:48,507 [slca.py] => Task 1, Epoch 18/20 => Loss 0.239
2023-12-19 07:13:50,004 [slca.py] => Task 1, Epoch 19/20 => Loss 0.151
2023-12-19 07:13:57,926 [slca.py] => Task 1, Epoch 20/20 => Loss 0.249, Train_accy 91.250, Test_accy 70.680
2023-12-19 07:14:08,367 [slca.py] => CA Task 1 => Loss 0.021, Test_accy 72.140
2023-12-19 07:14:13,446 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 72.240
2023-12-19 07:14:18,723 [slca.py] => CA Task 1 => Loss 0.001, Test_accy 72.340
2023-12-19 07:14:23,784 [slca.py] => CA Task 1 => Loss 0.001, Test_accy 72.660
2023-12-19 07:14:28,902 [slca.py] => CA Task 1 => Loss 0.001, Test_accy 72.710
2023-12-19 07:14:33,935 [slca.py] => Exemplar size: 0
2023-12-19 07:14:34,655 [trainer.py] => No NME accuracy.
2023-12-19 07:14:34,655 [trainer.py] => CNN: {'total': 72.71, '00-09': 81.77, '10-19': 63.65, 'old': 81.77, 'new': 63.65}
2023-12-19 07:14:34,656 [trainer.py] => CNN top1 curve: [85.49, 72.71]
2023-12-19 07:14:34,656 [trainer.py] => CNN top1 avg: 79.1
2023-12-19 07:14:34,656 [trainer.py] => CNN top5 curve: [99.44, 96.51]

2023-12-19 07:14:34,656 [trainer.py] => All params: 85814036
2023-12-19 07:14:34,657 [trainer.py] => Trainable params: 85814036
2023-12-19 07:14:34,657 [slca.py] => Learning on 20-30
2023-12-19 07:14:36,510 [slca.py] => Task 2, Epoch 1/20 => Loss 2.391
2023-12-19 07:14:38,416 [slca.py] => Task 2, Epoch 2/20 => Loss 2.269
2023-12-19 07:14:40,167 [slca.py] => Task 2, Epoch 3/20 => Loss 2.007
2023-12-19 07:14:42,034 [slca.py] => Task 2, Epoch 4/20 => Loss 1.919
2023-12-19 07:14:52,282 [slca.py] => Task 2, Epoch 5/20 => Loss 1.650, Train_accy 62.500, Test_accy 48.000
2023-12-19 07:14:54,123 [slca.py] => Task 2, Epoch 6/20 => Loss 1.481
2023-12-19 07:14:56,006 [slca.py] => Task 2, Epoch 7/20 => Loss 1.164
2023-12-19 07:14:58,032 [slca.py] => Task 2, Epoch 8/20 => Loss 0.973
2023-12-19 07:14:59,872 [slca.py] => Task 2, Epoch 9/20 => Loss 0.757
2023-12-19 07:15:10,253 [slca.py] => Task 2, Epoch 10/20 => Loss 0.748, Train_accy 83.330, Test_accy 59.240
2023-12-19 07:15:12,024 [slca.py] => Task 2, Epoch 11/20 => Loss 0.501
2023-12-19 07:15:13,990 [slca.py] => Task 2, Epoch 12/20 => Loss 0.307
2023-12-19 07:15:15,739 [slca.py] => Task 2, Epoch 13/20 => Loss 0.263
2023-12-19 07:15:17,577 [slca.py] => Task 2, Epoch 14/20 => Loss 0.321
2023-12-19 07:15:27,821 [slca.py] => Task 2, Epoch 15/20 => Loss 0.340, Train_accy 90.830, Test_accy 68.240
2023-12-19 07:15:29,715 [slca.py] => Task 2, Epoch 16/20 => Loss 0.210
2023-12-19 07:15:31,633 [slca.py] => Task 2, Epoch 17/20 => Loss 0.358
2023-12-19 07:15:33,509 [slca.py] => Task 2, Epoch 18/20 => Loss 0.221
2023-12-19 07:15:35,289 [slca.py] => Task 2, Epoch 19/20 => Loss 0.180
2023-12-19 07:15:45,904 [slca.py] => Task 2, Epoch 20/20 => Loss 0.148, Train_accy 92.500, Test_accy 68.720
2023-12-19 07:15:58,659 [slca.py] => CA Task 2 => Loss 0.025, Test_accy 69.600
2023-12-19 07:16:06,035 [slca.py] => CA Task 2 => Loss 0.002, Test_accy 70.650
2023-12-19 07:16:13,194 [slca.py] => CA Task 2 => Loss 0.001, Test_accy 71.160
2023-12-19 07:16:20,308 [slca.py] => CA Task 2 => Loss 0.001, Test_accy 71.300
2023-12-19 07:16:27,426 [slca.py] => CA Task 2 => Loss 0.001, Test_accy 71.370
2023-12-19 07:16:34,420 [slca.py] => Exemplar size: 0
2023-12-19 07:16:35,192 [trainer.py] => No NME accuracy.
2023-12-19 07:16:35,192 [trainer.py] => CNN: {'total': 71.37, '00-09': 78.63, '10-19': 61.55, '20-29': 73.96, 'old': 70.07, 'new': 73.96}
2023-12-19 07:16:35,192 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37]
2023-12-19 07:16:35,192 [trainer.py] => CNN top1 avg: 76.52333333333333
2023-12-19 07:16:35,192 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9]

2023-12-19 07:16:35,193 [trainer.py] => All params: 85821726
2023-12-19 07:16:35,193 [trainer.py] => Trainable params: 85821726
2023-12-19 07:16:35,194 [slca.py] => Learning on 30-40
2023-12-19 07:16:37,449 [slca.py] => Task 3, Epoch 1/20 => Loss 2.333
2023-12-19 07:16:39,496 [slca.py] => Task 3, Epoch 2/20 => Loss 2.039
2023-12-19 07:16:41,481 [slca.py] => Task 3, Epoch 3/20 => Loss 1.691
2023-12-19 07:16:43,540 [slca.py] => Task 3, Epoch 4/20 => Loss 1.434
2023-12-19 07:16:56,512 [slca.py] => Task 3, Epoch 5/20 => Loss 0.867, Train_accy 79.380, Test_accy 59.450
2023-12-19 07:16:58,574 [slca.py] => Task 3, Epoch 6/20 => Loss 0.726
2023-12-19 07:17:00,697 [slca.py] => Task 3, Epoch 7/20 => Loss 0.466
2023-12-19 07:17:02,793 [slca.py] => Task 3, Epoch 8/20 => Loss 0.203
2023-12-19 07:17:04,840 [slca.py] => Task 3, Epoch 9/20 => Loss 0.336
2023-12-19 07:17:17,378 [slca.py] => Task 3, Epoch 10/20 => Loss 0.185, Train_accy 80.000, Test_accy 67.540
2023-12-19 07:17:19,346 [slca.py] => Task 3, Epoch 11/20 => Loss 0.285
2023-12-19 07:17:21,367 [slca.py] => Task 3, Epoch 12/20 => Loss 0.131
2023-12-19 07:17:23,410 [slca.py] => Task 3, Epoch 13/20 => Loss 0.067
2023-12-19 07:17:25,541 [slca.py] => Task 3, Epoch 14/20 => Loss 0.060
2023-12-19 07:17:38,308 [slca.py] => Task 3, Epoch 15/20 => Loss 0.126, Train_accy 85.620, Test_accy 68.200
2023-12-19 07:17:40,300 [slca.py] => Task 3, Epoch 16/20 => Loss 0.065
2023-12-19 07:17:42,384 [slca.py] => Task 3, Epoch 17/20 => Loss 0.093
2023-12-19 07:17:44,475 [slca.py] => Task 3, Epoch 18/20 => Loss 0.110
2023-12-19 07:17:46,456 [slca.py] => Task 3, Epoch 19/20 => Loss 0.046
2023-12-19 07:17:59,373 [slca.py] => Task 3, Epoch 20/20 => Loss 0.027, Train_accy 85.620, Test_accy 67.210
2023-12-19 07:18:14,098 [slca.py] => CA Task 3 => Loss 0.045, Test_accy 70.490
2023-12-19 07:18:23,351 [slca.py] => CA Task 3 => Loss 0.004, Test_accy 71.570
2023-12-19 07:18:32,778 [slca.py] => CA Task 3 => Loss 0.002, Test_accy 71.880
2023-12-19 07:18:42,034 [slca.py] => CA Task 3 => Loss 0.001, Test_accy 71.980
2023-12-19 07:18:51,327 [slca.py] => CA Task 3 => Loss 0.001, Test_accy 71.950
2023-12-19 07:19:00,518 [slca.py] => Exemplar size: 0
2023-12-19 07:19:01,254 [trainer.py] => No NME accuracy.
2023-12-19 07:19:01,255 [trainer.py] => CNN: {'total': 71.95, '00-09': 73.23, '10-19': 56.45, '20-29': 74.4, '30-39': 83.7, 'old': 68.02, 'new': 83.7}
2023-12-19 07:19:01,255 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37, 71.95]
2023-12-19 07:19:01,255 [trainer.py] => CNN top1 avg: 75.38
2023-12-19 07:19:01,255 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9, 94.18]

2023-12-19 07:19:01,255 [trainer.py] => All params: 85829416
2023-12-19 07:19:01,256 [trainer.py] => Trainable params: 85829416
2023-12-19 07:19:01,256 [slca.py] => Learning on 40-50
2023-12-19 07:19:03,594 [slca.py] => Task 4, Epoch 1/20 => Loss 2.311
2023-12-19 07:19:05,789 [slca.py] => Task 4, Epoch 2/20 => Loss 1.975
2023-12-19 07:19:07,990 [slca.py] => Task 4, Epoch 3/20 => Loss 1.468
2023-12-19 07:19:10,309 [slca.py] => Task 4, Epoch 4/20 => Loss 1.005
2023-12-19 07:19:25,226 [slca.py] => Task 4, Epoch 5/20 => Loss 0.527, Train_accy 76.500, Test_accy 58.430
2023-12-19 07:19:27,437 [slca.py] => Task 4, Epoch 6/20 => Loss 0.408
2023-12-19 07:19:29,753 [slca.py] => Task 4, Epoch 7/20 => Loss 0.180
2023-12-19 07:19:32,008 [slca.py] => Task 4, Epoch 8/20 => Loss 0.356
2023-12-19 07:19:34,277 [slca.py] => Task 4, Epoch 9/20 => Loss 0.141
2023-12-19 07:19:49,038 [slca.py] => Task 4, Epoch 10/20 => Loss 0.159, Train_accy 84.000, Test_accy 65.060
2023-12-19 07:19:51,328 [slca.py] => Task 4, Epoch 11/20 => Loss 0.184
2023-12-19 07:19:53,528 [slca.py] => Task 4, Epoch 12/20 => Loss 0.072
2023-12-19 07:19:55,728 [slca.py] => Task 4, Epoch 13/20 => Loss 0.145
2023-12-19 07:19:58,012 [slca.py] => Task 4, Epoch 14/20 => Loss 0.026
2023-12-19 07:20:12,872 [slca.py] => Task 4, Epoch 15/20 => Loss 0.130, Train_accy 84.000, Test_accy 64.020
2023-12-19 07:20:15,149 [slca.py] => Task 4, Epoch 16/20 => Loss 0.117
2023-12-19 07:20:17,417 [slca.py] => Task 4, Epoch 17/20 => Loss 0.055
2023-12-19 07:20:19,770 [slca.py] => Task 4, Epoch 18/20 => Loss 0.076
2023-12-19 07:20:21,982 [slca.py] => Task 4, Epoch 19/20 => Loss 0.047
2023-12-19 07:20:36,983 [slca.py] => Task 4, Epoch 20/20 => Loss 0.100, Train_accy 84.500, Test_accy 63.020
2023-12-19 07:20:53,867 [slca.py] => CA Task 4 => Loss 0.043, Test_accy 67.710
2023-12-19 07:21:05,134 [slca.py] => CA Task 4 => Loss 0.003, Test_accy 67.970
2023-12-19 07:21:16,399 [slca.py] => CA Task 4 => Loss 0.003, Test_accy 68.430
2023-12-19 07:21:27,706 [slca.py] => CA Task 4 => Loss 0.001, Test_accy 68.590
2023-12-19 07:21:39,049 [slca.py] => CA Task 4 => Loss 0.002, Test_accy 68.610
2023-12-19 07:21:50,084 [slca.py] => Exemplar size: 0
2023-12-19 07:21:50,876 [trainer.py] => No NME accuracy.
2023-12-19 07:21:50,877 [trainer.py] => CNN: {'total': 68.61, '00-09': 67.33, '10-19': 53.85, '20-29': 64.93, '30-39': 80.34, '40-49': 76.6, 'old': 66.61, 'new': 76.6}
2023-12-19 07:21:50,877 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37, 71.95, 68.61]
2023-12-19 07:21:50,877 [trainer.py] => CNN top1 avg: 74.026
2023-12-19 07:21:50,877 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9, 94.18, 92.77]

2023-12-19 07:21:50,877 [trainer.py] => All params: 85837106
2023-12-19 07:21:50,878 [trainer.py] => Trainable params: 85837106
2023-12-19 07:21:50,878 [slca.py] => Learning on 50-60
2023-12-19 07:21:53,339 [slca.py] => Task 5, Epoch 1/20 => Loss 2.437
2023-12-19 07:21:55,733 [slca.py] => Task 5, Epoch 2/20 => Loss 2.073
2023-12-19 07:21:58,195 [slca.py] => Task 5, Epoch 3/20 => Loss 1.479
2023-12-19 07:22:00,581 [slca.py] => Task 5, Epoch 4/20 => Loss 1.036
2023-12-19 07:22:17,440 [slca.py] => Task 5, Epoch 5/20 => Loss 0.546, Train_accy 75.000, Test_accy 56.330
2023-12-19 07:22:19,912 [slca.py] => Task 5, Epoch 6/20 => Loss 0.349
2023-12-19 07:22:22,353 [slca.py] => Task 5, Epoch 7/20 => Loss 0.249
2023-12-19 07:22:24,733 [slca.py] => Task 5, Epoch 8/20 => Loss 0.130
2023-12-19 07:22:27,139 [slca.py] => Task 5, Epoch 9/20 => Loss 0.278
2023-12-19 07:22:44,140 [slca.py] => Task 5, Epoch 10/20 => Loss 0.095, Train_accy 85.000, Test_accy 61.290
2023-12-19 07:22:46,515 [slca.py] => Task 5, Epoch 11/20 => Loss 0.186
2023-12-19 07:22:48,881 [slca.py] => Task 5, Epoch 12/20 => Loss 0.053
2023-12-19 07:22:51,252 [slca.py] => Task 5, Epoch 13/20 => Loss 0.041
2023-12-19 07:22:53,642 [slca.py] => Task 5, Epoch 14/20 => Loss 0.151
2023-12-19 07:23:10,687 [slca.py] => Task 5, Epoch 15/20 => Loss 0.040, Train_accy 85.830, Test_accy 61.350
2023-12-19 07:23:13,101 [slca.py] => Task 5, Epoch 16/20 => Loss 0.072
2023-12-19 07:23:15,539 [slca.py] => Task 5, Epoch 17/20 => Loss 0.024
2023-12-19 07:23:17,961 [slca.py] => Task 5, Epoch 18/20 => Loss 0.038
2023-12-19 07:23:20,448 [slca.py] => Task 5, Epoch 19/20 => Loss 0.051
2023-12-19 07:23:37,412 [slca.py] => Task 5, Epoch 20/20 => Loss 0.023, Train_accy 82.080, Test_accy 60.850
2023-12-19 07:23:56,170 [slca.py] => CA Task 5 => Loss 0.042, Test_accy 67.370
2023-12-19 07:24:09,320 [slca.py] => CA Task 5 => Loss 0.003, Test_accy 67.560
2023-12-19 07:24:22,397 [slca.py] => CA Task 5 => Loss 0.002, Test_accy 67.830
2023-12-19 07:24:35,537 [slca.py] => CA Task 5 => Loss 0.001, Test_accy 67.830
2023-12-19 07:24:48,695 [slca.py] => CA Task 5 => Loss 0.001, Test_accy 67.880
2023-12-19 07:25:01,529 [slca.py] => Exemplar size: 0
2023-12-19 07:25:02,257 [trainer.py] => No NME accuracy.
2023-12-19 07:25:02,257 [trainer.py] => CNN: {'total': 67.88, '00-09': 65.24, '10-19': 53.04, '20-29': 63.08, '30-39': 80.45, '40-49': 70.71, '50-59': 74.97, 'old': 66.48, 'new': 74.97}
2023-12-19 07:25:02,257 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37, 71.95, 68.61, 67.88]
2023-12-19 07:25:02,257 [trainer.py] => CNN top1 avg: 73.00166666666667
2023-12-19 07:25:02,257 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9, 94.18, 92.77, 92.39]

2023-12-19 07:25:02,258 [trainer.py] => All params: 85844796
2023-12-19 07:25:02,258 [trainer.py] => Trainable params: 85844796
2023-12-19 07:25:02,259 [slca.py] => Learning on 60-70
2023-12-19 07:25:04,964 [slca.py] => Task 6, Epoch 1/20 => Loss 2.300
2023-12-19 07:25:07,648 [slca.py] => Task 6, Epoch 2/20 => Loss 1.326
2023-12-19 07:25:10,306 [slca.py] => Task 6, Epoch 3/20 => Loss 0.978
2023-12-19 07:25:13,071 [slca.py] => Task 6, Epoch 4/20 => Loss 0.782
2023-12-19 07:25:32,281 [slca.py] => Task 6, Epoch 5/20 => Loss 0.312, Train_accy 80.360, Test_accy 57.840
2023-12-19 07:25:34,911 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 07:25:37,540 [slca.py] => Task 6, Epoch 7/20 => Loss 0.146
2023-12-19 07:25:40,167 [slca.py] => Task 6, Epoch 8/20 => Loss 0.076
2023-12-19 07:25:42,777 [slca.py] => Task 6, Epoch 9/20 => Loss 0.082
2023-12-19 07:26:01,922 [slca.py] => Task 6, Epoch 10/20 => Loss 0.054, Train_accy 82.500, Test_accy 59.230
2023-12-19 07:26:04,505 [slca.py] => Task 6, Epoch 11/20 => Loss 0.050
2023-12-19 07:26:07,179 [slca.py] => Task 6, Epoch 12/20 => Loss 0.073
2023-12-19 07:26:09,772 [slca.py] => Task 6, Epoch 13/20 => Loss 0.051
2023-12-19 07:26:12,445 [slca.py] => Task 6, Epoch 14/20 => Loss 0.084
2023-12-19 07:26:31,678 [slca.py] => Task 6, Epoch 15/20 => Loss 0.026, Train_accy 78.570, Test_accy 59.220
2023-12-19 07:26:34,326 [slca.py] => Task 6, Epoch 16/20 => Loss 0.115
2023-12-19 07:26:36,913 [slca.py] => Task 6, Epoch 17/20 => Loss 0.078
2023-12-19 07:26:39,605 [slca.py] => Task 6, Epoch 18/20 => Loss 0.160
2023-12-19 07:26:42,288 [slca.py] => Task 6, Epoch 19/20 => Loss 0.032
2023-12-19 07:27:01,534 [slca.py] => Task 6, Epoch 20/20 => Loss 0.062, Train_accy 81.790, Test_accy 60.240
2023-12-19 07:27:22,425 [slca.py] => CA Task 6 => Loss 0.044, Test_accy 67.550
2023-12-19 07:27:37,627 [slca.py] => CA Task 6 => Loss 0.004, Test_accy 67.870
2023-12-19 07:27:53,022 [slca.py] => CA Task 6 => Loss 0.002, Test_accy 68.140
2023-12-19 07:28:08,273 [slca.py] => CA Task 6 => Loss 0.002, Test_accy 68.100
2023-12-19 07:28:23,734 [slca.py] => CA Task 6 => Loss 0.002, Test_accy 68.110
2023-12-19 07:28:38,576 [slca.py] => Exemplar size: 0
2023-12-19 07:28:39,356 [trainer.py] => No NME accuracy.
2023-12-19 07:28:39,356 [trainer.py] => CNN: {'total': 68.11, '00-09': 60.47, '10-19': 51.27, '20-29': 62.37, '30-39': 79.25, '40-49': 70.48, '50-59': 74.62, '60-69': 78.38, 'old': 66.41, 'new': 78.38}
2023-12-19 07:28:39,356 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37, 71.95, 68.61, 67.88, 68.11]
2023-12-19 07:28:39,356 [trainer.py] => CNN top1 avg: 72.30285714285715
2023-12-19 07:28:39,356 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9, 94.18, 92.77, 92.39, 91.17]

2023-12-19 07:28:39,357 [trainer.py] => All params: 85852486
2023-12-19 07:28:39,357 [trainer.py] => Trainable params: 85852486
2023-12-19 07:28:39,358 [slca.py] => Learning on 70-80
2023-12-19 07:28:42,614 [slca.py] => Task 7, Epoch 1/20 => Loss 2.476
2023-12-19 07:28:45,616 [slca.py] => Task 7, Epoch 2/20 => Loss 1.749
2023-12-19 07:28:48,668 [slca.py] => Task 7, Epoch 3/20 => Loss 0.995
2023-12-19 07:28:51,751 [slca.py] => Task 7, Epoch 4/20 => Loss 0.438
2023-12-19 07:29:13,637 [slca.py] => Task 7, Epoch 5/20 => Loss 0.296, Train_accy 78.440, Test_accy 57.380
2023-12-19 07:29:16,766 [slca.py] => Task 7, Epoch 6/20 => Loss 0.225
2023-12-19 07:29:19,803 [slca.py] => Task 7, Epoch 7/20 => Loss 0.223
2023-12-19 07:29:22,813 [slca.py] => Task 7, Epoch 8/20 => Loss 0.231
2023-12-19 07:29:25,821 [slca.py] => Task 7, Epoch 9/20 => Loss 0.099
2023-12-19 07:29:47,483 [slca.py] => Task 7, Epoch 10/20 => Loss 0.186, Train_accy 82.810, Test_accy 60.640
2023-12-19 07:29:50,495 [slca.py] => Task 7, Epoch 11/20 => Loss 0.179
2023-12-19 07:29:53,535 [slca.py] => Task 7, Epoch 12/20 => Loss 0.126
2023-12-19 07:29:56,562 [slca.py] => Task 7, Epoch 13/20 => Loss 0.235
2023-12-19 07:29:59,594 [slca.py] => Task 7, Epoch 14/20 => Loss 0.118
2023-12-19 07:30:21,275 [slca.py] => Task 7, Epoch 15/20 => Loss 0.022, Train_accy 81.250, Test_accy 59.690
2023-12-19 07:30:24,502 [slca.py] => Task 7, Epoch 16/20 => Loss 0.154
2023-12-19 07:30:27,585 [slca.py] => Task 7, Epoch 17/20 => Loss 0.016
2023-12-19 07:30:30,579 [slca.py] => Task 7, Epoch 18/20 => Loss 0.046
2023-12-19 07:30:33,556 [slca.py] => Task 7, Epoch 19/20 => Loss 0.029
2023-12-19 07:30:55,259 [slca.py] => Task 7, Epoch 20/20 => Loss 0.064, Train_accy 78.440, Test_accy 59.140
2023-12-19 07:31:19,023 [slca.py] => CA Task 7 => Loss 0.045, Test_accy 66.240
2023-12-19 07:31:36,101 [slca.py] => CA Task 7 => Loss 0.004, Test_accy 66.870
2023-12-19 07:31:53,433 [slca.py] => CA Task 7 => Loss 0.002, Test_accy 67.070
2023-12-19 07:32:10,533 [slca.py] => CA Task 7 => Loss 0.002, Test_accy 67.090
2023-12-19 07:32:27,709 [slca.py] => CA Task 7 => Loss 0.001, Test_accy 67.140
2023-12-19 07:32:44,393 [slca.py] => Exemplar size: 0
2023-12-19 07:32:45,118 [trainer.py] => No NME accuracy.
2023-12-19 07:32:45,118 [trainer.py] => CNN: {'total': 67.14, '00-09': 55.4, '10-19': 48.74, '20-29': 59.92, '30-39': 80.34, '40-49': 70.32, '50-59': 72.98, '60-69': 72.96, '70-79': 76.51, 'old': 65.8, 'new': 76.51}
2023-12-19 07:32:45,118 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37, 71.95, 68.61, 67.88, 68.11, 67.14]
2023-12-19 07:32:45,118 [trainer.py] => CNN top1 avg: 71.6575
2023-12-19 07:32:45,118 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9, 94.18, 92.77, 92.39, 91.17, 90.15]

2023-12-19 07:32:45,119 [trainer.py] => All params: 85860176
2023-12-19 07:32:45,119 [trainer.py] => Trainable params: 85860176
2023-12-19 07:32:45,120 [slca.py] => Learning on 80-90
2023-12-19 07:32:48,440 [slca.py] => Task 8, Epoch 1/20 => Loss 2.430
2023-12-19 07:32:51,670 [slca.py] => Task 8, Epoch 2/20 => Loss 1.606
2023-12-19 07:32:54,878 [slca.py] => Task 8, Epoch 3/20 => Loss 0.839
2023-12-19 07:32:58,085 [slca.py] => Task 8, Epoch 4/20 => Loss 0.292
2023-12-19 07:33:21,894 [slca.py] => Task 8, Epoch 5/20 => Loss 0.171, Train_accy 78.060, Test_accy 57.460
2023-12-19 07:33:25,266 [slca.py] => Task 8, Epoch 6/20 => Loss 0.201
2023-12-19 07:33:28,495 [slca.py] => Task 8, Epoch 7/20 => Loss 0.111
2023-12-19 07:33:31,843 [slca.py] => Task 8, Epoch 8/20 => Loss 0.068
2023-12-19 07:33:35,038 [slca.py] => Task 8, Epoch 9/20 => Loss 0.057
2023-12-19 07:33:58,962 [slca.py] => Task 8, Epoch 10/20 => Loss 0.058, Train_accy 80.830, Test_accy 59.690
2023-12-19 07:34:02,246 [slca.py] => Task 8, Epoch 11/20 => Loss 0.037
2023-12-19 07:34:05,488 [slca.py] => Task 8, Epoch 12/20 => Loss 0.038
2023-12-19 07:34:08,761 [slca.py] => Task 8, Epoch 13/20 => Loss 0.028
2023-12-19 07:34:11,981 [slca.py] => Task 8, Epoch 14/20 => Loss 0.113
2023-12-19 07:34:35,919 [slca.py] => Task 8, Epoch 15/20 => Loss 0.016, Train_accy 81.110, Test_accy 58.600
2023-12-19 07:34:39,213 [slca.py] => Task 8, Epoch 16/20 => Loss 0.077
2023-12-19 07:34:42,504 [slca.py] => Task 8, Epoch 17/20 => Loss 0.045
2023-12-19 07:34:45,736 [slca.py] => Task 8, Epoch 18/20 => Loss 0.098
2023-12-19 07:34:48,961 [slca.py] => Task 8, Epoch 19/20 => Loss 0.014
2023-12-19 07:35:13,033 [slca.py] => Task 8, Epoch 20/20 => Loss 0.069, Train_accy 79.170, Test_accy 58.210
2023-12-19 07:35:38,757 [slca.py] => CA Task 8 => Loss 0.043, Test_accy 64.070
2023-12-19 07:35:58,015 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 64.700
2023-12-19 07:36:17,223 [slca.py] => CA Task 8 => Loss 0.003, Test_accy 65.040
2023-12-19 07:36:36,406 [slca.py] => CA Task 8 => Loss 0.002, Test_accy 65.040
2023-12-19 07:36:55,867 [slca.py] => CA Task 8 => Loss 0.001, Test_accy 65.150
2023-12-19 07:37:14,512 [slca.py] => Exemplar size: 0
2023-12-19 07:37:15,289 [trainer.py] => No NME accuracy.
2023-12-19 07:37:15,289 [trainer.py] => CNN: {'total': 65.15, '00-09': 50.65, '10-19': 43.47, '20-29': 54.76, '30-39': 76.51, '40-49': 65.42, '50-59': 70.34, '60-69': 69.74, '70-79': 76.44, '80-89': 78.99, 'old': 63.41, 'new': 78.99}
2023-12-19 07:37:15,289 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37, 71.95, 68.61, 67.88, 68.11, 67.14, 65.15]
2023-12-19 07:37:15,289 [trainer.py] => CNN top1 avg: 70.93444444444444
2023-12-19 07:37:15,289 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9, 94.18, 92.77, 92.39, 91.17, 90.15, 89.44]

2023-12-19 07:37:15,290 [trainer.py] => All params: 85867866
2023-12-19 07:37:15,291 [trainer.py] => Trainable params: 85867866
2023-12-19 07:37:15,291 [slca.py] => Learning on 90-100
2023-12-19 07:37:18,897 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 07:37:22,421 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 07:37:25,971 [slca.py] => Task 9, Epoch 3/20 => Loss 0.694
2023-12-19 07:37:29,489 [slca.py] => Task 9, Epoch 4/20 => Loss 0.250
2023-12-19 07:37:55,776 [slca.py] => Task 9, Epoch 5/20 => Loss 0.082, Train_accy 80.000, Test_accy 55.340
2023-12-19 07:37:59,283 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 07:38:02,720 [slca.py] => Task 9, Epoch 7/20 => Loss 0.112
2023-12-19 07:38:06,162 [slca.py] => Task 9, Epoch 8/20 => Loss 0.120
2023-12-19 07:38:09,581 [slca.py] => Task 9, Epoch 9/20 => Loss 0.090
2023-12-19 07:38:35,856 [slca.py] => Task 9, Epoch 10/20 => Loss 0.014, Train_accy 81.250, Test_accy 56.730
2023-12-19 07:38:39,251 [slca.py] => Task 9, Epoch 11/20 => Loss 0.023
2023-12-19 07:38:42,671 [slca.py] => Task 9, Epoch 12/20 => Loss 0.043
2023-12-19 07:38:46,162 [slca.py] => Task 9, Epoch 13/20 => Loss 0.044
2023-12-19 07:38:49,673 [slca.py] => Task 9, Epoch 14/20 => Loss 0.090
2023-12-19 07:39:16,284 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 79.750, Test_accy 56.490
2023-12-19 07:39:19,716 [slca.py] => Task 9, Epoch 16/20 => Loss 0.019
2023-12-19 07:39:23,128 [slca.py] => Task 9, Epoch 17/20 => Loss 0.129
2023-12-19 07:39:26,589 [slca.py] => Task 9, Epoch 18/20 => Loss 0.008
2023-12-19 07:39:30,050 [slca.py] => Task 9, Epoch 19/20 => Loss 0.035
2023-12-19 07:39:56,191 [slca.py] => Task 9, Epoch 20/20 => Loss 0.052, Train_accy 80.500, Test_accy 56.880
2023-12-19 07:40:24,205 [slca.py] => CA Task 9 => Loss 0.041, Test_accy 63.040
2023-12-19 07:40:45,574 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 63.780
2023-12-19 07:41:06,862 [slca.py] => CA Task 9 => Loss 0.003, Test_accy 63.990
2023-12-19 07:41:28,118 [slca.py] => CA Task 9 => Loss 0.002, Test_accy 64.070
2023-12-19 07:41:49,359 [slca.py] => CA Task 9 => Loss 0.002, Test_accy 64.110
2023-12-19 07:42:09,843 [slca.py] => Exemplar size: 0
2023-12-19 07:42:10,645 [trainer.py] => No NME accuracy.
2023-12-19 07:42:10,645 [trainer.py] => CNN: {'total': 64.11, '00-09': 47.15, '10-19': 45.55, '20-29': 54.55, '30-39': 71.94, '40-49': 66.0, '50-59': 67.84, '60-69': 70.27, '70-79': 74.12, '80-89': 76.65, '90-99': 67.1, 'old': 63.78, 'new': 67.1}
2023-12-19 07:42:10,645 [trainer.py] => CNN top1 curve: [85.49, 72.71, 71.37, 71.95, 68.61, 67.88, 68.11, 67.14, 65.15, 64.11]
2023-12-19 07:42:10,645 [trainer.py] => CNN top1 avg: 70.252
2023-12-19 07:42:10,645 [trainer.py] => CNN top5 curve: [99.44, 96.51, 94.9, 94.18, 92.77, 92.39, 91.17, 90.15, 89.44, 89.47]

2023-12-19 07:42:10,646 [trainer.py] => final accs: [64.11]
2023-12-19 07:42:10,646 [trainer.py] => avg accs: [70.252]
