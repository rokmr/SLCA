2023-12-19 05:18:45,289 [trainer.py] => config: /home/gayathri/rohit/SLCA/exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 05:18:45,289 [trainer.py] => test_only: False
2023-12-19 05:18:45,289 [trainer.py] => prefix: reproduce
2023-12-19 05:18:45,289 [trainer.py] => dataset: cifar100_224
2023-12-19 05:18:45,289 [trainer.py] => memory_size: 0
2023-12-19 05:18:45,289 [trainer.py] => memory_per_class: 0
2023-12-19 05:18:45,289 [trainer.py] => fixed_memory: False
2023-12-19 05:18:45,289 [trainer.py] => shuffle: False
2023-12-19 05:18:45,289 [trainer.py] => init_cls: 10
2023-12-19 05:18:45,289 [trainer.py] => increment: 10
2023-12-19 05:18:45,289 [trainer.py] => model_name: slca_cifar
2023-12-19 05:18:45,289 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 05:18:45,289 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 05:18:45,289 [trainer.py] => device: [device(type='cuda', index=1)]
2023-12-19 05:18:45,289 [trainer.py] => seed: 0
2023-12-19 05:18:45,289 [trainer.py] => epochs: 20
2023-12-19 05:18:45,289 [trainer.py] => ca_epochs: 5
2023-12-19 05:18:45,289 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-19 05:18:45,289 [trainer.py] => milestones: [18]
2023-12-19 05:18:45,289 [trainer.py] => lr: 0.001
2023-12-19 05:18:45,289 [trainer.py] => lr_decay: 0.1
2023-12-19 05:18:45,289 [trainer.py] => weight_decay: 0.0005
2023-12-19 05:18:45,289 [trainer.py] => u_batch_size: 256
2023-12-19 05:18:45,289 [trainer.py] => s_batch_size: 3
2023-12-19 05:18:45,290 [trainer.py] => multicrop: 2
2023-12-19 05:18:45,290 [trainer.py] => us_multicrop: 2
2023-12-19 05:18:45,290 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 05:18:45,290 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 05:18:45,290 [trainer.py] => buffer_size: 500
2023-12-19 05:18:45,290 [trainer.py] => run_id: 0
2023-12-19 05:18:46,813 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 05:18:48,669 [trainer.py] => All params: 85798656
2023-12-19 05:18:48,669 [trainer.py] => Trainable params: 85798656
2023-12-19 05:18:48,670 [slca.py] => Learning on 0-10
2023-12-19 05:18:54,055 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 05:18:55,104 [slca.py] => Task 0, Epoch 2/20 => Loss 2.439
2023-12-19 05:18:56,153 [slca.py] => Task 0, Epoch 3/20 => Loss 2.424
2023-12-19 05:18:57,245 [slca.py] => Task 0, Epoch 4/20 => Loss 2.363
2023-12-19 05:19:01,909 [slca.py] => Task 0, Epoch 5/20 => Loss 2.269, Train_accy 15.000, Test_accy 13.620
2023-12-19 05:19:02,946 [slca.py] => Task 0, Epoch 6/20 => Loss 2.281
2023-12-19 05:19:03,958 [slca.py] => Task 0, Epoch 7/20 => Loss 2.121
2023-12-19 05:19:04,979 [slca.py] => Task 0, Epoch 8/20 => Loss 2.181
2023-12-19 05:19:06,022 [slca.py] => Task 0, Epoch 9/20 => Loss 2.056
2023-12-19 05:19:10,699 [slca.py] => Task 0, Epoch 10/20 => Loss 1.933, Train_accy 35.000, Test_accy 28.460
2023-12-19 05:19:11,741 [slca.py] => Task 0, Epoch 11/20 => Loss 1.884
2023-12-19 05:19:12,763 [slca.py] => Task 0, Epoch 12/20 => Loss 1.908
2023-12-19 05:19:13,793 [slca.py] => Task 0, Epoch 13/20 => Loss 1.763
2023-12-19 05:19:14,851 [slca.py] => Task 0, Epoch 14/20 => Loss 1.718
2023-12-19 05:19:19,677 [slca.py] => Task 0, Epoch 15/20 => Loss 1.644, Train_accy 72.500, Test_accy 44.420
2023-12-19 05:19:20,782 [slca.py] => Task 0, Epoch 16/20 => Loss 1.558
2023-12-19 05:19:21,871 [slca.py] => Task 0, Epoch 17/20 => Loss 1.497
2023-12-19 05:19:22,942 [slca.py] => Task 0, Epoch 18/20 => Loss 1.473
2023-12-19 05:19:23,988 [slca.py] => Task 0, Epoch 19/20 => Loss 1.496
2023-12-19 05:19:28,710 [slca.py] => Task 0, Epoch 20/20 => Loss 1.374, Train_accy 70.000, Test_accy 54.460
2023-12-19 05:19:36,411 [slca.py] => CA Task 0 => Loss 0.198, Test_accy 62.170
2023-12-19 05:19:39,203 [slca.py] => CA Task 0 => Loss 0.022, Test_accy 66.740
2023-12-19 05:19:41,962 [slca.py] => CA Task 0 => Loss 0.009, Test_accy 67.750
2023-12-19 05:19:44,754 [slca.py] => CA Task 0 => Loss 0.006, Test_accy 67.630
2023-12-19 05:19:47,602 [slca.py] => CA Task 0 => Loss 0.006, Test_accy 68.080
2023-12-19 05:19:50,519 [slca.py] => Exemplar size: 0
2023-12-19 05:19:51,249 [trainer.py] => No NME accuracy.
2023-12-19 05:19:51,249 [trainer.py] => CNN: {'total': 68.08, '00-09': 68.08, 'old': 0, 'new': 68.08}
2023-12-19 05:19:51,250 [trainer.py] => CNN top1 curve: [68.08]
2023-12-19 05:19:51,250 [trainer.py] => CNN top1 avg: 68.08
2023-12-19 05:19:51,250 [trainer.py] => CNN top5 curve: [95.65]

2023-12-19 05:19:51,250 [trainer.py] => All params: 85806346
2023-12-19 05:19:51,251 [trainer.py] => Trainable params: 85806346
2023-12-19 05:19:51,251 [slca.py] => Learning on 10-20
2023-12-19 05:19:52,668 [slca.py] => Task 1, Epoch 1/20 => Loss 2.502
2023-12-19 05:19:54,004 [slca.py] => Task 1, Epoch 2/20 => Loss 2.530
2023-12-19 05:19:55,318 [slca.py] => Task 1, Epoch 3/20 => Loss 2.501
2023-12-19 05:19:56,658 [slca.py] => Task 1, Epoch 4/20 => Loss 2.492
2023-12-19 05:20:03,697 [slca.py] => Task 1, Epoch 5/20 => Loss 2.458, Train_accy 33.750, Test_accy 24.480
2023-12-19 05:20:04,980 [slca.py] => Task 1, Epoch 6/20 => Loss 2.329
2023-12-19 05:20:06,299 [slca.py] => Task 1, Epoch 7/20 => Loss 2.415
2023-12-19 05:20:07,569 [slca.py] => Task 1, Epoch 8/20 => Loss 2.236
2023-12-19 05:20:08,919 [slca.py] => Task 1, Epoch 9/20 => Loss 2.238
2023-12-19 05:20:15,941 [slca.py] => Task 1, Epoch 10/20 => Loss 2.061, Train_accy 36.250, Test_accy 25.520
2023-12-19 05:20:17,268 [slca.py] => Task 1, Epoch 11/20 => Loss 2.120
2023-12-19 05:20:18,592 [slca.py] => Task 1, Epoch 12/20 => Loss 2.029
2023-12-19 05:20:19,934 [slca.py] => Task 1, Epoch 13/20 => Loss 1.957
2023-12-19 05:20:21,289 [slca.py] => Task 1, Epoch 14/20 => Loss 1.896
2023-12-19 05:20:28,835 [slca.py] => Task 1, Epoch 15/20 => Loss 1.788, Train_accy 53.750, Test_accy 30.100
2023-12-19 05:20:30,284 [slca.py] => Task 1, Epoch 16/20 => Loss 1.781
2023-12-19 05:20:31,676 [slca.py] => Task 1, Epoch 17/20 => Loss 1.683
2023-12-19 05:20:33,053 [slca.py] => Task 1, Epoch 18/20 => Loss 1.658
2023-12-19 05:20:34,388 [slca.py] => Task 1, Epoch 19/20 => Loss 1.510
2023-12-19 05:20:42,839 [slca.py] => Task 1, Epoch 20/20 => Loss 1.558, Train_accy 55.000, Test_accy 33.910
2023-12-19 05:20:52,525 [slca.py] => CA Task 1 => Loss 0.349, Test_accy 50.000
2023-12-19 05:20:57,538 [slca.py] => CA Task 1 => Loss 0.018, Test_accy 52.240
2023-12-19 05:21:02,429 [slca.py] => CA Task 1 => Loss 0.009, Test_accy 53.330
2023-12-19 05:21:07,877 [slca.py] => CA Task 1 => Loss 0.006, Test_accy 53.540
2023-12-19 05:21:14,292 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 53.490
2023-12-19 05:21:19,274 [slca.py] => Exemplar size: 0
2023-12-19 05:21:19,994 [trainer.py] => No NME accuracy.
2023-12-19 05:21:19,994 [trainer.py] => CNN: {'total': 53.49, '00-09': 62.71, '10-19': 44.27, 'old': 62.71, 'new': 44.27}
2023-12-19 05:21:19,994 [trainer.py] => CNN top1 curve: [68.08, 53.49]
2023-12-19 05:21:19,994 [trainer.py] => CNN top1 avg: 60.785
2023-12-19 05:21:19,994 [trainer.py] => CNN top5 curve: [95.65, 86.25]

2023-12-19 05:21:19,995 [trainer.py] => All params: 85814036
2023-12-19 05:21:19,995 [trainer.py] => Trainable params: 85814036
2023-12-19 05:21:19,996 [slca.py] => Learning on 20-30
2023-12-19 05:21:21,772 [slca.py] => Task 2, Epoch 1/20 => Loss 2.450
2023-12-19 05:21:23,473 [slca.py] => Task 2, Epoch 2/20 => Loss 2.396
2023-12-19 05:21:25,158 [slca.py] => Task 2, Epoch 3/20 => Loss 2.274
2023-12-19 05:21:26,819 [slca.py] => Task 2, Epoch 4/20 => Loss 2.293
2023-12-19 05:21:36,749 [slca.py] => Task 2, Epoch 5/20 => Loss 2.223, Train_accy 37.500, Test_accy 19.970
2023-12-19 05:21:38,353 [slca.py] => Task 2, Epoch 6/20 => Loss 2.247
2023-12-19 05:21:40,000 [slca.py] => Task 2, Epoch 7/20 => Loss 2.155
2023-12-19 05:21:41,978 [slca.py] => Task 2, Epoch 8/20 => Loss 2.135
2023-12-19 05:21:43,671 [slca.py] => Task 2, Epoch 9/20 => Loss 2.035
2023-12-19 05:21:53,275 [slca.py] => Task 2, Epoch 10/20 => Loss 1.977, Train_accy 38.330, Test_accy 20.990
2023-12-19 05:21:54,887 [slca.py] => Task 2, Epoch 11/20 => Loss 1.939
2023-12-19 05:21:56,508 [slca.py] => Task 2, Epoch 12/20 => Loss 1.828
2023-12-19 05:21:58,117 [slca.py] => Task 2, Epoch 13/20 => Loss 1.763
2023-12-19 05:21:59,739 [slca.py] => Task 2, Epoch 14/20 => Loss 1.794
2023-12-19 05:22:09,378 [slca.py] => Task 2, Epoch 15/20 => Loss 1.681, Train_accy 41.670, Test_accy 24.630
2023-12-19 05:22:11,013 [slca.py] => Task 2, Epoch 16/20 => Loss 1.575
2023-12-19 05:22:12,579 [slca.py] => Task 2, Epoch 17/20 => Loss 1.567
2023-12-19 05:22:14,259 [slca.py] => Task 2, Epoch 18/20 => Loss 1.455
2023-12-19 05:22:15,946 [slca.py] => Task 2, Epoch 19/20 => Loss 1.446
2023-12-19 05:22:25,523 [slca.py] => Task 2, Epoch 20/20 => Loss 1.359, Train_accy 50.000, Test_accy 27.410
2023-12-19 05:22:38,616 [slca.py] => CA Task 2 => Loss 0.418, Test_accy 46.470
2023-12-19 05:22:46,252 [slca.py] => CA Task 2 => Loss 0.022, Test_accy 48.880
2023-12-19 05:22:53,665 [slca.py] => CA Task 2 => Loss 0.012, Test_accy 49.420
2023-12-19 05:23:00,688 [slca.py] => CA Task 2 => Loss 0.010, Test_accy 49.590
2023-12-19 05:23:08,737 [slca.py] => CA Task 2 => Loss 0.009, Test_accy 49.690
2023-12-19 05:23:15,535 [slca.py] => Exemplar size: 0
2023-12-19 05:23:16,294 [trainer.py] => No NME accuracy.
2023-12-19 05:23:16,294 [trainer.py] => CNN: {'total': 49.69, '00-09': 57.77, '10-19': 42.32, '20-29': 49.03, 'old': 50.03, 'new': 49.03}
2023-12-19 05:23:16,295 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69]
2023-12-19 05:23:16,295 [trainer.py] => CNN top1 avg: 57.086666666666666
2023-12-19 05:23:16,295 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86]

2023-12-19 05:23:16,296 [trainer.py] => All params: 85821726
2023-12-19 05:23:16,298 [trainer.py] => Trainable params: 85821726
2023-12-19 05:23:16,299 [slca.py] => Learning on 30-40
2023-12-19 05:23:18,371 [slca.py] => Task 3, Epoch 1/20 => Loss 2.386
2023-12-19 05:23:20,332 [slca.py] => Task 3, Epoch 2/20 => Loss 2.281
2023-12-19 05:23:22,180 [slca.py] => Task 3, Epoch 3/20 => Loss 2.263
2023-12-19 05:23:24,021 [slca.py] => Task 3, Epoch 4/20 => Loss 2.285
2023-12-19 05:23:36,870 [slca.py] => Task 3, Epoch 5/20 => Loss 1.933, Train_accy 36.880, Test_accy 21.270
2023-12-19 05:23:38,715 [slca.py] => Task 3, Epoch 6/20 => Loss 1.889
2023-12-19 05:23:40,578 [slca.py] => Task 3, Epoch 7/20 => Loss 1.813
2023-12-19 05:23:42,434 [slca.py] => Task 3, Epoch 8/20 => Loss 1.609
2023-12-19 05:23:44,330 [slca.py] => Task 3, Epoch 9/20 => Loss 1.566
2023-12-19 05:23:58,215 [slca.py] => Task 3, Epoch 10/20 => Loss 1.333, Train_accy 49.380, Test_accy 27.600
2023-12-19 05:24:00,187 [slca.py] => Task 3, Epoch 11/20 => Loss 1.241
2023-12-19 05:24:02,251 [slca.py] => Task 3, Epoch 12/20 => Loss 1.053
2023-12-19 05:24:04,112 [slca.py] => Task 3, Epoch 13/20 => Loss 0.979
2023-12-19 05:24:06,198 [slca.py] => Task 3, Epoch 14/20 => Loss 0.888
2023-12-19 05:24:20,043 [slca.py] => Task 3, Epoch 15/20 => Loss 0.926, Train_accy 49.380, Test_accy 30.440
2023-12-19 05:24:21,971 [slca.py] => Task 3, Epoch 16/20 => Loss 0.879
2023-12-19 05:24:23,941 [slca.py] => Task 3, Epoch 17/20 => Loss 0.806
2023-12-19 05:24:25,802 [slca.py] => Task 3, Epoch 18/20 => Loss 0.800
2023-12-19 05:24:27,655 [slca.py] => Task 3, Epoch 19/20 => Loss 0.566
2023-12-19 05:24:39,670 [slca.py] => Task 3, Epoch 20/20 => Loss 0.716, Train_accy 50.000, Test_accy 30.770
2023-12-19 05:24:53,680 [slca.py] => CA Task 3 => Loss 0.417, Test_accy 51.290
2023-12-19 05:25:02,872 [slca.py] => CA Task 3 => Loss 0.030, Test_accy 52.770
2023-12-19 05:25:12,084 [slca.py] => CA Task 3 => Loss 0.017, Test_accy 53.280
2023-12-19 05:25:21,429 [slca.py] => CA Task 3 => Loss 0.015, Test_accy 53.500
2023-12-19 05:25:30,990 [slca.py] => CA Task 3 => Loss 0.013, Test_accy 53.400
2023-12-19 05:25:39,976 [slca.py] => Exemplar size: 0
2023-12-19 05:25:40,389 [trainer.py] => No NME accuracy.
2023-12-19 05:25:40,390 [trainer.py] => CNN: {'total': 53.4, '00-09': 57.68, '10-19': 40.22, '20-29': 51.81, '30-39': 63.88, 'old': 49.9, 'new': 63.88}
2023-12-19 05:25:40,390 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69, 53.4]
2023-12-19 05:25:40,390 [trainer.py] => CNN top1 avg: 56.165
2023-12-19 05:25:40,390 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86, 82.23]

2023-12-19 05:25:40,390 [trainer.py] => All params: 85829416
2023-12-19 05:25:40,391 [trainer.py] => Trainable params: 85829416
2023-12-19 05:25:40,391 [slca.py] => Learning on 40-50
2023-12-19 05:25:42,535 [slca.py] => Task 4, Epoch 1/20 => Loss 2.361
2023-12-19 05:25:44,585 [slca.py] => Task 4, Epoch 2/20 => Loss 2.321
2023-12-19 05:25:46,688 [slca.py] => Task 4, Epoch 3/20 => Loss 2.317
2023-12-19 05:25:48,783 [slca.py] => Task 4, Epoch 4/20 => Loss 2.187
2023-12-19 05:26:03,265 [slca.py] => Task 4, Epoch 5/20 => Loss 1.948, Train_accy 39.000, Test_accy 24.580
2023-12-19 05:26:05,372 [slca.py] => Task 4, Epoch 6/20 => Loss 1.896
2023-12-19 05:26:07,463 [slca.py] => Task 4, Epoch 7/20 => Loss 1.692
2023-12-19 05:26:09,516 [slca.py] => Task 4, Epoch 8/20 => Loss 1.513
2023-12-19 05:26:11,580 [slca.py] => Task 4, Epoch 9/20 => Loss 1.361
2023-12-19 05:26:25,992 [slca.py] => Task 4, Epoch 10/20 => Loss 1.259, Train_accy 47.000, Test_accy 28.390
2023-12-19 05:26:28,062 [slca.py] => Task 4, Epoch 11/20 => Loss 1.176
2023-12-19 05:26:30,155 [slca.py] => Task 4, Epoch 12/20 => Loss 0.959
2023-12-19 05:26:32,237 [slca.py] => Task 4, Epoch 13/20 => Loss 0.965
2023-12-19 05:26:34,307 [slca.py] => Task 4, Epoch 14/20 => Loss 0.733
2023-12-19 05:26:50,569 [slca.py] => Task 4, Epoch 15/20 => Loss 0.788, Train_accy 54.000, Test_accy 30.790
2023-12-19 05:26:52,601 [slca.py] => Task 4, Epoch 16/20 => Loss 0.710
2023-12-19 05:26:54,680 [slca.py] => Task 4, Epoch 17/20 => Loss 0.545
2023-12-19 05:26:56,857 [slca.py] => Task 4, Epoch 18/20 => Loss 0.612
2023-12-19 05:26:58,994 [slca.py] => Task 4, Epoch 19/20 => Loss 0.459
2023-12-19 05:27:13,788 [slca.py] => Task 4, Epoch 20/20 => Loss 0.691, Train_accy 53.500, Test_accy 31.050
2023-12-19 05:27:30,729 [slca.py] => CA Task 4 => Loss 0.401, Test_accy 50.120
2023-12-19 05:27:42,177 [slca.py] => CA Task 4 => Loss 0.036, Test_accy 51.900
2023-12-19 05:27:53,600 [slca.py] => CA Task 4 => Loss 0.023, Test_accy 52.820
2023-12-19 05:28:05,027 [slca.py] => CA Task 4 => Loss 0.016, Test_accy 53.060
2023-12-19 05:28:16,435 [slca.py] => CA Task 4 => Loss 0.016, Test_accy 53.190
2023-12-19 05:28:27,525 [slca.py] => Exemplar size: 0
2023-12-19 05:28:27,925 [trainer.py] => No NME accuracy.
2023-12-19 05:28:27,925 [trainer.py] => CNN: {'total': 53.19, '00-09': 57.62, '10-19': 41.14, '20-29': 48.5, '30-39': 64.69, '40-49': 54.0, 'old': 52.98, 'new': 54.0}
2023-12-19 05:28:27,925 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69, 53.4, 53.19]
2023-12-19 05:28:27,925 [trainer.py] => CNN top1 avg: 55.57000000000001
2023-12-19 05:28:27,925 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86, 82.23, 81.27]

2023-12-19 05:28:27,926 [trainer.py] => All params: 85837106
2023-12-19 05:28:27,926 [trainer.py] => Trainable params: 85837106
2023-12-19 05:28:27,927 [slca.py] => Learning on 50-60
2023-12-19 05:28:30,243 [slca.py] => Task 5, Epoch 1/20 => Loss 2.471
2023-12-19 05:28:32,564 [slca.py] => Task 5, Epoch 2/20 => Loss 2.440
2023-12-19 05:28:34,836 [slca.py] => Task 5, Epoch 3/20 => Loss 2.331
2023-12-19 05:28:37,132 [slca.py] => Task 5, Epoch 4/20 => Loss 2.166
2023-12-19 05:28:53,784 [slca.py] => Task 5, Epoch 5/20 => Loss 1.890, Train_accy 42.920, Test_accy 26.040
2023-12-19 05:28:56,208 [slca.py] => Task 5, Epoch 6/20 => Loss 1.786
2023-12-19 05:28:58,466 [slca.py] => Task 5, Epoch 7/20 => Loss 1.662
2023-12-19 05:29:00,742 [slca.py] => Task 5, Epoch 8/20 => Loss 1.397
2023-12-19 05:29:03,029 [slca.py] => Task 5, Epoch 9/20 => Loss 1.225
2023-12-19 05:29:21,031 [slca.py] => Task 5, Epoch 10/20 => Loss 1.079, Train_accy 52.920, Test_accy 29.300
2023-12-19 05:29:23,377 [slca.py] => Task 5, Epoch 11/20 => Loss 0.964
2023-12-19 05:29:25,967 [slca.py] => Task 5, Epoch 12/20 => Loss 0.820
2023-12-19 05:29:28,449 [slca.py] => Task 5, Epoch 13/20 => Loss 0.718
2023-12-19 05:29:30,748 [slca.py] => Task 5, Epoch 14/20 => Loss 0.787
2023-12-19 05:29:47,774 [slca.py] => Task 5, Epoch 15/20 => Loss 0.668, Train_accy 54.580, Test_accy 32.610
2023-12-19 05:29:50,053 [slca.py] => Task 5, Epoch 16/20 => Loss 0.655
2023-12-19 05:29:52,323 [slca.py] => Task 5, Epoch 17/20 => Loss 0.508
2023-12-19 05:29:54,554 [slca.py] => Task 5, Epoch 18/20 => Loss 0.507
2023-12-19 05:29:57,334 [slca.py] => Task 5, Epoch 19/20 => Loss 0.488
2023-12-19 05:30:15,419 [slca.py] => Task 5, Epoch 20/20 => Loss 0.424, Train_accy 54.170, Test_accy 32.810
2023-12-19 05:30:33,596 [slca.py] => CA Task 5 => Loss 0.392, Test_accy 50.440
2023-12-19 05:30:46,963 [slca.py] => CA Task 5 => Loss 0.044, Test_accy 52.550
2023-12-19 05:31:00,371 [slca.py] => CA Task 5 => Loss 0.027, Test_accy 53.210
2023-12-19 05:31:14,449 [slca.py] => CA Task 5 => Loss 0.021, Test_accy 53.580
2023-12-19 05:31:31,137 [slca.py] => CA Task 5 => Loss 0.020, Test_accy 53.620
2023-12-19 05:31:45,273 [slca.py] => Exemplar size: 0
2023-12-19 05:31:45,682 [trainer.py] => No NME accuracy.
2023-12-19 05:31:45,682 [trainer.py] => CNN: {'total': 53.62, '00-09': 57.32, '10-19': 40.77, '20-29': 47.36, '30-39': 61.51, '40-49': 51.33, '50-59': 63.59, 'old': 51.64, 'new': 63.59}
2023-12-19 05:31:45,682 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69, 53.4, 53.19, 53.62]
2023-12-19 05:31:45,682 [trainer.py] => CNN top1 avg: 55.245000000000005
2023-12-19 05:31:45,682 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86, 82.23, 81.27, 81.03]

2023-12-19 05:31:45,682 [trainer.py] => All params: 85844796
2023-12-19 05:31:45,683 [trainer.py] => Trainable params: 85844796
2023-12-19 05:31:45,683 [slca.py] => Learning on 60-70
2023-12-19 05:31:48,291 [slca.py] => Task 6, Epoch 1/20 => Loss 2.307
2023-12-19 05:31:50,780 [slca.py] => Task 6, Epoch 2/20 => Loss 2.030
2023-12-19 05:31:53,355 [slca.py] => Task 6, Epoch 3/20 => Loss 1.929
2023-12-19 05:31:55,855 [slca.py] => Task 6, Epoch 4/20 => Loss 1.842
2023-12-19 05:32:14,802 [slca.py] => Task 6, Epoch 5/20 => Loss 1.617, Train_accy 46.070, Test_accy 29.690
2023-12-19 05:32:17,404 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 05:32:19,923 [slca.py] => Task 6, Epoch 7/20 => Loss 1.003
2023-12-19 05:32:22,443 [slca.py] => Task 6, Epoch 8/20 => Loss 0.796
2023-12-19 05:32:25,087 [slca.py] => Task 6, Epoch 9/20 => Loss 0.668
2023-12-19 05:32:44,633 [slca.py] => Task 6, Epoch 10/20 => Loss 0.563, Train_accy 55.710, Test_accy 34.640
2023-12-19 05:32:47,158 [slca.py] => Task 6, Epoch 11/20 => Loss 0.739
2023-12-19 05:32:49,664 [slca.py] => Task 6, Epoch 12/20 => Loss 0.476
2023-12-19 05:32:52,129 [slca.py] => Task 6, Epoch 13/20 => Loss 0.451
2023-12-19 05:32:54,671 [slca.py] => Task 6, Epoch 14/20 => Loss 0.399
2023-12-19 05:33:14,374 [slca.py] => Task 6, Epoch 15/20 => Loss 0.339, Train_accy 51.430, Test_accy 35.080
2023-12-19 05:33:16,919 [slca.py] => Task 6, Epoch 16/20 => Loss 0.506
2023-12-19 05:33:19,448 [slca.py] => Task 6, Epoch 17/20 => Loss 0.297
2023-12-19 05:33:22,053 [slca.py] => Task 6, Epoch 18/20 => Loss 0.384
2023-12-19 05:33:24,549 [slca.py] => Task 6, Epoch 19/20 => Loss 0.178
2023-12-19 05:33:43,609 [slca.py] => Task 6, Epoch 20/20 => Loss 0.273, Train_accy 55.000, Test_accy 35.010
2023-12-19 05:34:04,750 [slca.py] => CA Task 6 => Loss 0.374, Test_accy 51.530
2023-12-19 05:34:20,623 [slca.py] => CA Task 6 => Loss 0.051, Test_accy 53.230
2023-12-19 05:34:36,680 [slca.py] => CA Task 6 => Loss 0.029, Test_accy 53.990
2023-12-19 05:34:52,564 [slca.py] => CA Task 6 => Loss 0.024, Test_accy 54.370
2023-12-19 05:35:08,629 [slca.py] => CA Task 6 => Loss 0.021, Test_accy 54.440
2023-12-19 05:35:23,620 [slca.py] => Exemplar size: 0
2023-12-19 05:35:24,028 [trainer.py] => No NME accuracy.
2023-12-19 05:35:24,028 [trainer.py] => CNN: {'total': 54.44, '00-09': 54.3, '10-19': 38.6, '20-29': 45.13, '30-39': 59.0, '40-49': 52.78, '50-59': 64.85, '60-69': 66.4, 'old': 52.45, 'new': 66.4}
2023-12-19 05:35:24,028 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69, 53.4, 53.19, 53.62, 54.44]
2023-12-19 05:35:24,028 [trainer.py] => CNN top1 avg: 55.13
2023-12-19 05:35:24,028 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86, 82.23, 81.27, 81.03, 81.42]

2023-12-19 05:35:24,029 [trainer.py] => All params: 85852486
2023-12-19 05:35:24,029 [trainer.py] => Trainable params: 85852486
2023-12-19 05:35:24,030 [slca.py] => Learning on 70-80
2023-12-19 05:35:26,876 [slca.py] => Task 7, Epoch 1/20 => Loss 2.511
2023-12-19 05:35:29,553 [slca.py] => Task 7, Epoch 2/20 => Loss 2.332
2023-12-19 05:35:32,258 [slca.py] => Task 7, Epoch 3/20 => Loss 2.140
2023-12-19 05:35:34,958 [slca.py] => Task 7, Epoch 4/20 => Loss 1.865
2023-12-19 05:35:56,181 [slca.py] => Task 7, Epoch 5/20 => Loss 1.563, Train_accy 46.250, Test_accy 31.630
2023-12-19 05:35:58,893 [slca.py] => Task 7, Epoch 6/20 => Loss 1.354
2023-12-19 05:36:01,581 [slca.py] => Task 7, Epoch 7/20 => Loss 1.105
2023-12-19 05:36:04,298 [slca.py] => Task 7, Epoch 8/20 => Loss 0.987
2023-12-19 05:36:06,980 [slca.py] => Task 7, Epoch 9/20 => Loss 0.787
2023-12-19 05:36:28,394 [slca.py] => Task 7, Epoch 10/20 => Loss 0.664, Train_accy 54.380, Test_accy 35.480
2023-12-19 05:36:31,125 [slca.py] => Task 7, Epoch 11/20 => Loss 0.691
2023-12-19 05:36:33,900 [slca.py] => Task 7, Epoch 12/20 => Loss 0.487
2023-12-19 05:36:36,623 [slca.py] => Task 7, Epoch 13/20 => Loss 0.436
2023-12-19 05:36:39,337 [slca.py] => Task 7, Epoch 14/20 => Loss 0.405
2023-12-19 05:37:00,672 [slca.py] => Task 7, Epoch 15/20 => Loss 0.347, Train_accy 53.440, Test_accy 36.530
2023-12-19 05:37:03,371 [slca.py] => Task 7, Epoch 16/20 => Loss 0.416
2023-12-19 05:37:06,073 [slca.py] => Task 7, Epoch 17/20 => Loss 0.289
2023-12-19 05:37:08,741 [slca.py] => Task 7, Epoch 18/20 => Loss 0.280
2023-12-19 05:37:11,427 [slca.py] => Task 7, Epoch 19/20 => Loss 0.272
2023-12-19 05:37:34,213 [slca.py] => Task 7, Epoch 20/20 => Loss 0.197, Train_accy 52.810, Test_accy 37.120
2023-12-19 05:37:56,990 [slca.py] => CA Task 7 => Loss 0.368, Test_accy 51.640
2023-12-19 05:38:14,975 [slca.py] => CA Task 7 => Loss 0.054, Test_accy 53.720
2023-12-19 05:38:32,937 [slca.py] => CA Task 7 => Loss 0.032, Test_accy 54.510
2023-12-19 05:38:50,952 [slca.py] => CA Task 7 => Loss 0.027, Test_accy 54.880
2023-12-19 05:39:10,260 [slca.py] => CA Task 7 => Loss 0.023, Test_accy 54.980
2023-12-19 05:39:27,304 [slca.py] => Exemplar size: 0
2023-12-19 05:39:27,714 [trainer.py] => No NME accuracy.
2023-12-19 05:39:27,715 [trainer.py] => CNN: {'total': 54.98, '00-09': 51.36, '10-19': 37.84, '20-29': 42.99, '30-39': 58.16, '40-49': 53.32, '50-59': 64.17, '60-69': 64.32, '70-79': 67.74, 'old': 53.15, 'new': 67.74}
2023-12-19 05:39:27,715 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69, 53.4, 53.19, 53.62, 54.44, 54.98]
2023-12-19 05:39:27,715 [trainer.py] => CNN top1 avg: 55.11125
2023-12-19 05:39:27,715 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86, 82.23, 81.27, 81.03, 81.42, 81.01]

2023-12-19 05:39:27,715 [trainer.py] => All params: 85860176
2023-12-19 05:39:27,716 [trainer.py] => Trainable params: 85860176
2023-12-19 05:39:27,716 [slca.py] => Learning on 80-90
2023-12-19 05:39:30,749 [slca.py] => Task 8, Epoch 1/20 => Loss 2.461
2023-12-19 05:39:33,651 [slca.py] => Task 8, Epoch 2/20 => Loss 2.310
2023-12-19 05:39:36,607 [slca.py] => Task 8, Epoch 3/20 => Loss 2.128
2023-12-19 05:39:39,592 [slca.py] => Task 8, Epoch 4/20 => Loss 1.783
2023-12-19 05:40:03,591 [slca.py] => Task 8, Epoch 5/20 => Loss 1.530, Train_accy 52.500, Test_accy 34.130
2023-12-19 05:40:06,575 [slca.py] => Task 8, Epoch 6/20 => Loss 1.200
2023-12-19 05:40:09,472 [slca.py] => Task 8, Epoch 7/20 => Loss 0.958
2023-12-19 05:40:12,790 [slca.py] => Task 8, Epoch 8/20 => Loss 0.748
2023-12-19 05:40:15,721 [slca.py] => Task 8, Epoch 9/20 => Loss 0.549
2023-12-19 05:40:41,109 [slca.py] => Task 8, Epoch 10/20 => Loss 0.538, Train_accy 56.940, Test_accy 38.020
2023-12-19 05:40:44,014 [slca.py] => Task 8, Epoch 11/20 => Loss 0.358
2023-12-19 05:40:46,936 [slca.py] => Task 8, Epoch 12/20 => Loss 0.337
2023-12-19 05:40:49,851 [slca.py] => Task 8, Epoch 13/20 => Loss 0.272
2023-12-19 05:40:52,760 [slca.py] => Task 8, Epoch 14/20 => Loss 0.304
2023-12-19 05:41:16,398 [slca.py] => Task 8, Epoch 15/20 => Loss 0.205, Train_accy 56.670, Test_accy 38.210
2023-12-19 05:41:19,454 [slca.py] => Task 8, Epoch 16/20 => Loss 0.244
2023-12-19 05:41:22,385 [slca.py] => Task 8, Epoch 17/20 => Loss 0.138
2023-12-19 05:41:25,325 [slca.py] => Task 8, Epoch 18/20 => Loss 0.234
2023-12-19 05:41:28,328 [slca.py] => Task 8, Epoch 19/20 => Loss 0.194
2023-12-19 05:41:52,282 [slca.py] => Task 8, Epoch 20/20 => Loss 0.205, Train_accy 55.560, Test_accy 38.070
2023-12-19 05:42:17,747 [slca.py] => CA Task 8 => Loss 0.354, Test_accy 52.120
2023-12-19 05:42:37,950 [slca.py] => CA Task 8 => Loss 0.056, Test_accy 54.190
2023-12-19 05:42:58,187 [slca.py] => CA Task 8 => Loss 0.033, Test_accy 54.990
2023-12-19 05:43:18,606 [slca.py] => CA Task 8 => Loss 0.026, Test_accy 55.100
2023-12-19 05:43:39,167 [slca.py] => CA Task 8 => Loss 0.024, Test_accy 55.170
2023-12-19 05:43:58,291 [slca.py] => Exemplar size: 0
2023-12-19 05:43:58,695 [trainer.py] => No NME accuracy.
2023-12-19 05:43:58,695 [trainer.py] => CNN: {'total': 55.17, '00-09': 48.14, '10-19': 34.74, '20-29': 41.52, '30-39': 57.33, '40-49': 50.4, '50-59': 64.33, '60-69': 64.93, '70-79': 67.98, '80-89': 67.14, 'old': 53.67, 'new': 67.14}
2023-12-19 05:43:58,695 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69, 53.4, 53.19, 53.62, 54.44, 54.98, 55.17]
2023-12-19 05:43:58,695 [trainer.py] => CNN top1 avg: 55.117777777777775
2023-12-19 05:43:58,695 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86, 82.23, 81.27, 81.03, 81.42, 81.01, 81.62]

2023-12-19 05:43:58,696 [trainer.py] => All params: 85867866
2023-12-19 05:43:58,696 [trainer.py] => Trainable params: 85867866
2023-12-19 05:43:58,697 [slca.py] => Learning on 90-100
2023-12-19 05:44:01,984 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 05:44:05,131 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 05:44:08,287 [slca.py] => Task 9, Epoch 3/20 => Loss 2.023
2023-12-19 05:44:11,415 [slca.py] => Task 9, Epoch 4/20 => Loss 1.421
2023-12-19 05:44:38,058 [slca.py] => Task 9, Epoch 5/20 => Loss 1.064, Train_accy 51.250, Test_accy 34.640
2023-12-19 05:44:41,272 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 05:44:44,429 [slca.py] => Task 9, Epoch 7/20 => Loss 0.945
2023-12-19 05:44:47,560 [slca.py] => Task 9, Epoch 8/20 => Loss 0.761
2023-12-19 05:44:50,715 [slca.py] => Task 9, Epoch 9/20 => Loss 0.374
2023-12-19 05:45:16,991 [slca.py] => Task 9, Epoch 10/20 => Loss 0.441, Train_accy 55.000, Test_accy 36.930
2023-12-19 05:45:20,121 [slca.py] => Task 9, Epoch 11/20 => Loss 0.303
2023-12-19 05:45:23,375 [slca.py] => Task 9, Epoch 12/20 => Loss 0.299
2023-12-19 05:45:26,554 [slca.py] => Task 9, Epoch 13/20 => Loss 0.189
2023-12-19 05:45:29,758 [slca.py] => Task 9, Epoch 14/20 => Loss 0.235
2023-12-19 05:45:55,812 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 55.500, Test_accy 36.760
2023-12-19 05:45:58,997 [slca.py] => Task 9, Epoch 16/20 => Loss 0.268
2023-12-19 05:46:02,198 [slca.py] => Task 9, Epoch 17/20 => Loss 0.184
2023-12-19 05:46:05,336 [slca.py] => Task 9, Epoch 18/20 => Loss 0.104
2023-12-19 05:46:08,557 [slca.py] => Task 9, Epoch 19/20 => Loss 0.172
2023-12-19 05:46:37,919 [slca.py] => Task 9, Epoch 20/20 => Loss 0.156, Train_accy 53.250, Test_accy 36.790
2023-12-19 05:47:05,213 [slca.py] => CA Task 9 => Loss 0.356, Test_accy 50.760
2023-12-19 05:47:27,773 [slca.py] => CA Task 9 => Loss 0.059, Test_accy 52.750
2023-12-19 05:47:50,527 [slca.py] => CA Task 9 => Loss 0.038, Test_accy 53.570
2023-12-19 05:48:13,239 [slca.py] => CA Task 9 => Loss 0.028, Test_accy 53.910
2023-12-19 05:48:35,739 [slca.py] => CA Task 9 => Loss 0.027, Test_accy 53.960
2023-12-19 05:48:57,143 [slca.py] => Exemplar size: 0
2023-12-19 05:48:57,544 [trainer.py] => No NME accuracy.
2023-12-19 05:48:57,544 [trainer.py] => CNN: {'total': 53.96, '00-09': 40.34, '10-19': 32.43, '20-29': 40.94, '30-39': 50.7, '40-49': 48.6, '50-59': 62.02, '60-69': 62.86, '70-79': 68.91, '80-89': 67.43, '90-99': 65.4, 'old': 52.69, 'new': 65.4}
2023-12-19 05:48:57,544 [trainer.py] => CNN top1 curve: [68.08, 53.49, 49.69, 53.4, 53.19, 53.62, 54.44, 54.98, 55.17, 53.96]
2023-12-19 05:48:57,544 [trainer.py] => CNN top1 avg: 55.001999999999995
2023-12-19 05:48:57,544 [trainer.py] => CNN top5 curve: [95.65, 86.25, 81.86, 82.23, 81.27, 81.03, 81.42, 81.01, 81.62, 81.69]

2023-12-19 05:48:57,545 [trainer.py] => final accs: [53.96]
2023-12-19 05:48:57,545 [trainer.py] => avg accs: [55.001999999999995]
