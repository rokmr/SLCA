2023-12-19 11:43:04,298 [trainer.py] => config: exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 11:43:04,298 [trainer.py] => test_only: False
2023-12-19 11:43:04,298 [trainer.py] => prefix: reproduce
2023-12-19 11:43:04,298 [trainer.py] => dataset: cifar100_224
2023-12-19 11:43:04,298 [trainer.py] => memory_size: 0
2023-12-19 11:43:04,298 [trainer.py] => memory_per_class: 0
2023-12-19 11:43:04,298 [trainer.py] => fixed_memory: False
2023-12-19 11:43:04,298 [trainer.py] => shuffle: False
2023-12-19 11:43:04,298 [trainer.py] => init_cls: 10
2023-12-19 11:43:04,298 [trainer.py] => increment: 10
2023-12-19 11:43:04,298 [trainer.py] => model_name: slca_cifar
2023-12-19 11:43:04,298 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 11:43:04,298 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 11:43:04,298 [trainer.py] => device: [device(type='cuda', index=0)]
2023-12-19 11:43:04,298 [trainer.py] => seed: 0
2023-12-19 11:43:04,298 [trainer.py] => epochs: 20
2023-12-19 11:43:04,298 [trainer.py] => ca_epochs: 5
2023-12-19 11:43:04,298 [trainer.py] => ca_with_logit_norm: 0.5
2023-12-19 11:43:04,298 [trainer.py] => milestones: [18]
2023-12-19 11:43:04,298 [trainer.py] => lr: 0.005
2023-12-19 11:43:04,298 [trainer.py] => lr_decay: 0.1
2023-12-19 11:43:04,298 [trainer.py] => weight_decay: 0.005
2023-12-19 11:43:04,298 [trainer.py] => u_batch_size: 256
2023-12-19 11:43:04,298 [trainer.py] => s_batch_size: 3
2023-12-19 11:43:04,299 [trainer.py] => multicrop: 2
2023-12-19 11:43:04,299 [trainer.py] => us_multicrop: 2
2023-12-19 11:43:04,299 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 11:43:04,299 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 11:43:04,299 [trainer.py] => buffer_size: 500
2023-12-19 11:43:04,299 [trainer.py] => run_id: 0
2023-12-19 11:43:05,887 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 11:43:07,811 [trainer.py] => All params: 85798656
2023-12-19 11:43:07,812 [trainer.py] => Trainable params: 85798656
2023-12-19 11:43:07,813 [slca.py] => Learning on 0-10
2023-12-19 11:43:14,459 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 11:43:15,566 [slca.py] => Task 0, Epoch 2/20 => Loss 2.382
2023-12-19 11:43:16,666 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 11:43:17,798 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 11:43:23,038 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 39.960
2023-12-19 11:43:24,175 [slca.py] => Task 0, Epoch 6/20 => Loss 1.699
2023-12-19 11:43:25,304 [slca.py] => Task 0, Epoch 7/20 => Loss 1.405
2023-12-19 11:43:26,479 [slca.py] => Task 0, Epoch 8/20 => Loss 1.355
2023-12-19 11:43:27,619 [slca.py] => Task 0, Epoch 9/20 => Loss 1.153
2023-12-19 11:43:32,647 [slca.py] => Task 0, Epoch 10/20 => Loss 0.953, Train_accy 92.500, Test_accy 69.750
2023-12-19 11:43:33,751 [slca.py] => Task 0, Epoch 11/20 => Loss 0.775
2023-12-19 11:43:34,889 [slca.py] => Task 0, Epoch 12/20 => Loss 0.800
2023-12-19 11:43:36,023 [slca.py] => Task 0, Epoch 13/20 => Loss 0.570
2023-12-19 11:43:37,221 [slca.py] => Task 0, Epoch 14/20 => Loss 0.456
2023-12-19 11:43:42,395 [slca.py] => Task 0, Epoch 15/20 => Loss 0.483, Train_accy 95.000, Test_accy 81.360
2023-12-19 11:43:43,604 [slca.py] => Task 0, Epoch 16/20 => Loss 0.325
2023-12-19 11:43:44,669 [slca.py] => Task 0, Epoch 17/20 => Loss 0.352
2023-12-19 11:43:45,744 [slca.py] => Task 0, Epoch 18/20 => Loss 0.292
2023-12-19 11:43:46,926 [slca.py] => Task 0, Epoch 19/20 => Loss 0.214
2023-12-19 11:43:52,004 [slca.py] => Task 0, Epoch 20/20 => Loss 0.297, Train_accy 97.500, Test_accy 84.600
2023-12-19 11:43:59,750 [slca.py] => CA Task 0 => Loss 0.927, Test_accy 84.820
2023-12-19 11:44:02,790 [slca.py] => CA Task 0 => Loss 0.848, Test_accy 86.610
2023-12-19 11:44:05,817 [slca.py] => CA Task 0 => Loss 0.791, Test_accy 89.060
2023-12-19 11:44:08,860 [slca.py] => CA Task 0 => Loss 0.773, Test_accy 89.170
2023-12-19 11:44:11,815 [slca.py] => CA Task 0 => Loss 0.767, Test_accy 89.290
2023-12-19 11:44:14,819 [slca.py] => Exemplar size: 0
2023-12-19 11:44:15,539 [trainer.py] => No NME accuracy.
2023-12-19 11:44:15,539 [trainer.py] => CNN: {'total': 89.29, '00-09': 89.29, 'old': 0, 'new': 89.29}
2023-12-19 11:44:15,539 [trainer.py] => CNN top1 curve: [89.29]
2023-12-19 11:44:15,540 [trainer.py] => CNN top1 avg: 89.29
2023-12-19 11:44:15,540 [trainer.py] => CNN top5 curve: [99.33]

2023-12-19 11:44:15,540 [trainer.py] => All params: 85806346
2023-12-19 11:44:15,540 [trainer.py] => Trainable params: 85806346
2023-12-19 11:44:15,541 [slca.py] => Learning on 10-20
2023-12-19 11:44:17,083 [slca.py] => Task 1, Epoch 1/20 => Loss 2.537
2023-12-19 11:44:18,533 [slca.py] => Task 1, Epoch 2/20 => Loss 2.492
2023-12-19 11:44:20,030 [slca.py] => Task 1, Epoch 3/20 => Loss 2.329
2023-12-19 11:44:21,576 [slca.py] => Task 1, Epoch 4/20 => Loss 2.190
2023-12-19 11:44:29,198 [slca.py] => Task 1, Epoch 5/20 => Loss 1.933, Train_accy 53.750, Test_accy 43.070
2023-12-19 11:44:30,701 [slca.py] => Task 1, Epoch 6/20 => Loss 1.619
2023-12-19 11:44:32,153 [slca.py] => Task 1, Epoch 7/20 => Loss 1.501
2023-12-19 11:44:33,757 [slca.py] => Task 1, Epoch 8/20 => Loss 1.269
2023-12-19 11:44:35,317 [slca.py] => Task 1, Epoch 9/20 => Loss 1.043
2023-12-19 11:44:42,955 [slca.py] => Task 1, Epoch 10/20 => Loss 0.808, Train_accy 81.250, Test_accy 56.560
2023-12-19 11:44:44,508 [slca.py] => Task 1, Epoch 11/20 => Loss 0.744
2023-12-19 11:44:45,995 [slca.py] => Task 1, Epoch 12/20 => Loss 0.606
2023-12-19 11:44:47,489 [slca.py] => Task 1, Epoch 13/20 => Loss 0.421
2023-12-19 11:44:49,023 [slca.py] => Task 1, Epoch 14/20 => Loss 0.460
2023-12-19 11:44:56,766 [slca.py] => Task 1, Epoch 15/20 => Loss 0.390, Train_accy 96.250, Test_accy 67.920
2023-12-19 11:44:58,221 [slca.py] => Task 1, Epoch 16/20 => Loss 0.250
2023-12-19 11:44:59,803 [slca.py] => Task 1, Epoch 17/20 => Loss 0.273
2023-12-19 11:45:01,346 [slca.py] => Task 1, Epoch 18/20 => Loss 0.245
2023-12-19 11:45:02,801 [slca.py] => Task 1, Epoch 19/20 => Loss 0.131
2023-12-19 11:45:10,496 [slca.py] => Task 1, Epoch 20/20 => Loss 0.215, Train_accy 92.500, Test_accy 70.830
2023-12-19 11:45:20,992 [slca.py] => CA Task 1 => Loss 1.021, Test_accy 74.110
2023-12-19 11:45:26,042 [slca.py] => CA Task 1 => Loss 0.618, Test_accy 77.400
2023-12-19 11:45:31,053 [slca.py] => CA Task 1 => Loss 0.435, Test_accy 78.070
2023-12-19 11:45:36,301 [slca.py] => CA Task 1 => Loss 0.361, Test_accy 78.020
2023-12-19 11:45:41,364 [slca.py] => CA Task 1 => Loss 0.339, Test_accy 77.760
2023-12-19 11:45:46,474 [slca.py] => Exemplar size: 0
2023-12-19 11:45:47,253 [trainer.py] => No NME accuracy.
2023-12-19 11:45:47,253 [trainer.py] => CNN: {'total': 77.76, '00-09': 81.77, '10-19': 73.75, 'old': 81.77, 'new': 73.75}
2023-12-19 11:45:47,253 [trainer.py] => CNN top1 curve: [89.29, 77.76]
2023-12-19 11:45:47,253 [trainer.py] => CNN top1 avg: 83.525
2023-12-19 11:45:47,253 [trainer.py] => CNN top5 curve: [99.33, 96.3]

2023-12-19 11:45:47,254 [trainer.py] => All params: 85814036
2023-12-19 11:45:47,254 [trainer.py] => Trainable params: 85814036
2023-12-19 11:45:47,255 [slca.py] => Learning on 20-30
2023-12-19 11:45:49,205 [slca.py] => Task 2, Epoch 1/20 => Loss 2.432
2023-12-19 11:45:51,075 [slca.py] => Task 2, Epoch 2/20 => Loss 2.285
2023-12-19 11:45:52,897 [slca.py] => Task 2, Epoch 3/20 => Loss 2.013
2023-12-19 11:45:54,775 [slca.py] => Task 2, Epoch 4/20 => Loss 1.917
2023-12-19 11:46:05,127 [slca.py] => Task 2, Epoch 5/20 => Loss 1.659, Train_accy 61.670, Test_accy 48.060
2023-12-19 11:46:06,994 [slca.py] => Task 2, Epoch 6/20 => Loss 1.483
2023-12-19 11:46:08,837 [slca.py] => Task 2, Epoch 7/20 => Loss 1.134
2023-12-19 11:46:10,733 [slca.py] => Task 2, Epoch 8/20 => Loss 0.911
2023-12-19 11:46:12,528 [slca.py] => Task 2, Epoch 9/20 => Loss 0.742
2023-12-19 11:46:22,993 [slca.py] => Task 2, Epoch 10/20 => Loss 0.714, Train_accy 81.670, Test_accy 60.050
2023-12-19 11:46:24,850 [slca.py] => Task 2, Epoch 11/20 => Loss 0.476
2023-12-19 11:46:26,671 [slca.py] => Task 2, Epoch 12/20 => Loss 0.291
2023-12-19 11:46:28,536 [slca.py] => Task 2, Epoch 13/20 => Loss 0.227
2023-12-19 11:46:30,301 [slca.py] => Task 2, Epoch 14/20 => Loss 0.301
2023-12-19 11:46:40,702 [slca.py] => Task 2, Epoch 15/20 => Loss 0.300, Train_accy 90.830, Test_accy 68.040
2023-12-19 11:46:42,536 [slca.py] => Task 2, Epoch 16/20 => Loss 0.204
2023-12-19 11:46:44,353 [slca.py] => Task 2, Epoch 17/20 => Loss 0.334
2023-12-19 11:46:46,206 [slca.py] => Task 2, Epoch 18/20 => Loss 0.195
2023-12-19 11:46:47,981 [slca.py] => Task 2, Epoch 19/20 => Loss 0.179
2023-12-19 11:46:58,291 [slca.py] => Task 2, Epoch 20/20 => Loss 0.127, Train_accy 90.830, Test_accy 69.090
2023-12-19 11:47:10,818 [slca.py] => CA Task 2 => Loss 1.034, Test_accy 75.610
2023-12-19 11:47:18,174 [slca.py] => CA Task 2 => Loss 0.451, Test_accy 78.460
2023-12-19 11:47:25,388 [slca.py] => CA Task 2 => Loss 0.236, Test_accy 78.400
2023-12-19 11:47:32,514 [slca.py] => CA Task 2 => Loss 0.165, Test_accy 78.230
2023-12-19 11:47:39,793 [slca.py] => CA Task 2 => Loss 0.143, Test_accy 78.190
2023-12-19 11:47:46,807 [slca.py] => Exemplar size: 0
2023-12-19 11:47:47,537 [trainer.py] => No NME accuracy.
2023-12-19 11:47:47,537 [trainer.py] => CNN: {'total': 78.19, '00-09': 78.63, '10-19': 71.82, '20-29': 84.13, 'old': 75.22, 'new': 84.13}
2023-12-19 11:47:47,537 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19]
2023-12-19 11:47:47,537 [trainer.py] => CNN top1 avg: 81.74666666666667
2023-12-19 11:47:47,537 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6]

2023-12-19 11:47:47,538 [trainer.py] => All params: 85821726
2023-12-19 11:47:47,538 [trainer.py] => Trainable params: 85821726
2023-12-19 11:47:47,539 [slca.py] => Learning on 30-40
2023-12-19 11:47:49,786 [slca.py] => Task 3, Epoch 1/20 => Loss 2.328
2023-12-19 11:47:51,782 [slca.py] => Task 3, Epoch 2/20 => Loss 2.067
2023-12-19 11:47:53,845 [slca.py] => Task 3, Epoch 3/20 => Loss 1.681
2023-12-19 11:47:55,884 [slca.py] => Task 3, Epoch 4/20 => Loss 1.381
2023-12-19 11:48:08,513 [slca.py] => Task 3, Epoch 5/20 => Loss 0.856, Train_accy 79.380, Test_accy 59.880
2023-12-19 11:48:10,648 [slca.py] => Task 3, Epoch 6/20 => Loss 0.748
2023-12-19 11:48:12,666 [slca.py] => Task 3, Epoch 7/20 => Loss 0.425
2023-12-19 11:48:14,785 [slca.py] => Task 3, Epoch 8/20 => Loss 0.192
2023-12-19 11:48:16,927 [slca.py] => Task 3, Epoch 9/20 => Loss 0.337
2023-12-19 11:48:29,570 [slca.py] => Task 3, Epoch 10/20 => Loss 0.177, Train_accy 81.250, Test_accy 68.670
2023-12-19 11:48:31,754 [slca.py] => Task 3, Epoch 11/20 => Loss 0.239
2023-12-19 11:48:33,763 [slca.py] => Task 3, Epoch 12/20 => Loss 0.114
2023-12-19 11:48:35,796 [slca.py] => Task 3, Epoch 13/20 => Loss 0.067
2023-12-19 11:48:37,878 [slca.py] => Task 3, Epoch 14/20 => Loss 0.069
2023-12-19 11:48:50,426 [slca.py] => Task 3, Epoch 15/20 => Loss 0.119, Train_accy 85.620, Test_accy 69.080
2023-12-19 11:48:52,498 [slca.py] => Task 3, Epoch 16/20 => Loss 0.117
2023-12-19 11:48:54,528 [slca.py] => Task 3, Epoch 17/20 => Loss 0.080
2023-12-19 11:48:56,650 [slca.py] => Task 3, Epoch 18/20 => Loss 0.147
2023-12-19 11:48:58,722 [slca.py] => Task 3, Epoch 19/20 => Loss 0.051
2023-12-19 11:49:11,440 [slca.py] => Task 3, Epoch 20/20 => Loss 0.036, Train_accy 86.880, Test_accy 68.220
2023-12-19 11:49:26,798 [slca.py] => CA Task 3 => Loss 1.068, Test_accy 75.810
2023-12-19 11:49:36,022 [slca.py] => CA Task 3 => Loss 0.432, Test_accy 77.770
2023-12-19 11:49:45,295 [slca.py] => CA Task 3 => Loss 0.219, Test_accy 77.850
2023-12-19 11:49:54,534 [slca.py] => CA Task 3 => Loss 0.150, Test_accy 77.920
2023-12-19 11:50:03,769 [slca.py] => CA Task 3 => Loss 0.127, Test_accy 77.900
2023-12-19 11:50:12,882 [slca.py] => Exemplar size: 0
2023-12-19 11:50:13,620 [trainer.py] => No NME accuracy.
2023-12-19 11:50:13,620 [trainer.py] => CNN: {'total': 77.9, '00-09': 69.39, '10-19': 67.34, '20-29': 82.76, '30-39': 92.05, 'old': 73.17, 'new': 92.05}
2023-12-19 11:50:13,620 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19, 77.9]
2023-12-19 11:50:13,620 [trainer.py] => CNN top1 avg: 80.785
2023-12-19 11:50:13,620 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6, 94.08]

2023-12-19 11:50:13,621 [trainer.py] => All params: 85829416
2023-12-19 11:50:13,621 [trainer.py] => Trainable params: 85829416
2023-12-19 11:50:13,621 [slca.py] => Learning on 40-50
2023-12-19 11:50:15,864 [slca.py] => Task 4, Epoch 1/20 => Loss 2.302
2023-12-19 11:50:18,049 [slca.py] => Task 4, Epoch 2/20 => Loss 1.964
2023-12-19 11:50:20,246 [slca.py] => Task 4, Epoch 3/20 => Loss 1.456
2023-12-19 11:50:22,480 [slca.py] => Task 4, Epoch 4/20 => Loss 0.965
2023-12-19 11:50:37,380 [slca.py] => Task 4, Epoch 5/20 => Loss 0.498, Train_accy 79.500, Test_accy 59.580
2023-12-19 11:50:39,684 [slca.py] => Task 4, Epoch 6/20 => Loss 0.381
2023-12-19 11:50:41,875 [slca.py] => Task 4, Epoch 7/20 => Loss 0.185
2023-12-19 11:50:44,171 [slca.py] => Task 4, Epoch 8/20 => Loss 0.332
2023-12-19 11:50:46,479 [slca.py] => Task 4, Epoch 9/20 => Loss 0.142
2023-12-19 11:51:01,301 [slca.py] => Task 4, Epoch 10/20 => Loss 0.180, Train_accy 82.000, Test_accy 65.020
2023-12-19 11:51:03,709 [slca.py] => Task 4, Epoch 11/20 => Loss 0.182
2023-12-19 11:51:05,925 [slca.py] => Task 4, Epoch 12/20 => Loss 0.081
2023-12-19 11:51:08,128 [slca.py] => Task 4, Epoch 13/20 => Loss 0.112
2023-12-19 11:51:10,347 [slca.py] => Task 4, Epoch 14/20 => Loss 0.026
2023-12-19 11:51:25,131 [slca.py] => Task 4, Epoch 15/20 => Loss 0.120, Train_accy 81.500, Test_accy 63.580
2023-12-19 11:51:27,413 [slca.py] => Task 4, Epoch 16/20 => Loss 0.108
2023-12-19 11:51:29,711 [slca.py] => Task 4, Epoch 17/20 => Loss 0.078
2023-12-19 11:51:31,918 [slca.py] => Task 4, Epoch 18/20 => Loss 0.095
2023-12-19 11:51:34,191 [slca.py] => Task 4, Epoch 19/20 => Loss 0.025
2023-12-19 11:51:49,042 [slca.py] => Task 4, Epoch 20/20 => Loss 0.128, Train_accy 82.000, Test_accy 62.680
2023-12-19 11:52:06,188 [slca.py] => CA Task 4 => Loss 1.059, Test_accy 72.180
2023-12-19 11:52:17,620 [slca.py] => CA Task 4 => Loss 0.390, Test_accy 75.220
2023-12-19 11:52:28,896 [slca.py] => CA Task 4 => Loss 0.197, Test_accy 75.580
2023-12-19 11:52:40,593 [slca.py] => CA Task 4 => Loss 0.135, Test_accy 75.540
2023-12-19 11:52:52,132 [slca.py] => CA Task 4 => Loss 0.117, Test_accy 75.520
2023-12-19 11:53:03,356 [slca.py] => Exemplar size: 0
2023-12-19 11:53:04,085 [trainer.py] => No NME accuracy.
2023-12-19 11:53:04,085 [trainer.py] => CNN: {'total': 75.52, '00-09': 68.84, '10-19': 64.56, '20-29': 75.05, '30-39': 86.96, '40-49': 82.2, 'old': 73.85, 'new': 82.2}
2023-12-19 11:53:04,085 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19, 77.9, 75.52]
2023-12-19 11:53:04,086 [trainer.py] => CNN top1 avg: 79.732
2023-12-19 11:53:04,086 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6, 94.08, 93.49]

2023-12-19 11:53:04,086 [trainer.py] => All params: 85837106
2023-12-19 11:53:04,087 [trainer.py] => Trainable params: 85837106
2023-12-19 11:53:04,087 [slca.py] => Learning on 50-60
2023-12-19 11:53:06,624 [slca.py] => Task 5, Epoch 1/20 => Loss 2.435
2023-12-19 11:53:09,095 [slca.py] => Task 5, Epoch 2/20 => Loss 2.053
2023-12-19 11:53:11,579 [slca.py] => Task 5, Epoch 3/20 => Loss 1.432
2023-12-19 11:53:13,972 [slca.py] => Task 5, Epoch 4/20 => Loss 0.986
2023-12-19 11:53:30,960 [slca.py] => Task 5, Epoch 5/20 => Loss 0.540, Train_accy 73.330, Test_accy 56.130
2023-12-19 11:53:33,455 [slca.py] => Task 5, Epoch 6/20 => Loss 0.350
2023-12-19 11:53:35,877 [slca.py] => Task 5, Epoch 7/20 => Loss 0.234
2023-12-19 11:53:38,299 [slca.py] => Task 5, Epoch 8/20 => Loss 0.137
2023-12-19 11:53:40,808 [slca.py] => Task 5, Epoch 9/20 => Loss 0.278
2023-12-19 11:53:57,786 [slca.py] => Task 5, Epoch 10/20 => Loss 0.110, Train_accy 84.170, Test_accy 60.840
2023-12-19 11:54:00,263 [slca.py] => Task 5, Epoch 11/20 => Loss 0.178
2023-12-19 11:54:02,751 [slca.py] => Task 5, Epoch 12/20 => Loss 0.051
2023-12-19 11:54:05,318 [slca.py] => Task 5, Epoch 13/20 => Loss 0.046
2023-12-19 11:54:07,699 [slca.py] => Task 5, Epoch 14/20 => Loss 0.150
2023-12-19 11:54:24,526 [slca.py] => Task 5, Epoch 15/20 => Loss 0.041, Train_accy 85.830, Test_accy 60.800
2023-12-19 11:54:27,163 [slca.py] => Task 5, Epoch 16/20 => Loss 0.070
2023-12-19 11:54:29,562 [slca.py] => Task 5, Epoch 17/20 => Loss 0.027
2023-12-19 11:54:31,917 [slca.py] => Task 5, Epoch 18/20 => Loss 0.031
2023-12-19 11:54:34,292 [slca.py] => Task 5, Epoch 19/20 => Loss 0.069
2023-12-19 11:54:51,321 [slca.py] => Task 5, Epoch 20/20 => Loss 0.023, Train_accy 81.250, Test_accy 60.430
2023-12-19 11:55:09,996 [slca.py] => CA Task 5 => Loss 1.065, Test_accy 70.690
2023-12-19 11:55:23,110 [slca.py] => CA Task 5 => Loss 0.385, Test_accy 74.200
2023-12-19 11:55:36,322 [slca.py] => CA Task 5 => Loss 0.194, Test_accy 74.590
2023-12-19 11:55:49,722 [slca.py] => CA Task 5 => Loss 0.134, Test_accy 74.640
2023-12-19 11:56:02,890 [slca.py] => CA Task 5 => Loss 0.117, Test_accy 74.580
2023-12-19 11:56:15,587 [slca.py] => Exemplar size: 0
2023-12-19 11:56:16,362 [trainer.py] => No NME accuracy.
2023-12-19 11:56:16,363 [trainer.py] => CNN: {'total': 74.58, '00-09': 67.17, '10-19': 63.79, '20-29': 76.57, '30-39': 83.83, '40-49': 76.02, '50-59': 80.21, 'old': 73.46, 'new': 80.21}
2023-12-19 11:56:16,363 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19, 77.9, 75.52, 74.58]
2023-12-19 11:56:16,363 [trainer.py] => CNN top1 avg: 78.87333333333332
2023-12-19 11:56:16,363 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6, 94.08, 93.49, 92.88]

2023-12-19 11:56:16,363 [trainer.py] => All params: 85844796
2023-12-19 11:56:16,364 [trainer.py] => Trainable params: 85844796
2023-12-19 11:56:16,364 [slca.py] => Learning on 60-70
2023-12-19 11:56:19,042 [slca.py] => Task 6, Epoch 1/20 => Loss 2.242
2023-12-19 11:56:21,718 [slca.py] => Task 6, Epoch 2/20 => Loss 1.336
2023-12-19 11:56:24,376 [slca.py] => Task 6, Epoch 3/20 => Loss 0.915
2023-12-19 11:56:26,971 [slca.py] => Task 6, Epoch 4/20 => Loss 0.798
2023-12-19 11:56:46,343 [slca.py] => Task 6, Epoch 5/20 => Loss 0.289, Train_accy 79.640, Test_accy 58.420
2023-12-19 11:56:48,957 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 11:56:51,621 [slca.py] => Task 6, Epoch 7/20 => Loss 0.144
2023-12-19 11:56:54,260 [slca.py] => Task 6, Epoch 8/20 => Loss 0.074
2023-12-19 11:56:56,867 [slca.py] => Task 6, Epoch 9/20 => Loss 0.128
2023-12-19 11:57:16,188 [slca.py] => Task 6, Epoch 10/20 => Loss 0.054, Train_accy 85.710, Test_accy 59.720
2023-12-19 11:57:18,843 [slca.py] => Task 6, Epoch 11/20 => Loss 0.045
2023-12-19 11:57:21,437 [slca.py] => Task 6, Epoch 12/20 => Loss 0.111
2023-12-19 11:57:24,038 [slca.py] => Task 6, Epoch 13/20 => Loss 0.073
2023-12-19 11:57:26,750 [slca.py] => Task 6, Epoch 14/20 => Loss 0.094
2023-12-19 11:57:46,022 [slca.py] => Task 6, Epoch 15/20 => Loss 0.056, Train_accy 79.640, Test_accy 59.740
2023-12-19 11:57:48,795 [slca.py] => Task 6, Epoch 16/20 => Loss 0.151
2023-12-19 11:57:51,508 [slca.py] => Task 6, Epoch 17/20 => Loss 0.084
2023-12-19 11:57:54,228 [slca.py] => Task 6, Epoch 18/20 => Loss 0.229
2023-12-19 11:57:56,923 [slca.py] => Task 6, Epoch 19/20 => Loss 0.020
2023-12-19 11:58:16,201 [slca.py] => Task 6, Epoch 20/20 => Loss 0.040, Train_accy 81.070, Test_accy 59.270
2023-12-19 11:58:37,028 [slca.py] => CA Task 6 => Loss 1.084, Test_accy 71.140
2023-12-19 11:58:52,228 [slca.py] => CA Task 6 => Loss 0.388, Test_accy 74.360
2023-12-19 11:59:07,461 [slca.py] => CA Task 6 => Loss 0.204, Test_accy 74.880
2023-12-19 11:59:22,612 [slca.py] => CA Task 6 => Loss 0.144, Test_accy 74.770
2023-12-19 11:59:37,830 [slca.py] => CA Task 6 => Loss 0.126, Test_accy 74.740
2023-12-19 11:59:52,616 [slca.py] => Exemplar size: 0
2023-12-19 11:59:53,399 [trainer.py] => No NME accuracy.
2023-12-19 11:59:53,399 [trainer.py] => CNN: {'total': 74.74, '00-09': 67.04, '10-19': 63.83, '20-29': 75.25, '30-39': 83.93, '40-49': 73.51, '50-59': 76.13, '60-69': 83.55, 'old': 73.27, 'new': 83.55}
2023-12-19 11:59:53,399 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19, 77.9, 75.52, 74.58, 74.74]
2023-12-19 11:59:53,399 [trainer.py] => CNN top1 avg: 78.28285714285713
2023-12-19 11:59:53,399 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6, 94.08, 93.49, 92.88, 92.52]

2023-12-19 11:59:53,400 [trainer.py] => All params: 85852486
2023-12-19 11:59:53,400 [trainer.py] => Trainable params: 85852486
2023-12-19 11:59:53,401 [slca.py] => Learning on 70-80
2023-12-19 11:59:56,347 [slca.py] => Task 7, Epoch 1/20 => Loss 2.442
2023-12-19 11:59:59,200 [slca.py] => Task 7, Epoch 2/20 => Loss 1.757
2023-12-19 12:00:02,098 [slca.py] => Task 7, Epoch 3/20 => Loss 1.055
2023-12-19 12:00:05,010 [slca.py] => Task 7, Epoch 4/20 => Loss 0.490
2023-12-19 12:00:26,646 [slca.py] => Task 7, Epoch 5/20 => Loss 0.342, Train_accy 76.880, Test_accy 56.510
2023-12-19 12:00:29,427 [slca.py] => Task 7, Epoch 6/20 => Loss 0.233
2023-12-19 12:00:32,317 [slca.py] => Task 7, Epoch 7/20 => Loss 0.188
2023-12-19 12:00:35,097 [slca.py] => Task 7, Epoch 8/20 => Loss 0.219
2023-12-19 12:00:37,980 [slca.py] => Task 7, Epoch 9/20 => Loss 0.090
2023-12-19 12:00:59,345 [slca.py] => Task 7, Epoch 10/20 => Loss 0.196, Train_accy 83.440, Test_accy 59.190
2023-12-19 12:01:02,188 [slca.py] => Task 7, Epoch 11/20 => Loss 0.201
2023-12-19 12:01:05,094 [slca.py] => Task 7, Epoch 12/20 => Loss 0.142
2023-12-19 12:01:07,921 [slca.py] => Task 7, Epoch 13/20 => Loss 0.130
2023-12-19 12:01:10,782 [slca.py] => Task 7, Epoch 14/20 => Loss 0.082
2023-12-19 12:01:32,263 [slca.py] => Task 7, Epoch 15/20 => Loss 0.033, Train_accy 79.690, Test_accy 59.000
2023-12-19 12:01:35,107 [slca.py] => Task 7, Epoch 16/20 => Loss 0.103
2023-12-19 12:01:38,030 [slca.py] => Task 7, Epoch 17/20 => Loss 0.032
2023-12-19 12:01:40,911 [slca.py] => Task 7, Epoch 18/20 => Loss 0.036
2023-12-19 12:01:43,792 [slca.py] => Task 7, Epoch 19/20 => Loss 0.032
2023-12-19 12:02:05,199 [slca.py] => Task 7, Epoch 20/20 => Loss 0.052, Train_accy 78.440, Test_accy 58.440
2023-12-19 12:02:28,255 [slca.py] => CA Task 7 => Loss 1.078, Test_accy 69.800
2023-12-19 12:02:45,587 [slca.py] => CA Task 7 => Loss 0.385, Test_accy 72.720
2023-12-19 12:03:02,897 [slca.py] => CA Task 7 => Loss 0.205, Test_accy 73.490
2023-12-19 12:03:20,256 [slca.py] => CA Task 7 => Loss 0.149, Test_accy 73.700
2023-12-19 12:03:37,810 [slca.py] => CA Task 7 => Loss 0.129, Test_accy 73.740
2023-12-19 12:03:54,868 [slca.py] => Exemplar size: 0
2023-12-19 12:03:55,601 [trainer.py] => No NME accuracy.
2023-12-19 12:03:55,601 [trainer.py] => CNN: {'total': 73.74, '00-09': 62.66, '10-19': 61.05, '20-29': 70.64, '30-39': 81.86, '40-49': 72.54, '50-59': 76.72, '60-69': 81.11, '70-79': 83.37, 'old': 72.36, 'new': 83.37}
2023-12-19 12:03:55,601 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19, 77.9, 75.52, 74.58, 74.74, 73.74]
2023-12-19 12:03:55,601 [trainer.py] => CNN top1 avg: 77.715
2023-12-19 12:03:55,601 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6, 94.08, 93.49, 92.88, 92.52, 91.77]

2023-12-19 12:03:55,602 [trainer.py] => All params: 85860176
2023-12-19 12:03:55,602 [trainer.py] => Trainable params: 85860176
2023-12-19 12:03:55,603 [slca.py] => Learning on 80-90
2023-12-19 12:03:58,792 [slca.py] => Task 8, Epoch 1/20 => Loss 2.424
2023-12-19 12:04:01,807 [slca.py] => Task 8, Epoch 2/20 => Loss 1.635
2023-12-19 12:04:04,821 [slca.py] => Task 8, Epoch 3/20 => Loss 0.867
2023-12-19 12:04:07,893 [slca.py] => Task 8, Epoch 4/20 => Loss 0.298
2023-12-19 12:04:31,787 [slca.py] => Task 8, Epoch 5/20 => Loss 0.193, Train_accy 78.610, Test_accy 57.290
2023-12-19 12:04:34,800 [slca.py] => Task 8, Epoch 6/20 => Loss 0.264
2023-12-19 12:04:37,915 [slca.py] => Task 8, Epoch 7/20 => Loss 0.088
2023-12-19 12:04:40,918 [slca.py] => Task 8, Epoch 8/20 => Loss 0.071
2023-12-19 12:04:44,042 [slca.py] => Task 8, Epoch 9/20 => Loss 0.041
2023-12-19 12:05:07,824 [slca.py] => Task 8, Epoch 10/20 => Loss 0.072, Train_accy 80.000, Test_accy 59.420
2023-12-19 12:05:10,925 [slca.py] => Task 8, Epoch 11/20 => Loss 0.015
2023-12-19 12:05:13,943 [slca.py] => Task 8, Epoch 12/20 => Loss 0.043
2023-12-19 12:05:17,062 [slca.py] => Task 8, Epoch 13/20 => Loss 0.020
2023-12-19 12:05:20,092 [slca.py] => Task 8, Epoch 14/20 => Loss 0.083
2023-12-19 12:05:43,659 [slca.py] => Task 8, Epoch 15/20 => Loss 0.014, Train_accy 83.060, Test_accy 59.390
2023-12-19 12:05:46,677 [slca.py] => Task 8, Epoch 16/20 => Loss 0.127
2023-12-19 12:05:49,719 [slca.py] => Task 8, Epoch 17/20 => Loss 0.049
2023-12-19 12:05:52,857 [slca.py] => Task 8, Epoch 18/20 => Loss 0.096
2023-12-19 12:05:55,954 [slca.py] => Task 8, Epoch 19/20 => Loss 0.013
2023-12-19 12:06:19,766 [slca.py] => Task 8, Epoch 20/20 => Loss 0.094, Train_accy 81.110, Test_accy 59.410
2023-12-19 12:06:44,792 [slca.py] => CA Task 8 => Loss 1.070, Test_accy 70.590
2023-12-19 12:07:04,513 [slca.py] => CA Task 8 => Loss 0.378, Test_accy 73.550
2023-12-19 12:07:24,303 [slca.py] => CA Task 8 => Loss 0.204, Test_accy 74.250
2023-12-19 12:07:43,810 [slca.py] => CA Task 8 => Loss 0.148, Test_accy 74.330
2023-12-19 12:08:03,415 [slca.py] => CA Task 8 => Loss 0.131, Test_accy 74.400
2023-12-19 12:08:22,107 [slca.py] => Exemplar size: 0
2023-12-19 12:08:22,884 [trainer.py] => No NME accuracy.
2023-12-19 12:08:22,884 [trainer.py] => CNN: {'total': 74.4, '00-09': 62.71, '10-19': 58.63, '20-29': 71.72, '30-39': 82.63, '40-49': 69.46, '50-59': 75.15, '60-69': 79.96, '70-79': 82.28, '80-89': 87.04, 'old': 72.82, 'new': 87.04}
2023-12-19 12:08:22,884 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19, 77.9, 75.52, 74.58, 74.74, 73.74, 74.4]
2023-12-19 12:08:22,885 [trainer.py] => CNN top1 avg: 77.34666666666666
2023-12-19 12:08:22,885 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6, 94.08, 93.49, 92.88, 92.52, 91.77, 92.25]

2023-12-19 12:08:22,885 [trainer.py] => All params: 85867866
2023-12-19 12:08:22,886 [trainer.py] => Trainable params: 85867866
2023-12-19 12:08:22,886 [slca.py] => Learning on 90-100
2023-12-19 12:08:26,175 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 12:08:29,386 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 12:08:32,699 [slca.py] => Task 9, Epoch 3/20 => Loss 0.626
2023-12-19 12:08:35,941 [slca.py] => Task 9, Epoch 4/20 => Loss 0.270
2023-12-19 12:09:02,232 [slca.py] => Task 9, Epoch 5/20 => Loss 0.118, Train_accy 81.000, Test_accy 56.790
2023-12-19 12:09:05,516 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 12:09:08,816 [slca.py] => Task 9, Epoch 7/20 => Loss 0.108
2023-12-19 12:09:12,078 [slca.py] => Task 9, Epoch 8/20 => Loss 0.114
2023-12-19 12:09:15,551 [slca.py] => Task 9, Epoch 9/20 => Loss 0.094
2023-12-19 12:09:41,532 [slca.py] => Task 9, Epoch 10/20 => Loss 0.010, Train_accy 81.750, Test_accy 58.190
2023-12-19 12:09:44,787 [slca.py] => Task 9, Epoch 11/20 => Loss 0.020
2023-12-19 12:09:48,043 [slca.py] => Task 9, Epoch 12/20 => Loss 0.027
2023-12-19 12:09:51,259 [slca.py] => Task 9, Epoch 13/20 => Loss 0.026
2023-12-19 12:09:54,581 [slca.py] => Task 9, Epoch 14/20 => Loss 0.062
2023-12-19 12:10:20,695 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 82.000, Test_accy 57.770
2023-12-19 12:10:23,954 [slca.py] => Task 9, Epoch 16/20 => Loss 0.037
2023-12-19 12:10:27,191 [slca.py] => Task 9, Epoch 17/20 => Loss 0.061
2023-12-19 12:10:30,492 [slca.py] => Task 9, Epoch 18/20 => Loss 0.008
2023-12-19 12:10:33,791 [slca.py] => Task 9, Epoch 19/20 => Loss 0.058
2023-12-19 12:11:00,059 [slca.py] => Task 9, Epoch 20/20 => Loss 0.049, Train_accy 80.750, Test_accy 58.040
2023-12-19 12:11:27,474 [slca.py] => CA Task 9 => Loss 1.097, Test_accy 69.350
2023-12-19 12:11:49,054 [slca.py] => CA Task 9 => Loss 0.390, Test_accy 72.470
2023-12-19 12:12:10,670 [slca.py] => CA Task 9 => Loss 0.213, Test_accy 73.450
2023-12-19 12:12:32,227 [slca.py] => CA Task 9 => Loss 0.155, Test_accy 73.550
2023-12-19 12:12:53,680 [slca.py] => CA Task 9 => Loss 0.139, Test_accy 73.580
2023-12-19 12:13:14,488 [slca.py] => Exemplar size: 0
2023-12-19 12:13:15,303 [trainer.py] => No NME accuracy.
2023-12-19 12:13:15,303 [trainer.py] => CNN: {'total': 73.58, '00-09': 60.26, '10-19': 58.86, '20-29': 72.97, '30-39': 80.16, '40-49': 70.3, '50-59': 75.05, '60-69': 78.58, '70-79': 78.44, '80-89': 85.67, '90-99': 75.53, 'old': 73.36, 'new': 75.53}
2023-12-19 12:13:15,303 [trainer.py] => CNN top1 curve: [89.29, 77.76, 78.19, 77.9, 75.52, 74.58, 74.74, 73.74, 74.4, 73.58]
2023-12-19 12:13:15,303 [trainer.py] => CNN top1 avg: 76.97
2023-12-19 12:13:15,303 [trainer.py] => CNN top5 curve: [99.33, 96.3, 94.6, 94.08, 93.49, 92.88, 92.52, 91.77, 92.25, 92.48]

2023-12-19 12:13:15,304 [trainer.py] => final accs: [73.58]
2023-12-19 12:13:15,304 [trainer.py] => avg accs: [76.97]
