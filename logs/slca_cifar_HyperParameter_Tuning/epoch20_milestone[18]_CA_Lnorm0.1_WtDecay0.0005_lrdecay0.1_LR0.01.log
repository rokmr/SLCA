2023-12-19 05:10:38,955 [trainer.py] => config: exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 05:10:38,955 [trainer.py] => test_only: False
2023-12-19 05:10:38,955 [trainer.py] => prefix: reproduce
2023-12-19 05:10:38,955 [trainer.py] => dataset: cifar100_224
2023-12-19 05:10:38,955 [trainer.py] => memory_size: 0
2023-12-19 05:10:38,955 [trainer.py] => memory_per_class: 0
2023-12-19 05:10:38,955 [trainer.py] => fixed_memory: False
2023-12-19 05:10:38,955 [trainer.py] => shuffle: False
2023-12-19 05:10:38,955 [trainer.py] => init_cls: 10
2023-12-19 05:10:38,955 [trainer.py] => increment: 10
2023-12-19 05:10:38,955 [trainer.py] => model_name: slca_cifar
2023-12-19 05:10:38,955 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 05:10:38,955 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 05:10:38,955 [trainer.py] => device: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2)]
2023-12-19 05:10:38,955 [trainer.py] => seed: 0
2023-12-19 05:10:38,955 [trainer.py] => epochs: 20
2023-12-19 05:10:38,955 [trainer.py] => ca_epochs: 5
2023-12-19 05:10:38,955 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-19 05:10:38,955 [trainer.py] => milestones: [18]
2023-12-19 05:10:38,955 [trainer.py] => lr: 0.01
2023-12-19 05:10:38,955 [trainer.py] => lr_decay: 0.1
2023-12-19 05:10:38,955 [trainer.py] => weight_decay: 0.0005
2023-12-19 05:10:38,956 [trainer.py] => u_batch_size: 256
2023-12-19 05:10:38,956 [trainer.py] => s_batch_size: 3
2023-12-19 05:10:38,956 [trainer.py] => multicrop: 2
2023-12-19 05:10:38,956 [trainer.py] => us_multicrop: 2
2023-12-19 05:10:38,956 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 05:10:38,956 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 05:10:38,956 [trainer.py] => buffer_size: 500
2023-12-19 05:10:38,956 [trainer.py] => run_id: 0
2023-12-19 05:11:06,232 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 05:11:08,323 [trainer.py] => All params: 85798656
2023-12-19 05:11:08,323 [trainer.py] => Trainable params: 85798656
2023-12-19 05:11:08,324 [slca.py] => Learning on 0-10
2023-12-19 05:11:22,321 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 05:11:23,348 [slca.py] => Task 0, Epoch 2/20 => Loss 2.314
2023-12-19 05:11:24,350 [slca.py] => Task 0, Epoch 3/20 => Loss 2.105
2023-12-19 05:11:25,344 [slca.py] => Task 0, Epoch 4/20 => Loss 1.740
2023-12-19 05:11:30,034 [slca.py] => Task 0, Epoch 5/20 => Loss 1.514, Train_accy 72.500, Test_accy 58.040
2023-12-19 05:11:31,125 [slca.py] => Task 0, Epoch 6/20 => Loss 1.184
2023-12-19 05:11:32,148 [slca.py] => Task 0, Epoch 7/20 => Loss 0.837
2023-12-19 05:11:33,283 [slca.py] => Task 0, Epoch 8/20 => Loss 0.778
2023-12-19 05:11:34,379 [slca.py] => Task 0, Epoch 9/20 => Loss 0.624
2023-12-19 05:11:39,234 [slca.py] => Task 0, Epoch 10/20 => Loss 0.476, Train_accy 95.000, Test_accy 79.240
2023-12-19 05:11:40,248 [slca.py] => Task 0, Epoch 11/20 => Loss 0.339
2023-12-19 05:11:41,332 [slca.py] => Task 0, Epoch 12/20 => Loss 0.394
2023-12-19 05:11:42,332 [slca.py] => Task 0, Epoch 13/20 => Loss 0.225
2023-12-19 05:11:43,384 [slca.py] => Task 0, Epoch 14/20 => Loss 0.203
2023-12-19 05:11:48,256 [slca.py] => Task 0, Epoch 15/20 => Loss 0.200, Train_accy 100.000, Test_accy 86.270
2023-12-19 05:11:49,364 [slca.py] => Task 0, Epoch 16/20 => Loss 0.107
2023-12-19 05:11:50,453 [slca.py] => Task 0, Epoch 17/20 => Loss 0.238
2023-12-19 05:11:51,508 [slca.py] => Task 0, Epoch 18/20 => Loss 0.087
2023-12-19 05:11:52,632 [slca.py] => Task 0, Epoch 19/20 => Loss 0.048
2023-12-19 05:11:57,546 [slca.py] => Task 0, Epoch 20/20 => Loss 0.112, Train_accy 97.500, Test_accy 88.500
2023-12-19 05:12:06,837 [slca.py] => CA Task 0 => Loss 0.018, Test_accy 89.840
2023-12-19 05:12:11,147 [slca.py] => CA Task 0 => Loss 0.007, Test_accy 90.850
2023-12-19 05:12:15,591 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 91.070
2023-12-19 05:12:20,072 [slca.py] => CA Task 0 => Loss 0.003, Test_accy 91.290
2023-12-19 05:12:24,611 [slca.py] => CA Task 0 => Loss 0.003, Test_accy 91.290
2023-12-19 05:12:27,425 [slca.py] => Exemplar size: 0
2023-12-19 05:12:27,907 [trainer.py] => No NME accuracy.
2023-12-19 05:12:27,907 [trainer.py] => CNN: {'total': 91.29, '00-09': 91.29, 'old': 0, 'new': 91.29}
2023-12-19 05:12:27,907 [trainer.py] => CNN top1 curve: [91.29]
2023-12-19 05:12:27,908 [trainer.py] => CNN top1 avg: 91.29
2023-12-19 05:12:27,908 [trainer.py] => CNN top5 curve: [99.78]

2023-12-19 05:12:27,908 [trainer.py] => All params: 85806346
2023-12-19 05:12:27,908 [trainer.py] => Trainable params: 85806346
2023-12-19 05:12:27,909 [slca.py] => Learning on 10-20
2023-12-19 05:12:29,215 [slca.py] => Task 1, Epoch 1/20 => Loss 2.425
2023-12-19 05:12:30,534 [slca.py] => Task 1, Epoch 2/20 => Loss 2.360
2023-12-19 05:12:31,852 [slca.py] => Task 1, Epoch 3/20 => Loss 2.063
2023-12-19 05:12:33,071 [slca.py] => Task 1, Epoch 4/20 => Loss 1.759
2023-12-19 05:12:40,108 [slca.py] => Task 1, Epoch 5/20 => Loss 1.357, Train_accy 62.500, Test_accy 46.560
2023-12-19 05:12:41,435 [slca.py] => Task 1, Epoch 6/20 => Loss 0.922
2023-12-19 05:12:42,651 [slca.py] => Task 1, Epoch 7/20 => Loss 0.715
2023-12-19 05:12:43,859 [slca.py] => Task 1, Epoch 8/20 => Loss 0.619
2023-12-19 05:12:45,043 [slca.py] => Task 1, Epoch 9/20 => Loss 0.391
2023-12-19 05:12:52,095 [slca.py] => Task 1, Epoch 10/20 => Loss 0.194, Train_accy 90.000, Test_accy 67.760
2023-12-19 05:12:53,312 [slca.py] => Task 1, Epoch 11/20 => Loss 0.202
2023-12-19 05:12:54,526 [slca.py] => Task 1, Epoch 12/20 => Loss 0.208
2023-12-19 05:12:55,884 [slca.py] => Task 1, Epoch 13/20 => Loss 0.071
2023-12-19 05:12:57,181 [slca.py] => Task 1, Epoch 14/20 => Loss 0.155
2023-12-19 05:13:04,266 [slca.py] => Task 1, Epoch 15/20 => Loss 0.126, Train_accy 97.500, Test_accy 76.150
2023-12-19 05:13:05,559 [slca.py] => Task 1, Epoch 16/20 => Loss 0.057
2023-12-19 05:13:06,918 [slca.py] => Task 1, Epoch 17/20 => Loss 0.080
2023-12-19 05:13:08,207 [slca.py] => Task 1, Epoch 18/20 => Loss 0.078
2023-12-19 05:13:09,475 [slca.py] => Task 1, Epoch 19/20 => Loss 0.041
2023-12-19 05:13:16,762 [slca.py] => Task 1, Epoch 20/20 => Loss 0.086, Train_accy 92.500, Test_accy 77.710
2023-12-19 05:13:29,342 [slca.py] => CA Task 1 => Loss 0.018, Test_accy 78.800
2023-12-19 05:13:37,205 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 79.840
2023-12-19 05:13:45,176 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 80.160
2023-12-19 05:13:53,156 [slca.py] => CA Task 1 => Loss 0.002, Test_accy 80.260
2023-12-19 05:14:01,101 [slca.py] => CA Task 1 => Loss 0.002, Test_accy 80.210
2023-12-19 05:14:05,840 [slca.py] => Exemplar size: 0
2023-12-19 05:14:06,324 [trainer.py] => No NME accuracy.
2023-12-19 05:14:06,325 [trainer.py] => CNN: {'total': 80.21, '00-09': 86.04, '10-19': 74.38, 'old': 86.04, 'new': 74.38}
2023-12-19 05:14:06,325 [trainer.py] => CNN top1 curve: [91.29, 80.21]
2023-12-19 05:14:06,325 [trainer.py] => CNN top1 avg: 85.75
2023-12-19 05:14:06,325 [trainer.py] => CNN top5 curve: [99.78, 98.39]

2023-12-19 05:14:06,325 [trainer.py] => All params: 85814036
2023-12-19 05:14:06,326 [trainer.py] => Trainable params: 85814036
2023-12-19 05:14:06,326 [slca.py] => Learning on 20-30
2023-12-19 05:14:07,794 [slca.py] => Task 2, Epoch 1/20 => Loss 2.370
2023-12-19 05:14:09,309 [slca.py] => Task 2, Epoch 2/20 => Loss 2.159
2023-12-19 05:14:10,722 [slca.py] => Task 2, Epoch 3/20 => Loss 1.705
2023-12-19 05:14:12,211 [slca.py] => Task 2, Epoch 4/20 => Loss 1.521
2023-12-19 05:14:21,516 [slca.py] => Task 2, Epoch 5/20 => Loss 1.114, Train_accy 70.000, Test_accy 53.500
2023-12-19 05:14:23,013 [slca.py] => Task 2, Epoch 6/20 => Loss 0.866
2023-12-19 05:14:24,581 [slca.py] => Task 2, Epoch 7/20 => Loss 0.485
2023-12-19 05:14:26,087 [slca.py] => Task 2, Epoch 8/20 => Loss 0.395
2023-12-19 05:14:27,595 [slca.py] => Task 2, Epoch 9/20 => Loss 0.322
2023-12-19 05:14:36,777 [slca.py] => Task 2, Epoch 10/20 => Loss 0.320, Train_accy 90.000, Test_accy 71.060
2023-12-19 05:14:38,198 [slca.py] => Task 2, Epoch 11/20 => Loss 0.127
2023-12-19 05:14:39,724 [slca.py] => Task 2, Epoch 12/20 => Loss 0.052
2023-12-19 05:14:41,201 [slca.py] => Task 2, Epoch 13/20 => Loss 0.053
2023-12-19 05:14:42,564 [slca.py] => Task 2, Epoch 14/20 => Loss 0.088
2023-12-19 05:14:52,062 [slca.py] => Task 2, Epoch 15/20 => Loss 0.188, Train_accy 92.500, Test_accy 76.020
2023-12-19 05:14:53,489 [slca.py] => Task 2, Epoch 16/20 => Loss 0.088
2023-12-19 05:14:54,892 [slca.py] => Task 2, Epoch 17/20 => Loss 0.203
2023-12-19 05:14:56,502 [slca.py] => Task 2, Epoch 18/20 => Loss 0.066
2023-12-19 05:14:58,012 [slca.py] => Task 2, Epoch 19/20 => Loss 0.061
2023-12-19 05:15:07,487 [slca.py] => Task 2, Epoch 20/20 => Loss 0.049, Train_accy 90.830, Test_accy 76.220
2023-12-19 05:15:23,574 [slca.py] => CA Task 2 => Loss 0.018, Test_accy 77.580
2023-12-19 05:15:35,139 [slca.py] => CA Task 2 => Loss 0.005, Test_accy 77.990
2023-12-19 05:15:46,653 [slca.py] => CA Task 2 => Loss 0.003, Test_accy 77.990
2023-12-19 05:15:58,236 [slca.py] => CA Task 2 => Loss 0.002, Test_accy 78.060
2023-12-19 05:16:09,860 [slca.py] => CA Task 2 => Loss 0.002, Test_accy 78.090
2023-12-19 05:16:16,634 [slca.py] => Exemplar size: 0
2023-12-19 05:16:17,116 [trainer.py] => No NME accuracy.
2023-12-19 05:16:17,116 [trainer.py] => CNN: {'total': 78.09, '00-09': 80.06, '10-19': 70.7, '20-29': 83.52, 'old': 75.37, 'new': 83.52}
2023-12-19 05:16:17,116 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09]
2023-12-19 05:16:17,116 [trainer.py] => CNN top1 avg: 83.19666666666667
2023-12-19 05:16:17,116 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11]

2023-12-19 05:16:17,117 [trainer.py] => All params: 85821726
2023-12-19 05:16:17,117 [trainer.py] => Trainable params: 85821726
2023-12-19 05:16:17,118 [slca.py] => Learning on 30-40
2023-12-19 05:16:19,028 [slca.py] => Task 3, Epoch 1/20 => Loss 2.381
2023-12-19 05:16:20,835 [slca.py] => Task 3, Epoch 2/20 => Loss 1.770
2023-12-19 05:16:22,619 [slca.py] => Task 3, Epoch 3/20 => Loss 1.216
2023-12-19 05:16:24,436 [slca.py] => Task 3, Epoch 4/20 => Loss 0.910
2023-12-19 05:16:36,093 [slca.py] => Task 3, Epoch 5/20 => Loss 0.426, Train_accy 85.000, Test_accy 70.720
2023-12-19 05:16:37,901 [slca.py] => Task 3, Epoch 6/20 => Loss 0.337
2023-12-19 05:16:39,727 [slca.py] => Task 3, Epoch 7/20 => Loss 0.181
2023-12-19 05:16:41,391 [slca.py] => Task 3, Epoch 8/20 => Loss 0.084
2023-12-19 05:16:43,047 [slca.py] => Task 3, Epoch 9/20 => Loss 0.221
2023-12-19 05:16:54,786 [slca.py] => Task 3, Epoch 10/20 => Loss 0.070, Train_accy 83.750, Test_accy 72.960
2023-12-19 05:16:56,495 [slca.py] => Task 3, Epoch 11/20 => Loss 0.065
2023-12-19 05:16:58,225 [slca.py] => Task 3, Epoch 12/20 => Loss 0.081
2023-12-19 05:17:00,016 [slca.py] => Task 3, Epoch 13/20 => Loss 0.029
2023-12-19 05:17:01,797 [slca.py] => Task 3, Epoch 14/20 => Loss 0.041
2023-12-19 05:17:13,545 [slca.py] => Task 3, Epoch 15/20 => Loss 0.029, Train_accy 83.750, Test_accy 72.830
2023-12-19 05:17:15,401 [slca.py] => Task 3, Epoch 16/20 => Loss 0.079
2023-12-19 05:17:17,220 [slca.py] => Task 3, Epoch 17/20 => Loss 0.093
2023-12-19 05:17:18,980 [slca.py] => Task 3, Epoch 18/20 => Loss 0.158
2023-12-19 05:17:20,858 [slca.py] => Task 3, Epoch 19/20 => Loss 0.048
2023-12-19 05:17:32,698 [slca.py] => Task 3, Epoch 20/20 => Loss 0.018, Train_accy 86.880, Test_accy 72.780
2023-12-19 05:17:52,982 [slca.py] => CA Task 3 => Loss 0.029, Test_accy 77.070
2023-12-19 05:18:08,353 [slca.py] => CA Task 3 => Loss 0.006, Test_accy 77.900
2023-12-19 05:18:23,581 [slca.py] => CA Task 3 => Loss 0.004, Test_accy 78.100
2023-12-19 05:18:38,841 [slca.py] => CA Task 3 => Loss 0.003, Test_accy 78.150
2023-12-19 05:18:54,087 [slca.py] => CA Task 3 => Loss 0.003, Test_accy 78.150
2023-12-19 05:19:02,879 [slca.py] => Exemplar size: 0
2023-12-19 05:19:03,365 [trainer.py] => No NME accuracy.
2023-12-19 05:19:03,365 [trainer.py] => CNN: {'total': 78.15, '00-09': 77.37, '10-19': 64.42, '20-29': 80.65, '30-39': 90.14, 'old': 74.14, 'new': 90.14}
2023-12-19 05:19:03,365 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09, 78.15]
2023-12-19 05:19:03,365 [trainer.py] => CNN top1 avg: 81.935
2023-12-19 05:19:03,365 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11, 96.12]

2023-12-19 05:19:03,366 [trainer.py] => All params: 85829416
2023-12-19 05:19:03,366 [trainer.py] => Trainable params: 85829416
2023-12-19 05:19:03,367 [slca.py] => Learning on 40-50
2023-12-19 05:19:05,181 [slca.py] => Task 4, Epoch 1/20 => Loss 2.337
2023-12-19 05:19:06,988 [slca.py] => Task 4, Epoch 2/20 => Loss 1.710
2023-12-19 05:19:08,807 [slca.py] => Task 4, Epoch 3/20 => Loss 0.913
2023-12-19 05:19:10,605 [slca.py] => Task 4, Epoch 4/20 => Loss 0.477
2023-12-19 05:19:24,374 [slca.py] => Task 4, Epoch 5/20 => Loss 0.239, Train_accy 82.000, Test_accy 64.100
2023-12-19 05:19:26,231 [slca.py] => Task 4, Epoch 6/20 => Loss 0.193
2023-12-19 05:19:28,151 [slca.py] => Task 4, Epoch 7/20 => Loss 0.054
2023-12-19 05:19:30,045 [slca.py] => Task 4, Epoch 8/20 => Loss 0.348
2023-12-19 05:19:32,029 [slca.py] => Task 4, Epoch 9/20 => Loss 0.115
2023-12-19 05:19:45,652 [slca.py] => Task 4, Epoch 10/20 => Loss 0.119, Train_accy 84.500, Test_accy 68.790
2023-12-19 05:19:47,530 [slca.py] => Task 4, Epoch 11/20 => Loss 0.081
2023-12-19 05:19:49,404 [slca.py] => Task 4, Epoch 12/20 => Loss 0.042
2023-12-19 05:19:51,257 [slca.py] => Task 4, Epoch 13/20 => Loss 0.045
2023-12-19 05:19:53,134 [slca.py] => Task 4, Epoch 14/20 => Loss 0.007
2023-12-19 05:20:06,759 [slca.py] => Task 4, Epoch 15/20 => Loss 0.090, Train_accy 86.000, Test_accy 67.970
2023-12-19 05:20:08,592 [slca.py] => Task 4, Epoch 16/20 => Loss 0.071
2023-12-19 05:20:10,336 [slca.py] => Task 4, Epoch 17/20 => Loss 0.033
2023-12-19 05:20:12,142 [slca.py] => Task 4, Epoch 18/20 => Loss 0.032
2023-12-19 05:20:13,958 [slca.py] => Task 4, Epoch 19/20 => Loss 0.094
2023-12-19 05:20:27,895 [slca.py] => Task 4, Epoch 20/20 => Loss 0.142, Train_accy 88.000, Test_accy 67.590
2023-12-19 05:20:51,559 [slca.py] => CA Task 4 => Loss 0.030, Test_accy 72.220
2023-12-19 05:21:10,544 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 73.120
2023-12-19 05:21:29,400 [slca.py] => CA Task 4 => Loss 0.004, Test_accy 73.360
2023-12-19 05:21:48,239 [slca.py] => CA Task 4 => Loss 0.003, Test_accy 73.520
2023-12-19 05:22:07,062 [slca.py] => CA Task 4 => Loss 0.004, Test_accy 73.600
2023-12-19 05:22:18,034 [slca.py] => Exemplar size: 0
2023-12-19 05:22:18,529 [trainer.py] => No NME accuracy.
2023-12-19 05:22:18,530 [trainer.py] => CNN: {'total': 73.6, '00-09': 75.35, '10-19': 62.06, '20-29': 70.24, '30-39': 83.75, '40-49': 76.6, 'old': 72.85, 'new': 76.6}
2023-12-19 05:22:18,530 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09, 78.15, 73.6]
2023-12-19 05:22:18,530 [trainer.py] => CNN top1 avg: 80.268
2023-12-19 05:22:18,530 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11, 96.12, 94.99]

2023-12-19 05:22:18,530 [trainer.py] => All params: 85837106
2023-12-19 05:22:18,531 [trainer.py] => Trainable params: 85837106
2023-12-19 05:22:18,532 [slca.py] => Learning on 50-60
2023-12-19 05:22:20,512 [slca.py] => Task 5, Epoch 1/20 => Loss 2.431
2023-12-19 05:22:22,431 [slca.py] => Task 5, Epoch 2/20 => Loss 1.697
2023-12-19 05:22:24,320 [slca.py] => Task 5, Epoch 3/20 => Loss 0.906
2023-12-19 05:22:26,322 [slca.py] => Task 5, Epoch 4/20 => Loss 0.527
2023-12-19 05:22:41,811 [slca.py] => Task 5, Epoch 5/20 => Loss 0.188, Train_accy 77.080, Test_accy 60.440
2023-12-19 05:22:43,751 [slca.py] => Task 5, Epoch 6/20 => Loss 0.108
2023-12-19 05:22:45,718 [slca.py] => Task 5, Epoch 7/20 => Loss 0.057
2023-12-19 05:22:47,607 [slca.py] => Task 5, Epoch 8/20 => Loss 0.049
2023-12-19 05:22:49,609 [slca.py] => Task 5, Epoch 9/20 => Loss 0.223
2023-12-19 05:23:05,205 [slca.py] => Task 5, Epoch 10/20 => Loss 0.062, Train_accy 87.920, Test_accy 65.370
2023-12-19 05:23:07,219 [slca.py] => Task 5, Epoch 11/20 => Loss 0.121
2023-12-19 05:23:09,180 [slca.py] => Task 5, Epoch 12/20 => Loss 0.022
2023-12-19 05:23:11,086 [slca.py] => Task 5, Epoch 13/20 => Loss 0.010
2023-12-19 05:23:13,003 [slca.py] => Task 5, Epoch 14/20 => Loss 0.122
2023-12-19 05:23:28,476 [slca.py] => Task 5, Epoch 15/20 => Loss 0.014, Train_accy 85.420, Test_accy 65.220
2023-12-19 05:23:30,467 [slca.py] => Task 5, Epoch 16/20 => Loss 0.037
2023-12-19 05:23:32,327 [slca.py] => Task 5, Epoch 17/20 => Loss 0.003
2023-12-19 05:23:34,169 [slca.py] => Task 5, Epoch 18/20 => Loss 0.034
2023-12-19 05:23:36,128 [slca.py] => Task 5, Epoch 19/20 => Loss 0.035
2023-12-19 05:23:51,572 [slca.py] => Task 5, Epoch 20/20 => Loss 0.008, Train_accy 85.000, Test_accy 65.170
2023-12-19 05:24:18,608 [slca.py] => CA Task 5 => Loss 0.035, Test_accy 69.960
2023-12-19 05:24:40,932 [slca.py] => CA Task 5 => Loss 0.008, Test_accy 70.430
2023-12-19 05:25:03,099 [slca.py] => CA Task 5 => Loss 0.005, Test_accy 70.770
2023-12-19 05:25:25,353 [slca.py] => CA Task 5 => Loss 0.004, Test_accy 70.840
2023-12-19 05:25:47,581 [slca.py] => CA Task 5 => Loss 0.004, Test_accy 70.870
2023-12-19 05:26:00,210 [slca.py] => Exemplar size: 0
2023-12-19 05:26:00,710 [trainer.py] => No NME accuracy.
2023-12-19 05:26:00,711 [trainer.py] => CNN: {'total': 70.87, '00-09': 69.41, '10-19': 56.9, '20-29': 72.62, '30-39': 83.11, '40-49': 68.88, '50-59': 74.46, 'old': 70.16, 'new': 74.46}
2023-12-19 05:26:00,711 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09, 78.15, 73.6, 70.87]
2023-12-19 05:26:00,711 [trainer.py] => CNN top1 avg: 78.70166666666667
2023-12-19 05:26:00,711 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11, 96.12, 94.99, 93.44]

2023-12-19 05:26:00,711 [trainer.py] => All params: 85844796
2023-12-19 05:26:00,712 [trainer.py] => Trainable params: 85844796
2023-12-19 05:26:00,712 [slca.py] => Learning on 60-70
2023-12-19 05:26:02,973 [slca.py] => Task 6, Epoch 1/20 => Loss 2.197
2023-12-19 05:26:05,143 [slca.py] => Task 6, Epoch 2/20 => Loss 0.974
2023-12-19 05:26:07,319 [slca.py] => Task 6, Epoch 3/20 => Loss 0.517
2023-12-19 05:26:09,490 [slca.py] => Task 6, Epoch 4/20 => Loss 0.479
2023-12-19 05:26:27,403 [slca.py] => Task 6, Epoch 5/20 => Loss 0.209, Train_accy 82.140, Test_accy 64.000
2023-12-19 05:26:29,598 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 05:26:31,688 [slca.py] => Task 6, Epoch 7/20 => Loss 0.161
2023-12-19 05:26:33,802 [slca.py] => Task 6, Epoch 8/20 => Loss 0.039
2023-12-19 05:26:35,875 [slca.py] => Task 6, Epoch 9/20 => Loss 0.028
2023-12-19 05:26:53,592 [slca.py] => Task 6, Epoch 10/20 => Loss 0.066, Train_accy 87.500, Test_accy 64.280
2023-12-19 05:26:55,684 [slca.py] => Task 6, Epoch 11/20 => Loss 0.109
2023-12-19 05:26:57,852 [slca.py] => Task 6, Epoch 12/20 => Loss 0.044
2023-12-19 05:26:59,945 [slca.py] => Task 6, Epoch 13/20 => Loss 0.026
2023-12-19 05:27:02,205 [slca.py] => Task 6, Epoch 14/20 => Loss 0.184
2023-12-19 05:27:20,120 [slca.py] => Task 6, Epoch 15/20 => Loss 0.117, Train_accy 82.140, Test_accy 64.370
2023-12-19 05:27:22,312 [slca.py] => Task 6, Epoch 16/20 => Loss 0.011
2023-12-19 05:27:24,425 [slca.py] => Task 6, Epoch 17/20 => Loss 0.178
2023-12-19 05:27:26,529 [slca.py] => Task 6, Epoch 18/20 => Loss 0.031
2023-12-19 05:27:28,619 [slca.py] => Task 6, Epoch 19/20 => Loss 0.008
2023-12-19 05:27:46,410 [slca.py] => Task 6, Epoch 20/20 => Loss 0.008, Train_accy 85.710, Test_accy 63.630
2023-12-19 05:28:16,949 [slca.py] => CA Task 6 => Loss 0.039, Test_accy 68.940
2023-12-19 05:28:42,725 [slca.py] => CA Task 6 => Loss 0.009, Test_accy 69.570
2023-12-19 05:29:08,440 [slca.py] => CA Task 6 => Loss 0.005, Test_accy 69.600
2023-12-19 05:29:34,246 [slca.py] => CA Task 6 => Loss 0.005, Test_accy 69.700
2023-12-19 05:30:00,288 [slca.py] => CA Task 6 => Loss 0.004, Test_accy 69.700
2023-12-19 05:30:14,875 [slca.py] => Exemplar size: 0
2023-12-19 05:30:15,364 [trainer.py] => No NME accuracy.
2023-12-19 05:30:15,364 [trainer.py] => CNN: {'total': 69.7, '00-09': 63.2, '10-19': 53.7, '20-29': 68.86, '30-39': 80.57, '40-49': 68.45, '50-59': 73.92, '60-69': 79.29, 'old': 68.11, 'new': 79.29}
2023-12-19 05:30:15,364 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09, 78.15, 73.6, 70.87, 69.7]
2023-12-19 05:30:15,365 [trainer.py] => CNN top1 avg: 77.4157142857143
2023-12-19 05:30:15,365 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11, 96.12, 94.99, 93.44, 92.01]

2023-12-19 05:30:15,365 [trainer.py] => All params: 85852486
2023-12-19 05:30:15,365 [trainer.py] => Trainable params: 85852486
2023-12-19 05:30:15,366 [slca.py] => Learning on 70-80
2023-12-19 05:30:17,654 [slca.py] => Task 7, Epoch 1/20 => Loss 2.369
2023-12-19 05:30:19,912 [slca.py] => Task 7, Epoch 2/20 => Loss 1.175
2023-12-19 05:30:22,222 [slca.py] => Task 7, Epoch 3/20 => Loss 0.552
2023-12-19 05:30:24,486 [slca.py] => Task 7, Epoch 4/20 => Loss 0.180
2023-12-19 05:30:44,379 [slca.py] => Task 7, Epoch 5/20 => Loss 0.171, Train_accy 81.250, Test_accy 62.830
2023-12-19 05:30:46,672 [slca.py] => Task 7, Epoch 6/20 => Loss 0.154
2023-12-19 05:30:48,849 [slca.py] => Task 7, Epoch 7/20 => Loss 0.061
2023-12-19 05:30:51,059 [slca.py] => Task 7, Epoch 8/20 => Loss 0.229
2023-12-19 05:30:53,387 [slca.py] => Task 7, Epoch 9/20 => Loss 0.064
2023-12-19 05:31:13,556 [slca.py] => Task 7, Epoch 10/20 => Loss 0.159, Train_accy 86.880, Test_accy 62.970
2023-12-19 05:31:15,805 [slca.py] => Task 7, Epoch 11/20 => Loss 0.219
2023-12-19 05:31:18,014 [slca.py] => Task 7, Epoch 12/20 => Loss 0.096
2023-12-19 05:31:20,327 [slca.py] => Task 7, Epoch 13/20 => Loss 0.141
2023-12-19 05:31:22,505 [slca.py] => Task 7, Epoch 14/20 => Loss 0.117
2023-12-19 05:31:42,488 [slca.py] => Task 7, Epoch 15/20 => Loss 0.052, Train_accy 82.810, Test_accy 61.880
2023-12-19 05:31:44,811 [slca.py] => Task 7, Epoch 16/20 => Loss 0.152
2023-12-19 05:31:46,997 [slca.py] => Task 7, Epoch 17/20 => Loss 0.032
2023-12-19 05:31:49,227 [slca.py] => Task 7, Epoch 18/20 => Loss 0.047
2023-12-19 05:31:51,455 [slca.py] => Task 7, Epoch 19/20 => Loss 0.014
2023-12-19 05:32:11,225 [slca.py] => Task 7, Epoch 20/20 => Loss 0.056, Train_accy 81.560, Test_accy 61.240
2023-12-19 05:32:45,706 [slca.py] => CA Task 7 => Loss 0.040, Test_accy 66.860
2023-12-19 05:33:15,187 [slca.py] => CA Task 7 => Loss 0.009, Test_accy 67.210
2023-12-19 05:33:44,608 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 67.600
2023-12-19 05:34:14,080 [slca.py] => CA Task 7 => Loss 0.005, Test_accy 67.700
2023-12-19 05:34:43,559 [slca.py] => CA Task 7 => Loss 0.004, Test_accy 67.730
2023-12-19 05:35:00,219 [slca.py] => Exemplar size: 0
2023-12-19 05:35:00,718 [trainer.py] => No NME accuracy.
2023-12-19 05:35:00,718 [trainer.py] => CNN: {'total': 67.73, '00-09': 58.32, '10-19': 50.66, '20-29': 68.24, '30-39': 80.55, '40-49': 68.41, '50-59': 69.43, '60-69': 74.27, '70-79': 71.98, 'old': 67.12, 'new': 71.98}
2023-12-19 05:35:00,718 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09, 78.15, 73.6, 70.87, 69.7, 67.73]
2023-12-19 05:35:00,718 [trainer.py] => CNN top1 avg: 76.205
2023-12-19 05:35:00,718 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11, 96.12, 94.99, 93.44, 92.01, 90.23]

2023-12-19 05:35:00,719 [trainer.py] => All params: 85860176
2023-12-19 05:35:00,719 [trainer.py] => Trainable params: 85860176
2023-12-19 05:35:00,720 [slca.py] => Learning on 80-90
2023-12-19 05:35:03,191 [slca.py] => Task 8, Epoch 1/20 => Loss 2.502
2023-12-19 05:35:05,627 [slca.py] => Task 8, Epoch 2/20 => Loss 1.117
2023-12-19 05:35:07,967 [slca.py] => Task 8, Epoch 3/20 => Loss 0.339
2023-12-19 05:35:10,371 [slca.py] => Task 8, Epoch 4/20 => Loss 0.100
2023-12-19 05:35:32,495 [slca.py] => Task 8, Epoch 5/20 => Loss 0.063, Train_accy 81.940, Test_accy 60.680
2023-12-19 05:35:34,797 [slca.py] => Task 8, Epoch 6/20 => Loss 0.090
2023-12-19 05:35:37,127 [slca.py] => Task 8, Epoch 7/20 => Loss 0.033
2023-12-19 05:35:39,516 [slca.py] => Task 8, Epoch 8/20 => Loss 0.075
2023-12-19 05:35:41,846 [slca.py] => Task 8, Epoch 9/20 => Loss 0.021
2023-12-19 05:36:03,736 [slca.py] => Task 8, Epoch 10/20 => Loss 0.034, Train_accy 82.780, Test_accy 61.630
2023-12-19 05:36:06,077 [slca.py] => Task 8, Epoch 11/20 => Loss 0.003
2023-12-19 05:36:08,325 [slca.py] => Task 8, Epoch 12/20 => Loss 0.037
2023-12-19 05:36:10,657 [slca.py] => Task 8, Epoch 13/20 => Loss 0.015
2023-12-19 05:36:13,041 [slca.py] => Task 8, Epoch 14/20 => Loss 0.061
2023-12-19 05:36:34,969 [slca.py] => Task 8, Epoch 15/20 => Loss 0.016, Train_accy 84.440, Test_accy 61.420
2023-12-19 05:36:37,191 [slca.py] => Task 8, Epoch 16/20 => Loss 0.081
2023-12-19 05:36:39,433 [slca.py] => Task 8, Epoch 17/20 => Loss 0.054
2023-12-19 05:36:41,824 [slca.py] => Task 8, Epoch 18/20 => Loss 0.115
2023-12-19 05:36:44,127 [slca.py] => Task 8, Epoch 19/20 => Loss 0.003
2023-12-19 05:37:05,956 [slca.py] => Task 8, Epoch 20/20 => Loss 0.077, Train_accy 81.670, Test_accy 60.090
2023-12-19 05:37:44,093 [slca.py] => CA Task 8 => Loss 0.039, Test_accy 65.070
2023-12-19 05:38:17,245 [slca.py] => CA Task 8 => Loss 0.009, Test_accy 65.980
2023-12-19 05:38:50,243 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 66.350
2023-12-19 05:39:23,264 [slca.py] => CA Task 8 => Loss 0.005, Test_accy 66.460
2023-12-19 05:39:56,148 [slca.py] => CA Task 8 => Loss 0.004, Test_accy 66.530
2023-12-19 05:40:14,906 [slca.py] => Exemplar size: 0
2023-12-19 05:40:15,381 [trainer.py] => No NME accuracy.
2023-12-19 05:40:15,381 [trainer.py] => CNN: {'total': 66.53, '00-09': 55.98, '10-19': 48.69, '20-29': 64.39, '30-39': 77.81, '40-49': 60.48, '50-59': 70.34, '60-69': 72.85, '70-79': 71.4, '80-89': 76.78, 'old': 65.25, 'new': 76.78}
2023-12-19 05:40:15,381 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09, 78.15, 73.6, 70.87, 69.7, 67.73, 66.53]
2023-12-19 05:40:15,381 [trainer.py] => CNN top1 avg: 75.13
2023-12-19 05:40:15,381 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11, 96.12, 94.99, 93.44, 92.01, 90.23, 90.03]

2023-12-19 05:40:15,382 [trainer.py] => All params: 85867866
2023-12-19 05:40:15,382 [trainer.py] => Trainable params: 85867866
2023-12-19 05:40:15,383 [slca.py] => Learning on 90-100
2023-12-19 05:40:18,150 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 05:40:20,750 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 05:40:23,308 [slca.py] => Task 9, Epoch 3/20 => Loss 0.233
2023-12-19 05:40:25,892 [slca.py] => Task 9, Epoch 4/20 => Loss 0.131
2023-12-19 05:40:50,082 [slca.py] => Task 9, Epoch 5/20 => Loss 0.111, Train_accy 85.750, Test_accy 59.280
2023-12-19 05:40:52,699 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 05:40:55,308 [slca.py] => Task 9, Epoch 7/20 => Loss 0.073
2023-12-19 05:40:57,938 [slca.py] => Task 9, Epoch 8/20 => Loss 0.107
2023-12-19 05:41:00,500 [slca.py] => Task 9, Epoch 9/20 => Loss 0.072
2023-12-19 05:41:24,752 [slca.py] => Task 9, Epoch 10/20 => Loss 0.040, Train_accy 84.250, Test_accy 60.150
2023-12-19 05:41:27,379 [slca.py] => Task 9, Epoch 11/20 => Loss 0.009
2023-12-19 05:41:30,083 [slca.py] => Task 9, Epoch 12/20 => Loss 0.038
2023-12-19 05:41:32,765 [slca.py] => Task 9, Epoch 13/20 => Loss 0.005
2023-12-19 05:41:35,343 [slca.py] => Task 9, Epoch 14/20 => Loss 0.104
2023-12-19 05:41:59,506 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 84.250, Test_accy 60.170
2023-12-19 05:42:02,041 [slca.py] => Task 9, Epoch 16/20 => Loss 0.080
2023-12-19 05:42:04,624 [slca.py] => Task 9, Epoch 17/20 => Loss 0.030
2023-12-19 05:42:07,248 [slca.py] => Task 9, Epoch 18/20 => Loss 0.005
2023-12-19 05:42:09,777 [slca.py] => Task 9, Epoch 19/20 => Loss 0.173
2023-12-19 05:42:34,223 [slca.py] => Task 9, Epoch 20/20 => Loss 0.006, Train_accy 84.250, Test_accy 58.660
2023-12-19 05:43:15,935 [slca.py] => CA Task 9 => Loss 0.040, Test_accy 64.360
2023-12-19 05:43:52,793 [slca.py] => CA Task 9 => Loss 0.009, Test_accy 65.380
2023-12-19 05:44:29,466 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 65.770
2023-12-19 05:45:06,064 [slca.py] => CA Task 9 => Loss 0.004, Test_accy 65.930
2023-12-19 05:45:42,710 [slca.py] => CA Task 9 => Loss 0.005, Test_accy 65.970
2023-12-19 05:46:03,429 [slca.py] => Exemplar size: 0
2023-12-19 05:46:03,914 [trainer.py] => No NME accuracy.
2023-12-19 05:46:03,914 [trainer.py] => CNN: {'total': 65.97, '00-09': 53.05, '10-19': 49.85, '20-29': 64.76, '30-39': 76.85, '40-49': 62.5, '50-59': 68.64, '60-69': 70.87, '70-79': 73.22, '80-89': 74.75, '90-99': 65.2, 'old': 66.05, 'new': 65.2}
2023-12-19 05:46:03,914 [trainer.py] => CNN top1 curve: [91.29, 80.21, 78.09, 78.15, 73.6, 70.87, 69.7, 67.73, 66.53, 65.97]
2023-12-19 05:46:03,914 [trainer.py] => CNN top1 avg: 74.214
2023-12-19 05:46:03,914 [trainer.py] => CNN top5 curve: [99.78, 98.39, 97.11, 96.12, 94.99, 93.44, 92.01, 90.23, 90.03, 90.48]

2023-12-19 05:46:03,915 [trainer.py] => final accs: [65.97]
2023-12-19 05:46:03,915 [trainer.py] => avg accs: [74.214]
