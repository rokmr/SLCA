2023-12-19 07:08:02,100 [trainer.py] => config: /home/gayathri/rohit/SLCA/exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 07:08:02,101 [trainer.py] => test_only: False
2023-12-19 07:08:02,101 [trainer.py] => prefix: reproduce
2023-12-19 07:08:02,101 [trainer.py] => dataset: cifar100_224
2023-12-19 07:08:02,101 [trainer.py] => memory_size: 0
2023-12-19 07:08:02,101 [trainer.py] => memory_per_class: 0
2023-12-19 07:08:02,101 [trainer.py] => fixed_memory: False
2023-12-19 07:08:02,101 [trainer.py] => shuffle: False
2023-12-19 07:08:02,101 [trainer.py] => init_cls: 10
2023-12-19 07:08:02,101 [trainer.py] => increment: 10
2023-12-19 07:08:02,101 [trainer.py] => model_name: slca_cifar
2023-12-19 07:08:02,101 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 07:08:02,101 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 07:08:02,101 [trainer.py] => device: [device(type='cuda', index=1)]
2023-12-19 07:08:02,101 [trainer.py] => seed: 0
2023-12-19 07:08:02,101 [trainer.py] => epochs: 20
2023-12-19 07:08:02,101 [trainer.py] => ca_epochs: 5
2023-12-19 07:08:02,101 [trainer.py] => ca_with_logit_norm: 0.1
2023-12-19 07:08:02,101 [trainer.py] => milestones: [18]
2023-12-19 07:08:02,101 [trainer.py] => lr: 0.005
2023-12-19 07:08:02,101 [trainer.py] => lr_decay: 0.1
2023-12-19 07:08:02,101 [trainer.py] => weight_decay: 0.005
2023-12-19 07:08:02,101 [trainer.py] => u_batch_size: 256
2023-12-19 07:08:02,101 [trainer.py] => s_batch_size: 3
2023-12-19 07:08:02,101 [trainer.py] => multicrop: 2
2023-12-19 07:08:02,101 [trainer.py] => us_multicrop: 2
2023-12-19 07:08:02,101 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 07:08:02,101 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 07:08:02,101 [trainer.py] => buffer_size: 500
2023-12-19 07:08:02,101 [trainer.py] => run_id: 0
2023-12-19 07:08:03,603 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 07:08:05,457 [trainer.py] => All params: 85798656
2023-12-19 07:08:05,457 [trainer.py] => Trainable params: 85798656
2023-12-19 07:08:05,458 [slca.py] => Learning on 0-10
2023-12-19 07:08:10,837 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 07:08:12,184 [slca.py] => Task 0, Epoch 2/20 => Loss 2.382
2023-12-19 07:08:13,240 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 07:08:14,356 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 07:08:19,256 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 39.960
2023-12-19 07:08:20,302 [slca.py] => Task 0, Epoch 6/20 => Loss 1.699
2023-12-19 07:08:21,396 [slca.py] => Task 0, Epoch 7/20 => Loss 1.405
2023-12-19 07:08:22,469 [slca.py] => Task 0, Epoch 8/20 => Loss 1.355
2023-12-19 07:08:23,572 [slca.py] => Task 0, Epoch 9/20 => Loss 1.153
2023-12-19 07:08:28,349 [slca.py] => Task 0, Epoch 10/20 => Loss 0.953, Train_accy 92.500, Test_accy 69.750
2023-12-19 07:08:29,432 [slca.py] => Task 0, Epoch 11/20 => Loss 0.775
2023-12-19 07:08:30,548 [slca.py] => Task 0, Epoch 12/20 => Loss 0.800
2023-12-19 07:08:31,660 [slca.py] => Task 0, Epoch 13/20 => Loss 0.570
2023-12-19 07:08:32,742 [slca.py] => Task 0, Epoch 14/20 => Loss 0.456
2023-12-19 07:08:37,666 [slca.py] => Task 0, Epoch 15/20 => Loss 0.483, Train_accy 95.000, Test_accy 81.360
2023-12-19 07:08:38,818 [slca.py] => Task 0, Epoch 16/20 => Loss 0.325
2023-12-19 07:08:39,915 [slca.py] => Task 0, Epoch 17/20 => Loss 0.352
2023-12-19 07:08:41,021 [slca.py] => Task 0, Epoch 18/20 => Loss 0.292
2023-12-19 07:08:42,180 [slca.py] => Task 0, Epoch 19/20 => Loss 0.214
2023-12-19 07:08:47,073 [slca.py] => Task 0, Epoch 20/20 => Loss 0.297, Train_accy 97.500, Test_accy 84.600
2023-12-19 07:08:55,289 [slca.py] => CA Task 0 => Loss 0.025, Test_accy 85.830
2023-12-19 07:08:58,370 [slca.py] => CA Task 0 => Loss 0.010, Test_accy 86.720
2023-12-19 07:09:01,412 [slca.py] => CA Task 0 => Loss 0.005, Test_accy 87.390
2023-12-19 07:09:04,270 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 87.390
2023-12-19 07:09:07,779 [slca.py] => CA Task 0 => Loss 0.004, Test_accy 87.610
2023-12-19 07:09:10,747 [slca.py] => Exemplar size: 0
2023-12-19 07:09:11,481 [trainer.py] => No NME accuracy.
2023-12-19 07:09:11,481 [trainer.py] => CNN: {'total': 87.61, '00-09': 87.61, 'old': 0, 'new': 87.61}
2023-12-19 07:09:11,481 [trainer.py] => CNN top1 curve: [87.61]
2023-12-19 07:09:11,482 [trainer.py] => CNN top1 avg: 87.61
2023-12-19 07:09:11,482 [trainer.py] => CNN top5 curve: [99.44]

2023-12-19 07:09:11,483 [trainer.py] => All params: 85806346
2023-12-19 07:09:11,484 [trainer.py] => Trainable params: 85806346
2023-12-19 07:09:11,485 [slca.py] => Learning on 10-20
2023-12-19 07:09:12,929 [slca.py] => Task 1, Epoch 1/20 => Loss 2.537
2023-12-19 07:09:14,290 [slca.py] => Task 1, Epoch 2/20 => Loss 2.492
2023-12-19 07:09:15,646 [slca.py] => Task 1, Epoch 3/20 => Loss 2.329
2023-12-19 07:09:17,035 [slca.py] => Task 1, Epoch 4/20 => Loss 2.190
2023-12-19 07:09:24,526 [slca.py] => Task 1, Epoch 5/20 => Loss 1.933, Train_accy 53.750, Test_accy 43.070
2023-12-19 07:09:25,961 [slca.py] => Task 1, Epoch 6/20 => Loss 1.619
2023-12-19 07:09:27,363 [slca.py] => Task 1, Epoch 7/20 => Loss 1.501
2023-12-19 07:09:28,778 [slca.py] => Task 1, Epoch 8/20 => Loss 1.269
2023-12-19 07:09:30,196 [slca.py] => Task 1, Epoch 9/20 => Loss 1.043
2023-12-19 07:09:37,532 [slca.py] => Task 1, Epoch 10/20 => Loss 0.808, Train_accy 81.250, Test_accy 56.560
2023-12-19 07:09:38,948 [slca.py] => Task 1, Epoch 11/20 => Loss 0.744
2023-12-19 07:09:40,374 [slca.py] => Task 1, Epoch 12/20 => Loss 0.606
2023-12-19 07:09:41,759 [slca.py] => Task 1, Epoch 13/20 => Loss 0.421
2023-12-19 07:09:43,162 [slca.py] => Task 1, Epoch 14/20 => Loss 0.460
2023-12-19 07:09:50,698 [slca.py] => Task 1, Epoch 15/20 => Loss 0.390, Train_accy 96.250, Test_accy 67.920
2023-12-19 07:09:52,131 [slca.py] => Task 1, Epoch 16/20 => Loss 0.250
2023-12-19 07:09:53,533 [slca.py] => Task 1, Epoch 17/20 => Loss 0.273
2023-12-19 07:09:54,931 [slca.py] => Task 1, Epoch 18/20 => Loss 0.245
2023-12-19 07:09:56,348 [slca.py] => Task 1, Epoch 19/20 => Loss 0.131
2023-12-19 07:10:03,688 [slca.py] => Task 1, Epoch 20/20 => Loss 0.215, Train_accy 92.500, Test_accy 70.830
2023-12-19 07:10:13,984 [slca.py] => CA Task 1 => Loss 0.031, Test_accy 72.970
2023-12-19 07:10:18,939 [slca.py] => CA Task 1 => Loss 0.009, Test_accy 73.800
2023-12-19 07:10:23,921 [slca.py] => CA Task 1 => Loss 0.005, Test_accy 74.740
2023-12-19 07:10:28,913 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 74.790
2023-12-19 07:10:33,893 [slca.py] => CA Task 1 => Loss 0.003, Test_accy 74.790
2023-12-19 07:10:38,828 [slca.py] => Exemplar size: 0
2023-12-19 07:10:39,552 [trainer.py] => No NME accuracy.
2023-12-19 07:10:39,552 [trainer.py] => CNN: {'total': 74.79, '00-09': 82.19, '10-19': 67.4, 'old': 82.19, 'new': 67.4}
2023-12-19 07:10:39,552 [trainer.py] => CNN top1 curve: [87.61, 74.79]
2023-12-19 07:10:39,553 [trainer.py] => CNN top1 avg: 81.2
2023-12-19 07:10:39,553 [trainer.py] => CNN top5 curve: [99.44, 97.5]

2023-12-19 07:10:39,553 [trainer.py] => All params: 85814036
2023-12-19 07:10:39,554 [trainer.py] => Trainable params: 85814036
2023-12-19 07:10:39,555 [slca.py] => Learning on 20-30
2023-12-19 07:10:41,351 [slca.py] => Task 2, Epoch 1/20 => Loss 2.432
2023-12-19 07:10:43,082 [slca.py] => Task 2, Epoch 2/20 => Loss 2.285
2023-12-19 07:10:44,806 [slca.py] => Task 2, Epoch 3/20 => Loss 2.013
2023-12-19 07:10:46,710 [slca.py] => Task 2, Epoch 4/20 => Loss 1.917
2023-12-19 07:10:56,601 [slca.py] => Task 2, Epoch 5/20 => Loss 1.659, Train_accy 61.670, Test_accy 48.060
2023-12-19 07:10:58,323 [slca.py] => Task 2, Epoch 6/20 => Loss 1.483
2023-12-19 07:11:00,335 [slca.py] => Task 2, Epoch 7/20 => Loss 1.134
2023-12-19 07:11:02,067 [slca.py] => Task 2, Epoch 8/20 => Loss 0.911
2023-12-19 07:11:03,674 [slca.py] => Task 2, Epoch 9/20 => Loss 0.742
2023-12-19 07:11:13,537 [slca.py] => Task 2, Epoch 10/20 => Loss 0.714, Train_accy 81.670, Test_accy 60.050
2023-12-19 07:11:15,262 [slca.py] => Task 2, Epoch 11/20 => Loss 0.476
2023-12-19 07:11:17,032 [slca.py] => Task 2, Epoch 12/20 => Loss 0.291
2023-12-19 07:11:18,763 [slca.py] => Task 2, Epoch 13/20 => Loss 0.227
2023-12-19 07:11:20,474 [slca.py] => Task 2, Epoch 14/20 => Loss 0.301
2023-12-19 07:11:30,295 [slca.py] => Task 2, Epoch 15/20 => Loss 0.300, Train_accy 90.830, Test_accy 68.040
2023-12-19 07:11:32,019 [slca.py] => Task 2, Epoch 16/20 => Loss 0.204
2023-12-19 07:11:33,757 [slca.py] => Task 2, Epoch 17/20 => Loss 0.334
2023-12-19 07:11:35,463 [slca.py] => Task 2, Epoch 18/20 => Loss 0.195
2023-12-19 07:11:37,195 [slca.py] => Task 2, Epoch 19/20 => Loss 0.179
2023-12-19 07:11:47,512 [slca.py] => Task 2, Epoch 20/20 => Loss 0.127, Train_accy 90.830, Test_accy 69.090
2023-12-19 07:12:01,301 [slca.py] => CA Task 2 => Loss 0.038, Test_accy 71.470
2023-12-19 07:12:08,489 [slca.py] => CA Task 2 => Loss 0.008, Test_accy 72.110
2023-12-19 07:12:16,087 [slca.py] => CA Task 2 => Loss 0.005, Test_accy 72.490
2023-12-19 07:12:23,939 [slca.py] => CA Task 2 => Loss 0.004, Test_accy 72.660
2023-12-19 07:12:31,053 [slca.py] => CA Task 2 => Loss 0.004, Test_accy 72.620
2023-12-19 07:12:38,000 [slca.py] => Exemplar size: 0
2023-12-19 07:12:38,745 [trainer.py] => No NME accuracy.
2023-12-19 07:12:38,745 [trainer.py] => CNN: {'total': 72.62, '00-09': 77.71, '10-19': 63.28, '20-29': 76.91, 'old': 70.47, 'new': 76.91}
2023-12-19 07:12:38,745 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62]
2023-12-19 07:12:38,746 [trainer.py] => CNN top1 avg: 78.34
2023-12-19 07:12:38,746 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31]

2023-12-19 07:12:38,746 [trainer.py] => All params: 85821726
2023-12-19 07:12:38,747 [trainer.py] => Trainable params: 85821726
2023-12-19 07:12:38,748 [slca.py] => Learning on 30-40
2023-12-19 07:12:40,839 [slca.py] => Task 3, Epoch 1/20 => Loss 2.328
2023-12-19 07:12:42,770 [slca.py] => Task 3, Epoch 2/20 => Loss 2.067
2023-12-19 07:12:44,685 [slca.py] => Task 3, Epoch 3/20 => Loss 1.681
2023-12-19 07:12:46,717 [slca.py] => Task 3, Epoch 4/20 => Loss 1.381
2023-12-19 07:13:00,936 [slca.py] => Task 3, Epoch 5/20 => Loss 0.856, Train_accy 79.380, Test_accy 59.880
2023-12-19 07:13:02,998 [slca.py] => Task 3, Epoch 6/20 => Loss 0.748
2023-12-19 07:13:05,006 [slca.py] => Task 3, Epoch 7/20 => Loss 0.425
2023-12-19 07:13:06,914 [slca.py] => Task 3, Epoch 8/20 => Loss 0.192
2023-12-19 07:13:08,881 [slca.py] => Task 3, Epoch 9/20 => Loss 0.337
2023-12-19 07:13:21,329 [slca.py] => Task 3, Epoch 10/20 => Loss 0.177, Train_accy 81.250, Test_accy 68.670
2023-12-19 07:13:23,296 [slca.py] => Task 3, Epoch 11/20 => Loss 0.239
2023-12-19 07:13:25,244 [slca.py] => Task 3, Epoch 12/20 => Loss 0.114
2023-12-19 07:13:27,235 [slca.py] => Task 3, Epoch 13/20 => Loss 0.067
2023-12-19 07:13:29,249 [slca.py] => Task 3, Epoch 14/20 => Loss 0.069
2023-12-19 07:13:43,302 [slca.py] => Task 3, Epoch 15/20 => Loss 0.119, Train_accy 85.620, Test_accy 69.080
2023-12-19 07:13:45,243 [slca.py] => Task 3, Epoch 16/20 => Loss 0.117
2023-12-19 07:13:47,183 [slca.py] => Task 3, Epoch 17/20 => Loss 0.080
2023-12-19 07:13:49,142 [slca.py] => Task 3, Epoch 18/20 => Loss 0.147
2023-12-19 07:13:51,046 [slca.py] => Task 3, Epoch 19/20 => Loss 0.051
2023-12-19 07:14:03,607 [slca.py] => Task 3, Epoch 20/20 => Loss 0.036, Train_accy 86.880, Test_accy 68.220
2023-12-19 07:14:18,930 [slca.py] => CA Task 3 => Loss 0.055, Test_accy 73.210
2023-12-19 07:14:28,329 [slca.py] => CA Task 3 => Loss 0.010, Test_accy 74.240
2023-12-19 07:14:37,654 [slca.py] => CA Task 3 => Loss 0.006, Test_accy 74.550
2023-12-19 07:14:48,538 [slca.py] => CA Task 3 => Loss 0.005, Test_accy 74.670
2023-12-19 07:14:57,971 [slca.py] => CA Task 3 => Loss 0.004, Test_accy 74.700
2023-12-19 07:15:08,261 [slca.py] => Exemplar size: 0
2023-12-19 07:15:08,969 [trainer.py] => No NME accuracy.
2023-12-19 07:15:08,970 [trainer.py] => CNN: {'total': 74.7, '00-09': 72.53, '10-19': 60.89, '20-29': 78.63, '30-39': 86.72, 'old': 70.68, 'new': 86.72}
2023-12-19 07:15:08,970 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62, 74.7]
2023-12-19 07:15:08,970 [trainer.py] => CNN top1 avg: 77.43
2023-12-19 07:15:08,970 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31, 94.88]

2023-12-19 07:15:08,972 [trainer.py] => All params: 85829416
2023-12-19 07:15:08,973 [trainer.py] => Trainable params: 85829416
2023-12-19 07:15:08,974 [slca.py] => Learning on 40-50
2023-12-19 07:15:11,148 [slca.py] => Task 4, Epoch 1/20 => Loss 2.302
2023-12-19 07:15:13,296 [slca.py] => Task 4, Epoch 2/20 => Loss 1.964
2023-12-19 07:15:15,462 [slca.py] => Task 4, Epoch 3/20 => Loss 1.456
2023-12-19 07:15:17,606 [slca.py] => Task 4, Epoch 4/20 => Loss 0.965
2023-12-19 07:15:32,206 [slca.py] => Task 4, Epoch 5/20 => Loss 0.498, Train_accy 79.500, Test_accy 59.580
2023-12-19 07:15:34,329 [slca.py] => Task 4, Epoch 6/20 => Loss 0.381
2023-12-19 07:15:36,436 [slca.py] => Task 4, Epoch 7/20 => Loss 0.185
2023-12-19 07:15:38,655 [slca.py] => Task 4, Epoch 8/20 => Loss 0.332
2023-12-19 07:15:40,788 [slca.py] => Task 4, Epoch 9/20 => Loss 0.142
2023-12-19 07:15:55,471 [slca.py] => Task 4, Epoch 10/20 => Loss 0.180, Train_accy 82.000, Test_accy 65.020
2023-12-19 07:15:57,689 [slca.py] => Task 4, Epoch 11/20 => Loss 0.182
2023-12-19 07:15:59,861 [slca.py] => Task 4, Epoch 12/20 => Loss 0.081
2023-12-19 07:16:02,070 [slca.py] => Task 4, Epoch 13/20 => Loss 0.112
2023-12-19 07:16:04,284 [slca.py] => Task 4, Epoch 14/20 => Loss 0.026
2023-12-19 07:16:19,029 [slca.py] => Task 4, Epoch 15/20 => Loss 0.120, Train_accy 81.500, Test_accy 63.580
2023-12-19 07:16:21,182 [slca.py] => Task 4, Epoch 16/20 => Loss 0.108
2023-12-19 07:16:23,325 [slca.py] => Task 4, Epoch 17/20 => Loss 0.078
2023-12-19 07:16:25,488 [slca.py] => Task 4, Epoch 18/20 => Loss 0.095
2023-12-19 07:16:27,742 [slca.py] => Task 4, Epoch 19/20 => Loss 0.025
2023-12-19 07:16:42,484 [slca.py] => Task 4, Epoch 20/20 => Loss 0.128, Train_accy 82.000, Test_accy 62.680
2023-12-19 07:16:59,759 [slca.py] => CA Task 4 => Loss 0.049, Test_accy 69.330
2023-12-19 07:17:10,964 [slca.py] => CA Task 4 => Loss 0.009, Test_accy 69.990
2023-12-19 07:17:23,257 [slca.py] => CA Task 4 => Loss 0.007, Test_accy 70.330
2023-12-19 07:17:34,758 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 70.390
2023-12-19 07:17:46,265 [slca.py] => CA Task 4 => Loss 0.005, Test_accy 70.370
2023-12-19 07:17:57,432 [slca.py] => Exemplar size: 0
2023-12-19 07:17:58,121 [trainer.py] => No NME accuracy.
2023-12-19 07:17:58,122 [trainer.py] => CNN: {'total': 70.37, '00-09': 67.64, '10-19': 56.66, '20-29': 69.54, '30-39': 82.05, '40-49': 76.0, 'old': 68.96, 'new': 76.0}
2023-12-19 07:17:58,122 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62, 74.7, 70.37]
2023-12-19 07:17:58,122 [trainer.py] => CNN top1 avg: 76.018
2023-12-19 07:17:58,122 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31, 94.88, 93.65]

2023-12-19 07:17:58,124 [trainer.py] => All params: 85837106
2023-12-19 07:17:58,125 [trainer.py] => Trainable params: 85837106
2023-12-19 07:17:58,127 [slca.py] => Learning on 50-60
2023-12-19 07:18:00,567 [slca.py] => Task 5, Epoch 1/20 => Loss 2.435
2023-12-19 07:18:02,897 [slca.py] => Task 5, Epoch 2/20 => Loss 2.053
2023-12-19 07:18:05,263 [slca.py] => Task 5, Epoch 3/20 => Loss 1.432
2023-12-19 07:18:07,606 [slca.py] => Task 5, Epoch 4/20 => Loss 0.986
2023-12-19 07:18:24,510 [slca.py] => Task 5, Epoch 5/20 => Loss 0.540, Train_accy 73.330, Test_accy 56.130
2023-12-19 07:18:26,965 [slca.py] => Task 5, Epoch 6/20 => Loss 0.350
2023-12-19 07:18:29,313 [slca.py] => Task 5, Epoch 7/20 => Loss 0.234
2023-12-19 07:18:31,698 [slca.py] => Task 5, Epoch 8/20 => Loss 0.137
2023-12-19 07:18:34,013 [slca.py] => Task 5, Epoch 9/20 => Loss 0.278
2023-12-19 07:18:50,802 [slca.py] => Task 5, Epoch 10/20 => Loss 0.110, Train_accy 84.170, Test_accy 60.840
2023-12-19 07:18:53,156 [slca.py] => Task 5, Epoch 11/20 => Loss 0.178
2023-12-19 07:18:55,484 [slca.py] => Task 5, Epoch 12/20 => Loss 0.051
2023-12-19 07:18:57,811 [slca.py] => Task 5, Epoch 13/20 => Loss 0.046
2023-12-19 07:19:00,209 [slca.py] => Task 5, Epoch 14/20 => Loss 0.150
2023-12-19 07:19:18,737 [slca.py] => Task 5, Epoch 15/20 => Loss 0.041, Train_accy 85.830, Test_accy 60.800
2023-12-19 07:19:21,074 [slca.py] => Task 5, Epoch 16/20 => Loss 0.070
2023-12-19 07:19:23,452 [slca.py] => Task 5, Epoch 17/20 => Loss 0.027
2023-12-19 07:19:25,803 [slca.py] => Task 5, Epoch 18/20 => Loss 0.031
2023-12-19 07:19:28,140 [slca.py] => Task 5, Epoch 19/20 => Loss 0.069
2023-12-19 07:19:45,060 [slca.py] => Task 5, Epoch 20/20 => Loss 0.023, Train_accy 81.250, Test_accy 60.430
2023-12-19 07:20:04,334 [slca.py] => CA Task 5 => Loss 0.053, Test_accy 68.040
2023-12-19 07:20:17,851 [slca.py] => CA Task 5 => Loss 0.011, Test_accy 68.680
2023-12-19 07:20:31,415 [slca.py] => CA Task 5 => Loss 0.007, Test_accy 68.990
2023-12-19 07:20:46,398 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 69.110
2023-12-19 07:21:00,013 [slca.py] => CA Task 5 => Loss 0.006, Test_accy 69.110
2023-12-19 07:21:13,044 [slca.py] => Exemplar size: 0
2023-12-19 07:21:13,810 [trainer.py] => No NME accuracy.
2023-12-19 07:21:13,810 [trainer.py] => CNN: {'total': 69.11, '00-09': 65.45, '10-19': 54.56, '20-29': 67.34, '30-39': 80.76, '40-49': 70.71, '50-59': 76.0, 'old': 67.74, 'new': 76.0}
2023-12-19 07:21:13,810 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62, 74.7, 70.37, 69.11]
2023-12-19 07:21:13,811 [trainer.py] => CNN top1 avg: 74.86666666666667
2023-12-19 07:21:13,811 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31, 94.88, 93.65, 92.53]

2023-12-19 07:21:13,812 [trainer.py] => All params: 85844796
2023-12-19 07:21:13,814 [trainer.py] => Trainable params: 85844796
2023-12-19 07:21:13,816 [slca.py] => Learning on 60-70
2023-12-19 07:21:16,827 [slca.py] => Task 6, Epoch 1/20 => Loss 2.242
2023-12-19 07:21:19,625 [slca.py] => Task 6, Epoch 2/20 => Loss 1.336
2023-12-19 07:21:22,331 [slca.py] => Task 6, Epoch 3/20 => Loss 0.915
2023-12-19 07:21:24,985 [slca.py] => Task 6, Epoch 4/20 => Loss 0.798
2023-12-19 07:21:45,713 [slca.py] => Task 6, Epoch 5/20 => Loss 0.289, Train_accy 79.640, Test_accy 58.420
2023-12-19 07:21:48,390 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 07:21:50,996 [slca.py] => Task 6, Epoch 7/20 => Loss 0.144
2023-12-19 07:21:53,580 [slca.py] => Task 6, Epoch 8/20 => Loss 0.074
2023-12-19 07:21:56,240 [slca.py] => Task 6, Epoch 9/20 => Loss 0.128
2023-12-19 07:22:15,911 [slca.py] => Task 6, Epoch 10/20 => Loss 0.054, Train_accy 85.710, Test_accy 59.720
2023-12-19 07:22:18,461 [slca.py] => Task 6, Epoch 11/20 => Loss 0.045
2023-12-19 07:22:21,046 [slca.py] => Task 6, Epoch 12/20 => Loss 0.111
2023-12-19 07:22:23,614 [slca.py] => Task 6, Epoch 13/20 => Loss 0.073
2023-12-19 07:22:26,213 [slca.py] => Task 6, Epoch 14/20 => Loss 0.094
2023-12-19 07:22:45,465 [slca.py] => Task 6, Epoch 15/20 => Loss 0.056, Train_accy 79.640, Test_accy 59.740
2023-12-19 07:22:48,260 [slca.py] => Task 6, Epoch 16/20 => Loss 0.151
2023-12-19 07:22:51,127 [slca.py] => Task 6, Epoch 17/20 => Loss 0.084
2023-12-19 07:22:53,929 [slca.py] => Task 6, Epoch 18/20 => Loss 0.229
2023-12-19 07:22:56,621 [slca.py] => Task 6, Epoch 19/20 => Loss 0.020
2023-12-19 07:23:15,969 [slca.py] => Task 6, Epoch 20/20 => Loss 0.040, Train_accy 81.070, Test_accy 59.270
2023-12-19 07:23:36,977 [slca.py] => CA Task 6 => Loss 0.056, Test_accy 67.450
2023-12-19 07:23:52,855 [slca.py] => CA Task 6 => Loss 0.011, Test_accy 68.390
2023-12-19 07:24:08,764 [slca.py] => CA Task 6 => Loss 0.008, Test_accy 68.710
2023-12-19 07:24:24,445 [slca.py] => CA Task 6 => Loss 0.007, Test_accy 68.750
2023-12-19 07:24:40,212 [slca.py] => CA Task 6 => Loss 0.005, Test_accy 68.780
2023-12-19 07:24:55,229 [slca.py] => Exemplar size: 0
2023-12-19 07:24:55,943 [trainer.py] => No NME accuracy.
2023-12-19 07:24:55,943 [trainer.py] => CNN: {'total': 68.78, '00-09': 62.39, '10-19': 54.61, '20-29': 67.34, '30-39': 80.57, '40-49': 67.64, '50-59': 72.31, '60-69': 76.65, 'old': 67.47, 'new': 76.65}
2023-12-19 07:24:55,943 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62, 74.7, 70.37, 69.11, 68.78]
2023-12-19 07:24:55,943 [trainer.py] => CNN top1 avg: 73.99714285714286
2023-12-19 07:24:55,943 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31, 94.88, 93.65, 92.53, 91.33]

2023-12-19 07:24:55,944 [trainer.py] => All params: 85852486
2023-12-19 07:24:55,944 [trainer.py] => Trainable params: 85852486
2023-12-19 07:24:55,945 [slca.py] => Learning on 70-80
2023-12-19 07:24:58,768 [slca.py] => Task 7, Epoch 1/20 => Loss 2.442
2023-12-19 07:25:01,564 [slca.py] => Task 7, Epoch 2/20 => Loss 1.757
2023-12-19 07:25:04,358 [slca.py] => Task 7, Epoch 3/20 => Loss 1.055
2023-12-19 07:25:07,098 [slca.py] => Task 7, Epoch 4/20 => Loss 0.490
2023-12-19 07:25:29,234 [slca.py] => Task 7, Epoch 5/20 => Loss 0.342, Train_accy 76.880, Test_accy 56.510
2023-12-19 07:25:32,027 [slca.py] => Task 7, Epoch 6/20 => Loss 0.233
2023-12-19 07:25:34,790 [slca.py] => Task 7, Epoch 7/20 => Loss 0.188
2023-12-19 07:25:37,589 [slca.py] => Task 7, Epoch 8/20 => Loss 0.219
2023-12-19 07:25:40,391 [slca.py] => Task 7, Epoch 9/20 => Loss 0.090
2023-12-19 07:26:02,512 [slca.py] => Task 7, Epoch 10/20 => Loss 0.196, Train_accy 83.440, Test_accy 59.190
2023-12-19 07:26:05,265 [slca.py] => Task 7, Epoch 11/20 => Loss 0.201
2023-12-19 07:26:08,058 [slca.py] => Task 7, Epoch 12/20 => Loss 0.142
2023-12-19 07:26:10,847 [slca.py] => Task 7, Epoch 13/20 => Loss 0.130
2023-12-19 07:26:13,658 [slca.py] => Task 7, Epoch 14/20 => Loss 0.082
2023-12-19 07:26:35,302 [slca.py] => Task 7, Epoch 15/20 => Loss 0.033, Train_accy 79.690, Test_accy 59.000
2023-12-19 07:26:38,049 [slca.py] => Task 7, Epoch 16/20 => Loss 0.103
2023-12-19 07:26:40,936 [slca.py] => Task 7, Epoch 17/20 => Loss 0.032
2023-12-19 07:26:43,709 [slca.py] => Task 7, Epoch 18/20 => Loss 0.036
2023-12-19 07:26:46,498 [slca.py] => Task 7, Epoch 19/20 => Loss 0.032
2023-12-19 07:27:08,167 [slca.py] => Task 7, Epoch 20/20 => Loss 0.052, Train_accy 78.440, Test_accy 58.440
2023-12-19 07:27:31,338 [slca.py] => CA Task 7 => Loss 0.055, Test_accy 65.660
2023-12-19 07:27:49,457 [slca.py] => CA Task 7 => Loss 0.011, Test_accy 66.570
2023-12-19 07:28:08,151 [slca.py] => CA Task 7 => Loss 0.008, Test_accy 66.950
2023-12-19 07:28:26,155 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 67.300
2023-12-19 07:28:47,014 [slca.py] => CA Task 7 => Loss 0.006, Test_accy 67.260
2023-12-19 07:29:04,101 [slca.py] => Exemplar size: 0
2023-12-19 07:29:04,822 [trainer.py] => No NME accuracy.
2023-12-19 07:29:04,823 [trainer.py] => CNN: {'total': 67.26, '00-09': 56.41, '10-19': 51.77, '20-29': 60.72, '30-39': 79.43, '40-49': 67.51, '50-59': 73.08, '60-69': 72.86, '70-79': 76.41, 'old': 65.96, 'new': 76.41}
2023-12-19 07:29:04,823 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62, 74.7, 70.37, 69.11, 68.78, 67.26]
2023-12-19 07:29:04,823 [trainer.py] => CNN top1 avg: 73.155
2023-12-19 07:29:04,823 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31, 94.88, 93.65, 92.53, 91.33, 90.4]

2023-12-19 07:29:04,824 [trainer.py] => All params: 85860176
2023-12-19 07:29:04,825 [trainer.py] => Trainable params: 85860176
2023-12-19 07:29:04,826 [slca.py] => Learning on 80-90
2023-12-19 07:29:07,925 [slca.py] => Task 8, Epoch 1/20 => Loss 2.424
2023-12-19 07:29:10,909 [slca.py] => Task 8, Epoch 2/20 => Loss 1.635
2023-12-19 07:29:13,935 [slca.py] => Task 8, Epoch 3/20 => Loss 0.867
2023-12-19 07:29:16,906 [slca.py] => Task 8, Epoch 4/20 => Loss 0.298
2023-12-19 07:29:42,618 [slca.py] => Task 8, Epoch 5/20 => Loss 0.193, Train_accy 78.610, Test_accy 57.290
2023-12-19 07:29:45,666 [slca.py] => Task 8, Epoch 6/20 => Loss 0.264
2023-12-19 07:29:48,665 [slca.py] => Task 8, Epoch 7/20 => Loss 0.088
2023-12-19 07:29:51,690 [slca.py] => Task 8, Epoch 8/20 => Loss 0.071
2023-12-19 07:29:54,720 [slca.py] => Task 8, Epoch 9/20 => Loss 0.041
2023-12-19 07:30:18,755 [slca.py] => Task 8, Epoch 10/20 => Loss 0.072, Train_accy 80.000, Test_accy 59.420
2023-12-19 07:30:21,787 [slca.py] => Task 8, Epoch 11/20 => Loss 0.015
2023-12-19 07:30:24,733 [slca.py] => Task 8, Epoch 12/20 => Loss 0.043
2023-12-19 07:30:27,728 [slca.py] => Task 8, Epoch 13/20 => Loss 0.020
2023-12-19 07:30:31,690 [slca.py] => Task 8, Epoch 14/20 => Loss 0.083
2023-12-19 07:30:57,630 [slca.py] => Task 8, Epoch 15/20 => Loss 0.014, Train_accy 83.060, Test_accy 59.390
2023-12-19 07:31:00,651 [slca.py] => Task 8, Epoch 16/20 => Loss 0.127
2023-12-19 07:31:03,590 [slca.py] => Task 8, Epoch 17/20 => Loss 0.049
2023-12-19 07:31:06,553 [slca.py] => Task 8, Epoch 18/20 => Loss 0.096
2023-12-19 07:31:09,548 [slca.py] => Task 8, Epoch 19/20 => Loss 0.013
2023-12-19 07:31:34,064 [slca.py] => Task 8, Epoch 20/20 => Loss 0.094, Train_accy 81.110, Test_accy 59.410
2023-12-19 07:32:00,164 [slca.py] => CA Task 8 => Loss 0.052, Test_accy 66.040
2023-12-19 07:32:21,563 [slca.py] => CA Task 8 => Loss 0.013, Test_accy 67.240
2023-12-19 07:32:41,900 [slca.py] => CA Task 8 => Loss 0.008, Test_accy 67.590
2023-12-19 07:33:03,419 [slca.py] => CA Task 8 => Loss 0.007, Test_accy 67.670
2023-12-19 07:33:23,676 [slca.py] => CA Task 8 => Loss 0.005, Test_accy 67.710
2023-12-19 07:33:43,263 [slca.py] => Exemplar size: 0
2023-12-19 07:33:43,977 [trainer.py] => No NME accuracy.
2023-12-19 07:33:43,978 [trainer.py] => CNN: {'total': 67.71, '00-09': 55.88, '10-19': 50.4, '20-29': 62.19, '30-39': 78.61, '40-49': 63.1, '50-59': 72.95, '60-69': 70.34, '70-79': 74.62, '80-89': 81.31, 'old': 66.01, 'new': 81.31}
2023-12-19 07:33:43,978 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62, 74.7, 70.37, 69.11, 68.78, 67.26, 67.71]
2023-12-19 07:33:43,978 [trainer.py] => CNN top1 avg: 72.55000000000001
2023-12-19 07:33:43,978 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31, 94.88, 93.65, 92.53, 91.33, 90.4, 90.46]

2023-12-19 07:33:43,979 [trainer.py] => All params: 85867866
2023-12-19 07:33:43,980 [trainer.py] => Trainable params: 85867866
2023-12-19 07:33:43,981 [slca.py] => Learning on 90-100
2023-12-19 07:33:47,280 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 07:33:50,579 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 07:33:53,759 [slca.py] => Task 9, Epoch 3/20 => Loss 0.626
2023-12-19 07:33:56,973 [slca.py] => Task 9, Epoch 4/20 => Loss 0.270
2023-12-19 07:34:23,105 [slca.py] => Task 9, Epoch 5/20 => Loss 0.118, Train_accy 81.000, Test_accy 56.790
2023-12-19 07:34:26,551 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 07:34:29,737 [slca.py] => Task 9, Epoch 7/20 => Loss 0.108
2023-12-19 07:34:32,919 [slca.py] => Task 9, Epoch 8/20 => Loss 0.114
2023-12-19 07:34:36,108 [slca.py] => Task 9, Epoch 9/20 => Loss 0.094
2023-12-19 07:35:02,971 [slca.py] => Task 9, Epoch 10/20 => Loss 0.010, Train_accy 81.750, Test_accy 58.190
2023-12-19 07:35:06,263 [slca.py] => Task 9, Epoch 11/20 => Loss 0.020
2023-12-19 07:35:09,470 [slca.py] => Task 9, Epoch 12/20 => Loss 0.027
2023-12-19 07:35:12,737 [slca.py] => Task 9, Epoch 13/20 => Loss 0.026
2023-12-19 07:35:15,918 [slca.py] => Task 9, Epoch 14/20 => Loss 0.062
2023-12-19 07:35:42,605 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 82.000, Test_accy 57.770
2023-12-19 07:35:45,857 [slca.py] => Task 9, Epoch 16/20 => Loss 0.037
2023-12-19 07:35:49,080 [slca.py] => Task 9, Epoch 17/20 => Loss 0.061
2023-12-19 07:35:52,301 [slca.py] => Task 9, Epoch 18/20 => Loss 0.008
2023-12-19 07:35:55,557 [slca.py] => Task 9, Epoch 19/20 => Loss 0.058
2023-12-19 07:36:21,877 [slca.py] => Task 9, Epoch 20/20 => Loss 0.049, Train_accy 80.750, Test_accy 58.040
2023-12-19 07:36:49,758 [slca.py] => CA Task 9 => Loss 0.052, Test_accy 64.640
2023-12-19 07:37:12,413 [slca.py] => CA Task 9 => Loss 0.012, Test_accy 65.410
2023-12-19 07:37:34,936 [slca.py] => CA Task 9 => Loss 0.008, Test_accy 65.970
2023-12-19 07:37:57,493 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 66.130
2023-12-19 07:38:21,390 [slca.py] => CA Task 9 => Loss 0.006, Test_accy 66.120
2023-12-19 07:38:42,624 [slca.py] => Exemplar size: 0
2023-12-19 07:38:43,348 [trainer.py] => No NME accuracy.
2023-12-19 07:38:43,348 [trainer.py] => CNN: {'total': 66.12, '00-09': 51.65, '10-19': 50.95, '20-29': 61.66, '30-39': 74.65, '40-49': 62.2, '50-59': 70.54, '60-69': 71.47, '70-79': 71.72, '80-89': 79.16, '90-99': 67.2, 'old': 66.0, 'new': 67.2}
2023-12-19 07:38:43,348 [trainer.py] => CNN top1 curve: [87.61, 74.79, 72.62, 74.7, 70.37, 69.11, 68.78, 67.26, 67.71, 66.12]
2023-12-19 07:38:43,349 [trainer.py] => CNN top1 avg: 71.90700000000001
2023-12-19 07:38:43,349 [trainer.py] => CNN top5 curve: [99.44, 97.5, 95.31, 94.88, 93.65, 92.53, 91.33, 90.4, 90.46, 90.09]

2023-12-19 07:38:43,349 [trainer.py] => final accs: [66.12]
2023-12-19 07:38:43,349 [trainer.py] => avg accs: [71.90700000000001]
