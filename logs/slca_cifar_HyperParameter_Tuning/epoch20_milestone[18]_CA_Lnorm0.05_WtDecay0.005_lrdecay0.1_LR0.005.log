2023-12-19 07:49:43,872 [trainer.py] => config: exps/slca_cifar100-0.8%_buffer500.json
2023-12-19 07:49:43,872 [trainer.py] => test_only: False
2023-12-19 07:49:43,872 [trainer.py] => prefix: reproduce
2023-12-19 07:49:43,873 [trainer.py] => dataset: cifar100_224
2023-12-19 07:49:43,873 [trainer.py] => memory_size: 0
2023-12-19 07:49:43,873 [trainer.py] => memory_per_class: 0
2023-12-19 07:49:43,873 [trainer.py] => fixed_memory: False
2023-12-19 07:49:43,873 [trainer.py] => shuffle: False
2023-12-19 07:49:43,873 [trainer.py] => init_cls: 10
2023-12-19 07:49:43,873 [trainer.py] => increment: 10
2023-12-19 07:49:43,873 [trainer.py] => model_name: slca_cifar
2023-12-19 07:49:43,873 [trainer.py] => model_postfix: HyperParameter_Tuning
2023-12-19 07:49:43,873 [trainer.py] => convnet_type: vit-b-p16
2023-12-19 07:49:43,873 [trainer.py] => device: [device(type='cuda', index=0)]
2023-12-19 07:49:43,873 [trainer.py] => seed: 0
2023-12-19 07:49:43,873 [trainer.py] => epochs: 20
2023-12-19 07:49:43,873 [trainer.py] => ca_epochs: 5
2023-12-19 07:49:43,873 [trainer.py] => ca_with_logit_norm: 0.05
2023-12-19 07:49:43,873 [trainer.py] => milestones: [18]
2023-12-19 07:49:43,873 [trainer.py] => lr: 0.005
2023-12-19 07:49:43,873 [trainer.py] => lr_decay: 0.1
2023-12-19 07:49:43,873 [trainer.py] => weight_decay: 0.005
2023-12-19 07:49:43,873 [trainer.py] => u_batch_size: 256
2023-12-19 07:49:43,873 [trainer.py] => s_batch_size: 3
2023-12-19 07:49:43,873 [trainer.py] => multicrop: 2
2023-12-19 07:49:43,873 [trainer.py] => us_multicrop: 2
2023-12-19 07:49:43,873 [trainer.py] => subset_path: ./subsets/cifar100/0.8%_seed0.txt
2023-12-19 07:49:43,873 [trainer.py] => subset_path_cls: ./subsets/cifar100/0.8%_seed0_cls.txt
2023-12-19 07:49:43,873 [trainer.py] => buffer_size: 500
2023-12-19 07:49:43,873 [trainer.py] => run_id: 0
2023-12-19 07:49:45,475 [data_manager.py] => [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]
2023-12-19 07:49:47,548 [trainer.py] => All params: 85798656
2023-12-19 07:49:47,549 [trainer.py] => Trainable params: 85798656
2023-12-19 07:49:47,549 [slca.py] => Learning on 0-10
2023-12-19 07:49:54,252 [slca.py] => Task 0, Epoch 1/20 => Loss 2.357
2023-12-19 07:49:55,381 [slca.py] => Task 0, Epoch 2/20 => Loss 2.382
2023-12-19 07:49:56,560 [slca.py] => Task 0, Epoch 3/20 => Loss 2.274
2023-12-19 07:49:57,768 [slca.py] => Task 0, Epoch 4/20 => Loss 2.058
2023-12-19 07:50:03,041 [slca.py] => Task 0, Epoch 5/20 => Loss 1.883, Train_accy 57.500, Test_accy 39.960
2023-12-19 07:50:04,240 [slca.py] => Task 0, Epoch 6/20 => Loss 1.699
2023-12-19 07:50:05,363 [slca.py] => Task 0, Epoch 7/20 => Loss 1.405
2023-12-19 07:50:06,513 [slca.py] => Task 0, Epoch 8/20 => Loss 1.355
2023-12-19 07:50:07,638 [slca.py] => Task 0, Epoch 9/20 => Loss 1.153
2023-12-19 07:50:12,804 [slca.py] => Task 0, Epoch 10/20 => Loss 0.953, Train_accy 92.500, Test_accy 69.750
2023-12-19 07:50:13,983 [slca.py] => Task 0, Epoch 11/20 => Loss 0.775
2023-12-19 07:50:15,154 [slca.py] => Task 0, Epoch 12/20 => Loss 0.800
2023-12-19 07:50:16,285 [slca.py] => Task 0, Epoch 13/20 => Loss 0.570
2023-12-19 07:50:17,440 [slca.py] => Task 0, Epoch 14/20 => Loss 0.456
2023-12-19 07:50:22,494 [slca.py] => Task 0, Epoch 15/20 => Loss 0.483, Train_accy 95.000, Test_accy 81.360
2023-12-19 07:50:23,665 [slca.py] => Task 0, Epoch 16/20 => Loss 0.325
2023-12-19 07:50:24,841 [slca.py] => Task 0, Epoch 17/20 => Loss 0.352
2023-12-19 07:50:26,053 [slca.py] => Task 0, Epoch 18/20 => Loss 0.292
2023-12-19 07:50:27,291 [slca.py] => Task 0, Epoch 19/20 => Loss 0.214
2023-12-19 07:50:32,445 [slca.py] => Task 0, Epoch 20/20 => Loss 0.297, Train_accy 97.500, Test_accy 84.600
2023-12-19 07:50:40,964 [slca.py] => CA Task 0 => Loss 0.012, Test_accy 85.270
2023-12-19 07:50:44,086 [slca.py] => CA Task 0 => Loss 0.002, Test_accy 86.380
2023-12-19 07:50:47,162 [slca.py] => CA Task 0 => Loss 0.001, Test_accy 86.610
2023-12-19 07:50:50,136 [slca.py] => CA Task 0 => Loss 0.001, Test_accy 86.720
2023-12-19 07:50:53,252 [slca.py] => CA Task 0 => Loss 0.001, Test_accy 86.610
2023-12-19 07:50:56,176 [slca.py] => Exemplar size: 0
2023-12-19 07:50:56,971 [trainer.py] => No NME accuracy.
2023-12-19 07:50:56,971 [trainer.py] => CNN: {'total': 86.61, '00-09': 86.61, 'old': 0, 'new': 86.61}
2023-12-19 07:50:56,971 [trainer.py] => CNN top1 curve: [86.61]
2023-12-19 07:50:56,971 [trainer.py] => CNN top1 avg: 86.61
2023-12-19 07:50:56,971 [trainer.py] => CNN top5 curve: [99.67]

2023-12-19 07:50:56,972 [trainer.py] => All params: 85806346
2023-12-19 07:50:56,972 [trainer.py] => Trainable params: 85806346
2023-12-19 07:50:56,973 [slca.py] => Learning on 10-20
2023-12-19 07:50:58,538 [slca.py] => Task 1, Epoch 1/20 => Loss 2.537
2023-12-19 07:51:00,154 [slca.py] => Task 1, Epoch 2/20 => Loss 2.492
2023-12-19 07:51:01,730 [slca.py] => Task 1, Epoch 3/20 => Loss 2.329
2023-12-19 07:51:03,172 [slca.py] => Task 1, Epoch 4/20 => Loss 2.190
2023-12-19 07:51:10,974 [slca.py] => Task 1, Epoch 5/20 => Loss 1.933, Train_accy 53.750, Test_accy 43.070
2023-12-19 07:51:12,464 [slca.py] => Task 1, Epoch 6/20 => Loss 1.619
2023-12-19 07:51:14,018 [slca.py] => Task 1, Epoch 7/20 => Loss 1.501
2023-12-19 07:51:15,580 [slca.py] => Task 1, Epoch 8/20 => Loss 1.269
2023-12-19 07:51:17,157 [slca.py] => Task 1, Epoch 9/20 => Loss 1.043
2023-12-19 07:51:24,836 [slca.py] => Task 1, Epoch 10/20 => Loss 0.808, Train_accy 81.250, Test_accy 56.560
2023-12-19 07:51:26,380 [slca.py] => Task 1, Epoch 11/20 => Loss 0.744
2023-12-19 07:51:27,882 [slca.py] => Task 1, Epoch 12/20 => Loss 0.606
2023-12-19 07:51:29,356 [slca.py] => Task 1, Epoch 13/20 => Loss 0.421
2023-12-19 07:51:30,847 [slca.py] => Task 1, Epoch 14/20 => Loss 0.460
2023-12-19 07:51:38,650 [slca.py] => Task 1, Epoch 15/20 => Loss 0.390, Train_accy 96.250, Test_accy 67.920
2023-12-19 07:51:40,229 [slca.py] => Task 1, Epoch 16/20 => Loss 0.250
2023-12-19 07:51:41,684 [slca.py] => Task 1, Epoch 17/20 => Loss 0.273
2023-12-19 07:51:43,226 [slca.py] => Task 1, Epoch 18/20 => Loss 0.245
2023-12-19 07:51:44,793 [slca.py] => Task 1, Epoch 19/20 => Loss 0.131
2023-12-19 07:51:52,505 [slca.py] => Task 1, Epoch 20/20 => Loss 0.215, Train_accy 92.500, Test_accy 70.830
2023-12-19 07:52:03,193 [slca.py] => CA Task 1 => Loss 0.018, Test_accy 72.140
2023-12-19 07:52:08,296 [slca.py] => CA Task 1 => Loss 0.004, Test_accy 72.810
2023-12-19 07:52:13,358 [slca.py] => CA Task 1 => Loss 0.001, Test_accy 73.590
2023-12-19 07:52:18,445 [slca.py] => CA Task 1 => Loss 0.001, Test_accy 73.490
2023-12-19 07:52:23,562 [slca.py] => CA Task 1 => Loss 0.001, Test_accy 73.590
2023-12-19 07:52:28,543 [slca.py] => Exemplar size: 0
2023-12-19 07:52:29,271 [trainer.py] => No NME accuracy.
2023-12-19 07:52:29,271 [trainer.py] => CNN: {'total': 73.59, '00-09': 82.08, '10-19': 65.1, 'old': 82.08, 'new': 65.1}
2023-12-19 07:52:29,271 [trainer.py] => CNN top1 curve: [86.61, 73.59]
2023-12-19 07:52:29,272 [trainer.py] => CNN top1 avg: 80.1
2023-12-19 07:52:29,272 [trainer.py] => CNN top5 curve: [99.67, 97.08]

2023-12-19 07:52:29,272 [trainer.py] => All params: 85814036
2023-12-19 07:52:29,273 [trainer.py] => Trainable params: 85814036
2023-12-19 07:52:29,273 [slca.py] => Learning on 20-30
2023-12-19 07:52:31,265 [slca.py] => Task 2, Epoch 1/20 => Loss 2.432
2023-12-19 07:52:33,040 [slca.py] => Task 2, Epoch 2/20 => Loss 2.285
2023-12-19 07:52:34,845 [slca.py] => Task 2, Epoch 3/20 => Loss 2.013
2023-12-19 07:52:36,664 [slca.py] => Task 2, Epoch 4/20 => Loss 1.917
2023-12-19 07:52:46,983 [slca.py] => Task 2, Epoch 5/20 => Loss 1.659, Train_accy 61.670, Test_accy 48.060
2023-12-19 07:52:48,813 [slca.py] => Task 2, Epoch 6/20 => Loss 1.483
2023-12-19 07:52:50,679 [slca.py] => Task 2, Epoch 7/20 => Loss 1.134
2023-12-19 07:52:52,488 [slca.py] => Task 2, Epoch 8/20 => Loss 0.911
2023-12-19 07:52:54,376 [slca.py] => Task 2, Epoch 9/20 => Loss 0.742
2023-12-19 07:53:05,007 [slca.py] => Task 2, Epoch 10/20 => Loss 0.714, Train_accy 81.670, Test_accy 60.050
2023-12-19 07:53:06,931 [slca.py] => Task 2, Epoch 11/20 => Loss 0.476
2023-12-19 07:53:08,809 [slca.py] => Task 2, Epoch 12/20 => Loss 0.291
2023-12-19 07:53:10,697 [slca.py] => Task 2, Epoch 13/20 => Loss 0.227
2023-12-19 07:53:12,557 [slca.py] => Task 2, Epoch 14/20 => Loss 0.301
2023-12-19 07:53:22,932 [slca.py] => Task 2, Epoch 15/20 => Loss 0.300, Train_accy 90.830, Test_accy 68.040
2023-12-19 07:53:24,772 [slca.py] => Task 2, Epoch 16/20 => Loss 0.204
2023-12-19 07:53:26,569 [slca.py] => Task 2, Epoch 17/20 => Loss 0.334
2023-12-19 07:53:28,345 [slca.py] => Task 2, Epoch 18/20 => Loss 0.195
2023-12-19 07:53:30,121 [slca.py] => Task 2, Epoch 19/20 => Loss 0.179
2023-12-19 07:53:40,339 [slca.py] => Task 2, Epoch 20/20 => Loss 0.127, Train_accy 90.830, Test_accy 69.090
2023-12-19 07:53:53,122 [slca.py] => CA Task 2 => Loss 0.026, Test_accy 70.820
2023-12-19 07:54:00,292 [slca.py] => CA Task 2 => Loss 0.002, Test_accy 71.030
2023-12-19 07:54:07,380 [slca.py] => CA Task 2 => Loss 0.001, Test_accy 71.540
2023-12-19 07:54:14,648 [slca.py] => CA Task 2 => Loss 0.001, Test_accy 71.540
2023-12-19 07:54:21,950 [slca.py] => CA Task 2 => Loss 0.001, Test_accy 71.500
2023-12-19 07:54:28,974 [slca.py] => Exemplar size: 0
2023-12-19 07:54:29,706 [trainer.py] => No NME accuracy.
2023-12-19 07:54:29,706 [trainer.py] => CNN: {'total': 71.5, '00-09': 78.63, '10-19': 62.67, '20-29': 73.25, 'old': 70.63, 'new': 73.25}
2023-12-19 07:54:29,706 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5]
2023-12-19 07:54:29,707 [trainer.py] => CNN top1 avg: 77.23333333333333
2023-12-19 07:54:29,707 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74]

2023-12-19 07:54:29,707 [trainer.py] => All params: 85821726
2023-12-19 07:54:29,708 [trainer.py] => Trainable params: 85821726
2023-12-19 07:54:29,708 [slca.py] => Learning on 30-40
2023-12-19 07:54:31,891 [slca.py] => Task 3, Epoch 1/20 => Loss 2.328
2023-12-19 07:54:33,873 [slca.py] => Task 3, Epoch 2/20 => Loss 2.067
2023-12-19 07:54:35,985 [slca.py] => Task 3, Epoch 3/20 => Loss 1.681
2023-12-19 07:54:37,999 [slca.py] => Task 3, Epoch 4/20 => Loss 1.381
2023-12-19 07:54:50,537 [slca.py] => Task 3, Epoch 5/20 => Loss 0.856, Train_accy 79.380, Test_accy 59.880
2023-12-19 07:54:52,724 [slca.py] => Task 3, Epoch 6/20 => Loss 0.748
2023-12-19 07:54:54,747 [slca.py] => Task 3, Epoch 7/20 => Loss 0.425
2023-12-19 07:54:56,758 [slca.py] => Task 3, Epoch 8/20 => Loss 0.192
2023-12-19 07:54:58,796 [slca.py] => Task 3, Epoch 9/20 => Loss 0.337
2023-12-19 07:55:11,566 [slca.py] => Task 3, Epoch 10/20 => Loss 0.177, Train_accy 81.250, Test_accy 68.670
2023-12-19 07:55:13,605 [slca.py] => Task 3, Epoch 11/20 => Loss 0.239
2023-12-19 07:55:15,648 [slca.py] => Task 3, Epoch 12/20 => Loss 0.114
2023-12-19 07:55:17,642 [slca.py] => Task 3, Epoch 13/20 => Loss 0.067
2023-12-19 07:55:19,675 [slca.py] => Task 3, Epoch 14/20 => Loss 0.069
2023-12-19 07:55:32,256 [slca.py] => Task 3, Epoch 15/20 => Loss 0.119, Train_accy 85.620, Test_accy 69.080
2023-12-19 07:55:34,273 [slca.py] => Task 3, Epoch 16/20 => Loss 0.117
2023-12-19 07:55:36,408 [slca.py] => Task 3, Epoch 17/20 => Loss 0.080
2023-12-19 07:55:38,416 [slca.py] => Task 3, Epoch 18/20 => Loss 0.147
2023-12-19 07:55:40,424 [slca.py] => Task 3, Epoch 19/20 => Loss 0.051
2023-12-19 07:55:53,042 [slca.py] => Task 3, Epoch 20/20 => Loss 0.036, Train_accy 86.880, Test_accy 68.220
2023-12-19 07:56:08,214 [slca.py] => CA Task 3 => Loss 0.043, Test_accy 72.200
2023-12-19 07:56:17,596 [slca.py] => CA Task 3 => Loss 0.004, Test_accy 72.580
2023-12-19 07:56:27,005 [slca.py] => CA Task 3 => Loss 0.002, Test_accy 72.860
2023-12-19 07:56:36,186 [slca.py] => CA Task 3 => Loss 0.002, Test_accy 72.980
2023-12-19 07:56:45,519 [slca.py] => CA Task 3 => Loss 0.001, Test_accy 73.010
2023-12-19 07:56:54,535 [slca.py] => Exemplar size: 0
2023-12-19 07:56:55,318 [trainer.py] => No NME accuracy.
2023-12-19 07:56:55,318 [trainer.py] => CNN: {'total': 73.01, '00-09': 72.02, '10-19': 59.48, '20-29': 76.21, '30-39': 84.31, 'old': 69.23, 'new': 84.31}
2023-12-19 07:56:55,319 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5, 73.01]
2023-12-19 07:56:55,319 [trainer.py] => CNN top1 avg: 76.1775
2023-12-19 07:56:55,319 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74, 94.38]

2023-12-19 07:56:55,319 [trainer.py] => All params: 85829416
2023-12-19 07:56:55,320 [trainer.py] => Trainable params: 85829416
2023-12-19 07:56:55,320 [slca.py] => Learning on 40-50
2023-12-19 07:56:57,689 [slca.py] => Task 4, Epoch 1/20 => Loss 2.302
2023-12-19 07:57:00,001 [slca.py] => Task 4, Epoch 2/20 => Loss 1.964
2023-12-19 07:57:02,278 [slca.py] => Task 4, Epoch 3/20 => Loss 1.456
2023-12-19 07:57:04,558 [slca.py] => Task 4, Epoch 4/20 => Loss 0.965
2023-12-19 07:57:19,423 [slca.py] => Task 4, Epoch 5/20 => Loss 0.498, Train_accy 79.500, Test_accy 59.580
2023-12-19 07:57:21,816 [slca.py] => Task 4, Epoch 6/20 => Loss 0.381
2023-12-19 07:57:24,111 [slca.py] => Task 4, Epoch 7/20 => Loss 0.185
2023-12-19 07:57:26,387 [slca.py] => Task 4, Epoch 8/20 => Loss 0.332
2023-12-19 07:57:28,575 [slca.py] => Task 4, Epoch 9/20 => Loss 0.142
2023-12-19 07:57:43,483 [slca.py] => Task 4, Epoch 10/20 => Loss 0.180, Train_accy 82.000, Test_accy 65.020
2023-12-19 07:57:45,770 [slca.py] => Task 4, Epoch 11/20 => Loss 0.182
2023-12-19 07:57:48,045 [slca.py] => Task 4, Epoch 12/20 => Loss 0.081
2023-12-19 07:57:50,234 [slca.py] => Task 4, Epoch 13/20 => Loss 0.112
2023-12-19 07:57:52,548 [slca.py] => Task 4, Epoch 14/20 => Loss 0.026
2023-12-19 07:58:07,463 [slca.py] => Task 4, Epoch 15/20 => Loss 0.120, Train_accy 81.500, Test_accy 63.580
2023-12-19 07:58:09,772 [slca.py] => Task 4, Epoch 16/20 => Loss 0.108
2023-12-19 07:58:12,085 [slca.py] => Task 4, Epoch 17/20 => Loss 0.078
2023-12-19 07:58:14,315 [slca.py] => Task 4, Epoch 18/20 => Loss 0.095
2023-12-19 07:58:16,542 [slca.py] => Task 4, Epoch 19/20 => Loss 0.025
2023-12-19 07:58:31,516 [slca.py] => Task 4, Epoch 20/20 => Loss 0.128, Train_accy 82.000, Test_accy 62.680
2023-12-19 07:58:48,922 [slca.py] => CA Task 4 => Loss 0.040, Test_accy 68.290
2023-12-19 07:59:00,467 [slca.py] => CA Task 4 => Loss 0.003, Test_accy 68.510
2023-12-19 07:59:12,134 [slca.py] => CA Task 4 => Loss 0.003, Test_accy 68.610
2023-12-19 07:59:23,530 [slca.py] => CA Task 4 => Loss 0.001, Test_accy 68.790
2023-12-19 07:59:34,872 [slca.py] => CA Task 4 => Loss 0.002, Test_accy 68.830
2023-12-19 07:59:45,892 [slca.py] => Exemplar size: 0
2023-12-19 07:59:46,668 [trainer.py] => No NME accuracy.
2023-12-19 07:59:46,669 [trainer.py] => CNN: {'total': 68.83, '00-09': 69.04, '10-19': 55.06, '20-29': 66.43, '30-39': 79.04, '40-49': 74.6, 'old': 67.38, 'new': 74.6}
2023-12-19 07:59:46,669 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5, 73.01, 68.83]
2023-12-19 07:59:46,669 [trainer.py] => CNN top1 avg: 74.708
2023-12-19 07:59:46,669 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74, 94.38, 93.07]

2023-12-19 07:59:46,669 [trainer.py] => All params: 85837106
2023-12-19 07:59:46,670 [trainer.py] => Trainable params: 85837106
2023-12-19 07:59:46,670 [slca.py] => Learning on 50-60
2023-12-19 07:59:49,231 [slca.py] => Task 5, Epoch 1/20 => Loss 2.435
2023-12-19 07:59:51,708 [slca.py] => Task 5, Epoch 2/20 => Loss 2.053
2023-12-19 07:59:54,169 [slca.py] => Task 5, Epoch 3/20 => Loss 1.432
2023-12-19 07:59:56,678 [slca.py] => Task 5, Epoch 4/20 => Loss 0.986
2023-12-19 08:00:13,637 [slca.py] => Task 5, Epoch 5/20 => Loss 0.540, Train_accy 73.330, Test_accy 56.130
2023-12-19 08:00:16,162 [slca.py] => Task 5, Epoch 6/20 => Loss 0.350
2023-12-19 08:00:18,668 [slca.py] => Task 5, Epoch 7/20 => Loss 0.234
2023-12-19 08:00:21,090 [slca.py] => Task 5, Epoch 8/20 => Loss 0.137
2023-12-19 08:00:23,498 [slca.py] => Task 5, Epoch 9/20 => Loss 0.278
2023-12-19 08:00:40,417 [slca.py] => Task 5, Epoch 10/20 => Loss 0.110, Train_accy 84.170, Test_accy 60.840
2023-12-19 08:00:42,894 [slca.py] => Task 5, Epoch 11/20 => Loss 0.178
2023-12-19 08:00:45,361 [slca.py] => Task 5, Epoch 12/20 => Loss 0.051
2023-12-19 08:00:47,874 [slca.py] => Task 5, Epoch 13/20 => Loss 0.046
2023-12-19 08:00:50,398 [slca.py] => Task 5, Epoch 14/20 => Loss 0.150
2023-12-19 08:01:07,425 [slca.py] => Task 5, Epoch 15/20 => Loss 0.041, Train_accy 85.830, Test_accy 60.800
2023-12-19 08:01:09,851 [slca.py] => Task 5, Epoch 16/20 => Loss 0.070
2023-12-19 08:01:12,337 [slca.py] => Task 5, Epoch 17/20 => Loss 0.027
2023-12-19 08:01:14,763 [slca.py] => Task 5, Epoch 18/20 => Loss 0.031
2023-12-19 08:01:17,190 [slca.py] => Task 5, Epoch 19/20 => Loss 0.069
2023-12-19 08:01:34,119 [slca.py] => Task 5, Epoch 20/20 => Loss 0.023, Train_accy 81.250, Test_accy 60.430
2023-12-19 08:01:53,335 [slca.py] => CA Task 5 => Loss 0.040, Test_accy 67.480
2023-12-19 08:02:06,742 [slca.py] => CA Task 5 => Loss 0.003, Test_accy 67.750
2023-12-19 08:02:19,934 [slca.py] => CA Task 5 => Loss 0.002, Test_accy 67.870
2023-12-19 08:02:33,119 [slca.py] => CA Task 5 => Loss 0.001, Test_accy 67.900
2023-12-19 08:02:46,406 [slca.py] => CA Task 5 => Loss 0.001, Test_accy 67.900
2023-12-19 08:02:59,223 [slca.py] => Exemplar size: 0
2023-12-19 08:03:00,009 [trainer.py] => No NME accuracy.
2023-12-19 08:03:00,010 [trainer.py] => CNN: {'total': 67.9, '00-09': 65.85, '10-19': 53.25, '20-29': 63.79, '30-39': 79.22, '40-49': 69.8, '50-59': 75.69, 'old': 66.35, 'new': 75.69}
2023-12-19 08:03:00,010 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5, 73.01, 68.83, 67.9]
2023-12-19 08:03:00,010 [trainer.py] => CNN top1 avg: 73.57333333333332
2023-12-19 08:03:00,010 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74, 94.38, 93.07, 91.76]

2023-12-19 08:03:00,010 [trainer.py] => All params: 85844796
2023-12-19 08:03:00,011 [trainer.py] => Trainable params: 85844796
2023-12-19 08:03:00,011 [slca.py] => Learning on 60-70
2023-12-19 08:03:02,697 [slca.py] => Task 6, Epoch 1/20 => Loss 2.242
2023-12-19 08:03:05,368 [slca.py] => Task 6, Epoch 2/20 => Loss 1.336
2023-12-19 08:03:08,020 [slca.py] => Task 6, Epoch 3/20 => Loss 0.915
2023-12-19 08:03:10,720 [slca.py] => Task 6, Epoch 4/20 => Loss 0.798
2023-12-19 08:03:29,941 [slca.py] => Task 6, Epoch 5/20 => Loss 0.289, Train_accy 79.640, Test_accy 58.420
2023-12-19 08:03:32,623 [slca.py] => Task 6, Epoch 6/20 => Loss nan
2023-12-19 08:03:35,316 [slca.py] => Task 6, Epoch 7/20 => Loss 0.144
2023-12-19 08:03:37,967 [slca.py] => Task 6, Epoch 8/20 => Loss 0.074
2023-12-19 08:03:40,684 [slca.py] => Task 6, Epoch 9/20 => Loss 0.128
2023-12-19 08:04:00,116 [slca.py] => Task 6, Epoch 10/20 => Loss 0.054, Train_accy 85.710, Test_accy 59.720
2023-12-19 08:04:02,719 [slca.py] => Task 6, Epoch 11/20 => Loss 0.045
2023-12-19 08:04:05,423 [slca.py] => Task 6, Epoch 12/20 => Loss 0.111
2023-12-19 08:04:08,187 [slca.py] => Task 6, Epoch 13/20 => Loss 0.073
2023-12-19 08:04:10,886 [slca.py] => Task 6, Epoch 14/20 => Loss 0.094
2023-12-19 08:04:30,235 [slca.py] => Task 6, Epoch 15/20 => Loss 0.056, Train_accy 79.640, Test_accy 59.740
2023-12-19 08:04:32,949 [slca.py] => Task 6, Epoch 16/20 => Loss 0.151
2023-12-19 08:04:35,675 [slca.py] => Task 6, Epoch 17/20 => Loss 0.084
2023-12-19 08:04:38,266 [slca.py] => Task 6, Epoch 18/20 => Loss 0.229
2023-12-19 08:04:40,848 [slca.py] => Task 6, Epoch 19/20 => Loss 0.020
2023-12-19 08:05:00,422 [slca.py] => Task 6, Epoch 20/20 => Loss 0.040, Train_accy 81.070, Test_accy 59.270
2023-12-19 08:05:21,654 [slca.py] => CA Task 6 => Loss 0.043, Test_accy 66.650
2023-12-19 08:05:36,951 [slca.py] => CA Task 6 => Loss 0.004, Test_accy 66.910
2023-12-19 08:05:52,253 [slca.py] => CA Task 6 => Loss 0.003, Test_accy 67.190
2023-12-19 08:06:07,502 [slca.py] => CA Task 6 => Loss 0.002, Test_accy 67.200
2023-12-19 08:06:22,800 [slca.py] => CA Task 6 => Loss 0.002, Test_accy 67.260
2023-12-19 08:06:37,642 [slca.py] => Exemplar size: 0
2023-12-19 08:06:38,382 [trainer.py] => No NME accuracy.
2023-12-19 08:06:38,382 [trainer.py] => CNN: {'total': 67.26, '00-09': 62.08, '10-19': 52.58, '20-29': 63.08, '30-39': 78.33, '40-49': 66.43, '50-59': 72.21, '60-69': 76.14, 'old': 65.78, 'new': 76.14}
2023-12-19 08:06:38,382 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5, 73.01, 68.83, 67.9, 67.26]
2023-12-19 08:06:38,382 [trainer.py] => CNN top1 avg: 72.67142857142856
2023-12-19 08:06:38,382 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74, 94.38, 93.07, 91.76, 90.7]

2023-12-19 08:06:38,383 [trainer.py] => All params: 85852486
2023-12-19 08:06:38,383 [trainer.py] => Trainable params: 85852486
2023-12-19 08:06:38,384 [slca.py] => Learning on 70-80
2023-12-19 08:06:41,358 [slca.py] => Task 7, Epoch 1/20 => Loss 2.442
2023-12-19 08:06:44,210 [slca.py] => Task 7, Epoch 2/20 => Loss 1.757
2023-12-19 08:06:47,040 [slca.py] => Task 7, Epoch 3/20 => Loss 1.055
2023-12-19 08:06:49,924 [slca.py] => Task 7, Epoch 4/20 => Loss 0.490
2023-12-19 08:07:11,473 [slca.py] => Task 7, Epoch 5/20 => Loss 0.342, Train_accy 76.880, Test_accy 56.510
2023-12-19 08:07:14,377 [slca.py] => Task 7, Epoch 6/20 => Loss 0.233
2023-12-19 08:07:17,288 [slca.py] => Task 7, Epoch 7/20 => Loss 0.188
2023-12-19 08:07:20,130 [slca.py] => Task 7, Epoch 8/20 => Loss 0.219
2023-12-19 08:07:23,147 [slca.py] => Task 7, Epoch 9/20 => Loss 0.090
2023-12-19 08:07:44,955 [slca.py] => Task 7, Epoch 10/20 => Loss 0.196, Train_accy 83.440, Test_accy 59.190
2023-12-19 08:07:47,886 [slca.py] => Task 7, Epoch 11/20 => Loss 0.201
2023-12-19 08:07:50,770 [slca.py] => Task 7, Epoch 12/20 => Loss 0.142
2023-12-19 08:07:53,577 [slca.py] => Task 7, Epoch 13/20 => Loss 0.130
2023-12-19 08:07:56,501 [slca.py] => Task 7, Epoch 14/20 => Loss 0.082
2023-12-19 08:08:18,028 [slca.py] => Task 7, Epoch 15/20 => Loss 0.033, Train_accy 79.690, Test_accy 59.000
2023-12-19 08:08:20,942 [slca.py] => Task 7, Epoch 16/20 => Loss 0.103
2023-12-19 08:08:23,815 [slca.py] => Task 7, Epoch 17/20 => Loss 0.032
2023-12-19 08:08:26,627 [slca.py] => Task 7, Epoch 18/20 => Loss 0.036
2023-12-19 08:08:29,489 [slca.py] => Task 7, Epoch 19/20 => Loss 0.032
2023-12-19 08:08:51,281 [slca.py] => Task 7, Epoch 20/20 => Loss 0.052, Train_accy 78.440, Test_accy 58.440
2023-12-19 08:09:14,413 [slca.py] => CA Task 7 => Loss 0.044, Test_accy 64.890
2023-12-19 08:09:31,853 [slca.py] => CA Task 7 => Loss 0.004, Test_accy 65.750
2023-12-19 08:09:49,264 [slca.py] => CA Task 7 => Loss 0.002, Test_accy 65.910
2023-12-19 08:10:06,601 [slca.py] => CA Task 7 => Loss 0.002, Test_accy 65.970
2023-12-19 08:10:23,960 [slca.py] => CA Task 7 => Loss 0.001, Test_accy 66.090
2023-12-19 08:10:40,829 [slca.py] => Exemplar size: 0
2023-12-19 08:10:41,626 [trainer.py] => No NME accuracy.
2023-12-19 08:10:41,626 [trainer.py] => CNN: {'total': 66.09, '00-09': 56.51, '10-19': 50.35, '20-29': 59.02, '30-39': 78.01, '40-49': 66.1, '50-59': 72.37, '60-69': 71.96, '70-79': 74.5, 'old': 64.89, 'new': 74.5}
2023-12-19 08:10:41,626 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5, 73.01, 68.83, 67.9, 67.26, 66.09]
2023-12-19 08:10:41,626 [trainer.py] => CNN top1 avg: 71.84875
2023-12-19 08:10:41,626 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74, 94.38, 93.07, 91.76, 90.7, 89.4]

2023-12-19 08:10:41,627 [trainer.py] => All params: 85860176
2023-12-19 08:10:41,627 [trainer.py] => Trainable params: 85860176
2023-12-19 08:10:41,628 [slca.py] => Learning on 80-90
2023-12-19 08:10:44,844 [slca.py] => Task 8, Epoch 1/20 => Loss 2.424
2023-12-19 08:10:47,943 [slca.py] => Task 8, Epoch 2/20 => Loss 1.635
2023-12-19 08:10:51,017 [slca.py] => Task 8, Epoch 3/20 => Loss 0.867
2023-12-19 08:10:54,146 [slca.py] => Task 8, Epoch 4/20 => Loss 0.298
2023-12-19 08:11:18,050 [slca.py] => Task 8, Epoch 5/20 => Loss 0.193, Train_accy 78.610, Test_accy 57.290
2023-12-19 08:11:21,152 [slca.py] => Task 8, Epoch 6/20 => Loss 0.264
2023-12-19 08:11:24,224 [slca.py] => Task 8, Epoch 7/20 => Loss 0.088
2023-12-19 08:11:27,313 [slca.py] => Task 8, Epoch 8/20 => Loss 0.071
2023-12-19 08:11:30,320 [slca.py] => Task 8, Epoch 9/20 => Loss 0.041
2023-12-19 08:11:54,179 [slca.py] => Task 8, Epoch 10/20 => Loss 0.072, Train_accy 80.000, Test_accy 59.420
2023-12-19 08:11:57,226 [slca.py] => Task 8, Epoch 11/20 => Loss 0.015
2023-12-19 08:12:00,204 [slca.py] => Task 8, Epoch 12/20 => Loss 0.043
2023-12-19 08:12:03,231 [slca.py] => Task 8, Epoch 13/20 => Loss 0.020
2023-12-19 08:12:06,224 [slca.py] => Task 8, Epoch 14/20 => Loss 0.083
2023-12-19 08:12:30,178 [slca.py] => Task 8, Epoch 15/20 => Loss 0.014, Train_accy 83.060, Test_accy 59.390
2023-12-19 08:12:33,262 [slca.py] => Task 8, Epoch 16/20 => Loss 0.127
2023-12-19 08:12:36,260 [slca.py] => Task 8, Epoch 17/20 => Loss 0.049
2023-12-19 08:12:39,392 [slca.py] => Task 8, Epoch 18/20 => Loss 0.096
2023-12-19 08:12:42,505 [slca.py] => Task 8, Epoch 19/20 => Loss 0.013
2023-12-19 08:13:06,231 [slca.py] => Task 8, Epoch 20/20 => Loss 0.094, Train_accy 81.110, Test_accy 59.410
2023-12-19 08:13:31,543 [slca.py] => CA Task 8 => Loss 0.041, Test_accy 64.920
2023-12-19 08:13:50,984 [slca.py] => CA Task 8 => Loss 0.006, Test_accy 65.670
2023-12-19 08:14:10,606 [slca.py] => CA Task 8 => Loss 0.003, Test_accy 66.150
2023-12-19 08:14:30,173 [slca.py] => CA Task 8 => Loss 0.002, Test_accy 66.280
2023-12-19 08:14:49,598 [slca.py] => CA Task 8 => Loss 0.002, Test_accy 66.340
2023-12-19 08:15:08,458 [slca.py] => Exemplar size: 0
2023-12-19 08:15:09,265 [trainer.py] => No NME accuracy.
2023-12-19 08:15:09,265 [trainer.py] => CNN: {'total': 66.34, '00-09': 55.68, '10-19': 47.29, '20-29': 59.18, '30-39': 76.41, '40-49': 61.79, '50-59': 71.74, '60-69': 69.84, '70-79': 73.41, '80-89': 81.71, 'old': 64.42, 'new': 81.71}
2023-12-19 08:15:09,265 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5, 73.01, 68.83, 67.9, 67.26, 66.09, 66.34]
2023-12-19 08:15:09,265 [trainer.py] => CNN top1 avg: 71.23666666666666
2023-12-19 08:15:09,265 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74, 94.38, 93.07, 91.76, 90.7, 89.4, 89.55]

2023-12-19 08:15:09,266 [trainer.py] => All params: 85867866
2023-12-19 08:15:09,266 [trainer.py] => Trainable params: 85867866
2023-12-19 08:15:09,267 [slca.py] => Learning on 90-100
2023-12-19 08:15:12,635 [slca.py] => Task 9, Epoch 1/20 => Loss nan
2023-12-19 08:15:15,939 [slca.py] => Task 9, Epoch 2/20 => Loss nan
2023-12-19 08:15:19,115 [slca.py] => Task 9, Epoch 3/20 => Loss 0.626
2023-12-19 08:15:22,334 [slca.py] => Task 9, Epoch 4/20 => Loss 0.270
2023-12-19 08:15:48,488 [slca.py] => Task 9, Epoch 5/20 => Loss 0.118, Train_accy 81.000, Test_accy 56.790
2023-12-19 08:15:51,749 [slca.py] => Task 9, Epoch 6/20 => Loss nan
2023-12-19 08:15:54,960 [slca.py] => Task 9, Epoch 7/20 => Loss 0.108
2023-12-19 08:15:58,167 [slca.py] => Task 9, Epoch 8/20 => Loss 0.114
2023-12-19 08:16:01,511 [slca.py] => Task 9, Epoch 9/20 => Loss 0.094
2023-12-19 08:16:27,717 [slca.py] => Task 9, Epoch 10/20 => Loss 0.010, Train_accy 81.750, Test_accy 58.190
2023-12-19 08:16:30,944 [slca.py] => Task 9, Epoch 11/20 => Loss 0.020
2023-12-19 08:16:34,242 [slca.py] => Task 9, Epoch 12/20 => Loss 0.027
2023-12-19 08:16:37,482 [slca.py] => Task 9, Epoch 13/20 => Loss 0.026
2023-12-19 08:16:40,728 [slca.py] => Task 9, Epoch 14/20 => Loss 0.062
2023-12-19 08:17:06,792 [slca.py] => Task 9, Epoch 15/20 => Loss nan, Train_accy 82.000, Test_accy 57.770
2023-12-19 08:17:10,127 [slca.py] => Task 9, Epoch 16/20 => Loss 0.037
2023-12-19 08:17:13,408 [slca.py] => Task 9, Epoch 17/20 => Loss 0.061
2023-12-19 08:17:16,718 [slca.py] => Task 9, Epoch 18/20 => Loss 0.008
2023-12-19 08:17:20,060 [slca.py] => Task 9, Epoch 19/20 => Loss 0.058
2023-12-19 08:17:46,220 [slca.py] => Task 9, Epoch 20/20 => Loss 0.049, Train_accy 80.750, Test_accy 58.040
2023-12-19 08:18:13,878 [slca.py] => CA Task 9 => Loss 0.040, Test_accy 63.730
2023-12-19 08:18:35,490 [slca.py] => CA Task 9 => Loss 0.005, Test_accy 64.100
2023-12-19 08:18:57,095 [slca.py] => CA Task 9 => Loss 0.003, Test_accy 64.620
2023-12-19 08:19:18,725 [slca.py] => CA Task 9 => Loss 0.002, Test_accy 64.600
2023-12-19 08:19:40,348 [slca.py] => CA Task 9 => Loss 0.002, Test_accy 64.630
2023-12-19 08:20:01,131 [slca.py] => Exemplar size: 0
2023-12-19 08:20:01,912 [trainer.py] => No NME accuracy.
2023-12-19 08:20:01,912 [trainer.py] => CNN: {'total': 64.63, '00-09': 50.25, '10-19': 50.15, '20-29': 56.96, '30-39': 71.64, '40-49': 60.5, '50-59': 69.04, '60-69': 70.47, '70-79': 70.81, '80-89': 79.06, '90-99': 67.5, 'old': 64.32, 'new': 67.5}
2023-12-19 08:20:01,912 [trainer.py] => CNN top1 curve: [86.61, 73.59, 71.5, 73.01, 68.83, 67.9, 67.26, 66.09, 66.34, 64.63]
2023-12-19 08:20:01,912 [trainer.py] => CNN top1 avg: 70.576
2023-12-19 08:20:01,912 [trainer.py] => CNN top5 curve: [99.67, 97.08, 94.74, 94.38, 93.07, 91.76, 90.7, 89.4, 89.55, 89.47]

2023-12-19 08:20:01,913 [trainer.py] => final accs: [64.63]
2023-12-19 08:20:01,913 [trainer.py] => avg accs: [70.576]
